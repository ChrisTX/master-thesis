\documentclass[../thesis.tex]{subfiles}

\begin{document}
\chapter{Discretization of the control problem}
In this chapter, we're going back to the optimal control problem introduced in \cref{sec:OptimalControlProblem}.
We are going to restrict ourselves to the boundary and inner heating problems with no restrictions made to the control itself, i.e.\ $u_a = -\infty$ and $u_b = \infty$.
\section{Existence of solutions for a generalized problem}
We're going to work with weak solutions in appropriate spaces. To this end, we introduce a generalized form of the state equation, that can cover both, \cref{eq:BoundaryOptimalControl,eq:InnerOptimalControl}:
\begin{equation}
\label{eq:generalized-problem}
\begin{IEEEeqnarraybox}[][c]{rCl"lCl}
\frac{\partial y}{\partial t} (x, t) - \lapl y(x, t) &=& g_I(x, t) & \text{for } (x, t) \in \Omega &\coloneqq& \Omega \times (0, T), \\
n_x(x,t) \cdot \nabla_x y(x, t) + \alpha y(x, t) &=& g_R(x, t) & \text{for } (x, t) \in \Sigma &\coloneqq& \partial \Omega \times (0, T) \\
y(x, 0) &=& y_0(x) & \text{for } (x, t) \in \Omega,
\end{IEEEeqnarraybox}
\end{equation}
where we assume that $\Omega$ is a bounded Lipschitz domain, $T > 0$ is a fixed time and $\alpha \in L^\infty(\Sigma)$, $\alpha(x, t) \geq 0$ almost everywhere.

Formally, we test the first equation of \cref{eq:generalized-problem} by multiplying with a function $v$ and integrating over $Q$:
\begin{IEEEeqnarray*}{rCl}
	\iint_Q \left(\frac{\partial y}{\partial t} - \lapl y\right) v \dd x \dd t &=& \iint_Q \frac{\partial y}{\partial t} v \dd x \dd t - \iint_Q \lapl y v \dd x \dd t \\
	&=& \iint_Q g_I v \dd x \dd t
\end{IEEEeqnarray*}
We proceed by splitting the integral on the left hand side and consider each part individually.
For the term containing the time derivative, we then obtain using integration by parts and using that $y(\cdot, 0) = y_0(\cdot)$:
\begin{IEEEeqnarray*}{rCl}
	\iint_Q \frac{\partial y}{\partial t} v \dd x \dd t &=& - \iint_Q y \frac{\partial v}{\partial t} \dd x \dd t - \int_\Omega y(\cdot, 0) v(\cdot, 0) \dd x + \int_\Omega y(\cdot, T) v(\cdot, T) \dd x \\
	&=& - \iint_Q y \frac{\partial v}{\partial t} \dd x \dd t + \int_\Omega y(\cdot, T) v(\cdot, T) \dd x - \int_\Omega y_0(\cdot) v(\cdot, 0) \dd x.
\end{IEEEeqnarray*}
In order to deal with the term containing $\lapl y$, we use Green's first identity and exploit the given normal derivative on the boundary:
\begin{IEEEeqnarray*}{rCl}
	- \iint_Q \lapl y v \dd x \dd t &=& \iint_Q \nabla y \nabla v \dd x \dd t - \iint_{\Sigma} (\nabla y v) \cdot \nu \dd s(x) \dd t \\
	&=& \iint_Q \nabla y \nabla v \dd x \dd t - \iint_{\Sigma} \left(-\alpha y - g_R \right) v \dd s(x) \dd t \\
	&=& \iint_Q \nabla y \nabla v \dd x \dd t + \alpha \iint_{\Sigma} y v \dd s(x) \dd t - \iint_{\Sigma} g_R v \dd s(x) \dd t.
\end{IEEEeqnarray*}

Adding these two terms up again yields
\begin{IEEEeqnarray*}{rCl}
	\iint_Q \left(\frac{\partial y}{\partial t} - \lapl y\right) v \dd x \dd t &=& \iint_Q \nabla y \nabla v \dd x \dd t - \iint_Q y \frac{\partial v}{\partial t} \dd x \dd t \\
	&& \quad {} + \alpha \iint_{\Sigma} y v \dd s(x) \dd t + \int_\Omega y(\cdot, T) v(\cdot, T) \dd x \\
	&& \quad {} - \int_\Omega y_0(\cdot) v(\cdot, 0) \dd x  - \iint_{\Sigma} g_R v \dd s(x) \dd t.
\end{IEEEeqnarray*}
Reorganizing the terms gives us the formulation
\begin{equation}
\label{eq:generalized-weak-form}
\begin{IEEEeqnarraybox}[][c]{l}
	\iint_Q \nabla y \nabla v \dd x \dd t - \iint_Q y \frac{\partial v}{\partial t} \dd x \dd t + \alpha \iint_{\Sigma} y v \dd s(x) \dd t + \int_\Omega y(\cdot, T) v(\cdot, T) \dd x \qquad \\
	\IEEEeqnarraymulticol{1}{r}{ {} = \iint_Q g_I v \dd x \dd t + \int_\Omega y_0 v(\cdot, 0) \dd x + \iint_{\Sigma} g_R v \dd s(x) \dd t }.
\end{IEEEeqnarraybox}
\end{equation}
While we have only used a formal function $v$, we conclude that $Q$ must be bounded and have a piecewise smooth boundary, $y$ must be twice continuously differentiable in space and once continuously differentiable in time, whereas $v$ has to be continuously differentiable in both space and time directions.

Before proceeding, we consider the following partial differential equation as well:
\begin{equation}
\label{eq:generalized-adjoint-problem}
\begin{IEEEeqnarraybox}[][c]{rCl"lCl}
- \frac{\partial p}{\partial t} (x, t) - \lapl p(x, t) &=& d_I(x, t) & \text{for } (x, t) \in \Omega), \\
n_x(x,t) \cdot \nabla_x p(x, t) + \alpha p(x, t) &=& d_R(x, t) & \text{for } (x, t) \in \Sigma, \\
p(x, T) &=& p_T(x) & \text{for } (x, t) \in \Omega,
\end{IEEEeqnarraybox}
\end{equation}
The motivation for considering this problem is that it will turn out to play an important role for defining the so called adjoint state en route to an optimality system for \cref{eq:BoundaryOptimalControl,eq:InnerOptimalControl}.

One could perform the analogous procedure we just performed for $p$.
Instead of doing that, we consider the function $\tilde{p}(x, t) \coloneqq p(x, T - t)$.
Observing that we have
\[
	\lapl \tilde{p}(x, t) = \lapl p(x, T - t) \quad \text{and} \quad \frac{\partial \tilde{p}}{\partial t}(x, t) = - \frac{\partial p}{\partial t}(x, T - t),
\] 
the system for $p$ can be written instead as
\begin{IEEEeqnarray*}{rCl}
\frac{\partial \tilde{p}}{\partial t} - \lapl \tilde{p} &=& d_I(x, T - t) \\
\partial_\nu \tilde{p} + \alpha \tilde{p} &=& d_R(x, T - t) \\
\tilde{p}(0) &=& p(T) \\
p(x, t) &=& \tilde{p}(x, T - t)
\end{IEEEeqnarray*}
by applying the results for $y$, and substituting $\tilde{p}$ again, one arrives at the following weak formulation:
\begin{IEEEeqnarray*}{l}
	\iint_Q \nabla p \nabla v \dd x \dd t + \iint_Q p \frac{\partial v}{\partial t} \dd x \dd t + \alpha \iint_{\Sigma} p v \dd s(x) \dd t + \int_\Omega p(\cdot, 0) v(\cdot, 0) \dd x \qquad\qquad \\
	\IEEEeqnarraymulticol{1}{r}{ {} = \int_\Omega p(T) v(\cdot, T) \dd x + \iint_Q d_I v \dd x \dd t + \iint_\Sigma d_R v \dd s(x) \dd t }.
\end{IEEEeqnarray*}
Assuming that $\Omega$ is bounded Lipschitz domain and that $\alpha \in L^\infty(\Sigma)$, $\alpha (x, t) \geq 0$ almost everywhere in $\Sigma$, one can prove that these formal calculations can be put into a setting where they make sense and have a unique solution, which continuously depends on the right hand side.
To this end, one defines two different spaces for $v$ and $y$, one which has weak continuous space derivatives and one which has both, weak continuous space and time derivatives (see \cite[p.\ 111]{Troeltzsch} for the definition of the spaces and \cite[Satz 3.9, p.\ 112]{Troeltzsch} for the existence and uniqueness theorem).
This however is not a result directly useful to us, as we would like to seek $v$ and $y$ in the same space. As of such, we're going to skip this discussion here.

Instead, one can prove these solutions lie in a more general space in which is what we want to work with. As of such, we're first going to introduce that space, following \cite[3.4.1 Abstrakte Funktionen, p.\ 113ff.]{Troeltzsch}.
\begin{definition}
For a Banach space $X$ let $L^p(a, b; X)$, $1 \leq p < \infty$ be the linear space of the equivalence classes of functions $y : [a, b] \to X$ with the property
\[
	\int_a^b \| y(t) \|_X^p \dd t < \infty.
\]
The norm of this space is
\[
	\| y \|_{L^p(a, b; X)} \coloneqq \left( \int_a^b \| y(t) \|_X^p \dd t \right)^{\frac{1}{p}}.
\]
\end{definition}
As a next step, we need a notion of derivatives for this space. For this, we introduce vector-valued distributions, see \cite[p.\ 117]{Troeltzsch}:
\begin{definition}
For a given function $y \in L^2(0, T; X)$ we define a vector-valued distribution $\distT : C_0^\infty(0, T) \to X$ by
\[
	\distT \varphi \coloneqq \int_0^T y(t) \varphi(t) \dd t \quad \forall \varphi \in C_0^\infty(0, T).
\] 
The derivative $\distT'$ is defined by
\[
	\distT' \varphi \coloneqq - \int_0^T y(t) \varphi'(t) \dd t.
\]
If a function $w = w(t)$ in $L^1(0, T; X)$ exists with the property that
\[
	\distT' \varphi = - \int_0^T y(t) \varphi'(t) \dd t = \int_0^T w(t) \varphi(t) \dd t \quad \forall \varphi \in C_0^\infty(0, T),
\]
then we define the distributional derivative $y'(t) \coloneqq w(t)$.
\end{definition}
With these definitions, we can introduce the space we were looking for (see \cite[p.\ 118]{Troeltzsch}):
\begin{definition}
Let $W(0, T)$ be the space of all $y \in L^2(0, T; X)$ with distributional derivative $y'$ in $L^2(0, T; X^\adj)$ with the norm
\[
	\| y \|_{W(0, T)} = \left( \int_0^T ( \| y(t) \|_X^2 + \| y'(t) \|_{X^\adj}^2 ) \dd t \right)^{\frac{1}{2}},
\]
or in other words:
\[
	W(0, T) = \left\{ y \in L^2(0, T; X) \midcolon y' \in L^2(0, T; X^\adj) \right\}.
\]
\end{definition}
In the following we always choose $X = H^1(Q)$. A weak formulation of the problem \cref{eq:generalized-problem} is that for all $v \in W(0, T)$ the equation \cref{eq:generalized-weak-form} should hold.

In order to proof that this formulation makes sense, one uses the aforementioned spaces with a stronger regularity requirement first, proves existence and uniqueness of a solution in them and then proves that the solution is in $W(0, T)$ and testing with $v \in W(0, T)$ makes sense. For more details, see \cite[3.3 Schwache LÃ¶sungen in $W^{1, 0}_2(Q)$]{Troeltzsch}.
Eventually, one can prove the following result:
\begin{theorem}
\label{thm:generalized-problem-solution}
The system \cref{eq:generalized-weak-form} omits a unique solution with the estimate
\[
	\| y \|_{W(0, T)} \leq c_w ( \| g_I \|_{L^2(Q)} + \| g_R \|_{L^2(\Sigma)} + \| y_0 \|_{L^2(\Omega)} )
\] 
with a constant $c_w > 0$ independent of $g_I$, $g_R$ and $y_0$.
Consequentially, the mapping $(g_I, g_R, y_0) \mapsto f$ is continuous from $L^2(Q) \times L^2(\Sigma) \times L^2(\Omega)$ to $W(0, T)$, especially $C([0, T], L^2(\Omega))$.
\end{theorem}
\begin{proof}
For the existence of a solution in a more general space, see \cite[Satz 3.9, p.\ 112]{Troeltzsch}, for that solution lying in $W(0, T)$ see \cite[Satz 3.12, p.\ 120]{Troeltzsch} and for the continuity result, see \cite[Satz 3.13, p.\ 121]{Troeltzsch}.
\end{proof}
Due to \cref{thm:generalized-problem-solution}, linear and continuous operators $G_Q : L^2(Q) \to W(0, T)$, $G_\Sigma : L^2(\Sigma) \to W(0, T)$ and $G_0 : L^2(\Omega) \to W(0, T)$ exist such that
\[
	y = G_Q g_I + G_\Sigma g_R + G_0 y_0.
\]
\section{Optimal control with boundary controls}
We'll now go back to the optimal control problem with boundary controls:
\begin{equation}
\label{eq:BoundaryOptimalControl-restricted}
\begin{IEEEeqnarraybox}[][c]{c}
\min J(y, u) = \frac{1}{2} \int_\Omega \left( y(x, T) - y_\Omega(x) \right)^2 \dd x + \frac{\lambda}{2} \int_0^T \int_{\partial \Omega} u(x, t)^2 \dd s(x) \dd t \\
\noalign{\noindent subject to\vspace{\jot}}
\begin{IEEEeqnarraybox}{rCl"l}
\frac{\partial y}{\partial t} - \lapl y &=& 0 & \text{in } Q \\
\partial_\nu y + \alpha y &=& \beta u & \text{in } \Sigma\\
y(x, 0) &=& y_0(x) & \text{in } \Omega
\end{IEEEeqnarraybox}
\end{IEEEeqnarraybox}
\end{equation}
Due to \cref{thm:generalized-problem-solution}, we can express $y$ as
\[
	y = G_\Sigma \beta u + G_0 y_0.
\]
If we introduce an observation operator $E_T : y \mapsto y(T)$, we can use the embedding property of the solution and obtain
\[
	y(T) = E_T (G_\Sigma \beta u + G_0 y_0).
\]
By defining the so called state operator $S \coloneqq E_T G_\Sigma \beta \cdot$, we notice that with $z \coloneqq y_\Omega - (G_0 y_0)(T)$ it holds:
\[
	y(T) - y_\Omega = S u + (G_0 y_0)(T) - y_\Omega = S u - z.
\]
For the discontinuous Galerkin method introduced in \cref{sec:dG-method}, we can define an analogous operator. 
To that end, we define operators that map $g_I$, $g_R$ and $y_0$ into their discrete right hand sides, i.e.\ we define $\boldsymbol{g_I} : L^2(Q) \to \R^m$, $\boldsymbol{g_R} : L^2(\Sigma) \to \R^m$ and $\boldsymbol{y_0} : L^2(\Omega) \to \R^m$ component-wise by
\begin{IEEEeqnarray*}{rCl}
	\boldsymbol{g_I}[i] (g_I) &=& \iint_Q g_I \varphi_i \dd x \dd t, \\
	\boldsymbol{g_R}[i] (g_R) &=& \iint_\Sigma g_R \varphi_i \dd x \dd t, \\
	\boldsymbol{y_0}[i] (y_0) &=& \int_\Omega y_0 \varphi_i(\cdot, 0) \dd x \dd t.
\end{IEEEeqnarray*}
Moreover, we define an operator to map discrete solutions of \cref{eq:dg-discrete-prob} into the function they correspond to in $\Shp(\meshT_N)$:
\begin{IEEEeqnarray*}{rCl}
	E_V : \R^m &\to& \Shp(\meshT_N) \\
	\boldsymbol{v} &\mapsto& v \coloneqq \sum_{j=1}^M \boldsymbol{v}[j] \varphi_j(x, t).
\end{IEEEeqnarray*}
Then, we can define the solution operators
\begin{IEEEeqnarray*}{rCl}
	G_\Sigma^h (g_R) &\coloneqq& E_V A_h^{-1} \boldsymbol{g_R} (g_R), \\
	G_0^h (y_0) &\coloneqq& E_V A_h^{-1} \boldsymbol{y_0} (y_0).
\end{IEEEeqnarray*}
Note that these operators are well defined for $\sigma \geq 4 c_K$ due to \cref{thm:A-injective}. Therefore, the rest of this discussion assumes that \cref{as:mesh-assumptions} holds and that $\sigma$ is chosen such that $\sigma \geq 4 c_K$.
As of such, we can define a discrete analogous to $S$, called $S_h$:
\[
	y_h(T) - y_\Omega = S_h u_h + (G_0^h y_0) - y_\Omega = S_h u_h - z_h,
\]
where $S_h \coloneqq E_T G_\Sigma \beta \cdot$ and $z_h \coloneqq y_\Omega - (G_0^h y_0)(T)$.
Note that both operators work from $L^2(\Sigma)$ to $L^2(\Omega)$, because \cref{thm:asip-lower-bound} (and in a stronger sense, for quasi-uniform meshes, \cref{thm:Astab-est}) guarantees that $S_h$ is a continuous and linear operator between these spaces.

With these operators defined, we can express the optimal control problems as
\begin{equation}
\label{eq:f-S}
\min_{u \in L^2(\Omega)} f(u) \coloneqq \frac{1}{2} \| S u - z \|_{L^2(\Omega)}^2 + \frac{\lambda}{2} \| u \|_{L^2(\Sigma)}^2,
\end{equation}
and
\begin{equation}
\label{eq:f-Sh}
\min_{u_h \in L^2(\Omega)} f_h(u_h) \coloneqq \frac{1}{2} \| S_h u_h - z_h \|_{L^2(\Omega)}^2 + \frac{\lambda}{2} \| u_h \|_{L^2(\Sigma)}^2.
\end{equation}

As a next step, note that for a function $u \in L^2(\Sigma)$, we can define
\[
	u^h = \sum_{j=1}^m (u, \varphi_j)_{L^2(\Sigma)} \varphi_j
\]
and observe that
\begin{IEEEeqnarray*}{rCl}
	\iint_\Sigma u^h \varphi_i \dd x \dd t &=& \iint_\Sigma \sum_{j=1}^m (u, \varphi_j)_{L^2(\Sigma)} \varphi_j \varphi_i \dd x \dd t \\
	&=& \sum_{j=1}^m (u, \varphi_j)_{L^2(\Sigma)} \iint_\Sigma \varphi_j \varphi_i \dd x \dd t \\
	&=& \sum_{j=1}^m (u, \varphi_j)_{L^2(\Sigma)} \delta_{ij} \\
	&=& (u, \varphi_i)_{L^2(\Sigma)} \\
	&=& \iint_\Sigma u \varphi_i \dd x \dd t.
\end{IEEEeqnarray*}
As of such, $S_h u^h = S_h u$. Therefore, if there is an optimal solution $u_h \in L^2(\Sigma)$, there is also an optimal solution attaining the same value in the subspace $\Shp(\meshT_N)$.

The approach to look for $u_h$ in a non-discretized space is called variational discretization, see \cite{Hinze}.
In order to discuss uniqueness and existence of solutions to these problems, we can use the following, general result:
\begin{theorem}
\label{thm:optimal-control-existence}
Let $\{ U, \| \cdot \|_U \}$ and $\{ H, \| \cdot \|_H \}$ be real Hilbert spaces and a nonempty, closed and convex set $U_{ad} \subset U$, an element $z \in H$ and a constant $\lambda > 0$.
Moreover, set $S : U \to H$ be a linear and continuous operator.
Then the quadratic optimization problem in the Hilbert space
\[
	\min_{u \in U_{ad}} f(u) \coloneqq \frac{1}{2} \| Su - z \|_H^2 + \frac{\lambda}{2} \| u \|_U^2
\]
admits an optimal solution $\bar{u}$. If $\lambda$ is positive or $S$ injective, then is is uniquely determined.
\end{theorem}
\begin{proof}
See \cite[Satz 2.14]{Troeltzsch} and \cite[Satz 2.15]{Troeltzsch}.

Assume $U_{ad}$ is bounded. Because $f \geq 0$, the infimum of all possible function values exists:
\[
	j \coloneqq \inf_{u \in U_{ad}} f(u).
\]
Hence, a sequence $\{ u_n \}_{n=1}^\infty \subset U_{ad}$ with $f(u_n) \to j$ for $n \to \infty$ exists.
Therefore one knows that $U_{ad}$ is weakly sequentially compact, i.e.\ a weakly convergent subsequence $\{ u_{n_k} \}_{k=1}^\infty \subset U_{ad}$ weakly converging against a $\bar{u} \in U_{ad}$ exists.
As $S$ is continuous and $f$ strictly convex (because $\lambda > 0$), one can conclude weak lower semi-continuity of $f$ and thus has
\[
	j \leq f(\bar{u}) \leq \min_{k \to \infty} \inf f(u_{n_k}) = j.
\]
Because $f$ is strictly convex, $\bar{u}$ is uniquely determined.

Now let $U_{ad}$ not be bounded. Because $U_{ad}$ is nonempty, $u_0 \in U_{ad}$ exists. For $\| u \|_U^2 > 2 \lambda^{-1} f(u_0)$, one has
\[
	f(u) = \frac{1}{2} \| S u - z \|_H^2 + \frac{\lambda}{2} \| u \|_U^2 \geq \frac{\lambda}{2} \| u \|_U^2 > f(u_0).
\]
Hence, one can simply search within the bounded, convex and closed set
\[
	U_{ad} \cap \{ u \in U : \| u \|_U^2 \leq 2 \lambda^{-1} f(u_0) \}
\]
and use the previously derived result.
\end{proof}
For $S$ continuity is guaranteed by \cref{thm:generalized-problem-solution} (with the choices $g_I = 0$, $g_R = \beta u$).
On the other hand, for the discrete operator $S_h$ this is guaranteed by \cref{thm:Astab-est}.
In conclusion \cref{thm:optimal-control-existence} implies that both \cref{eq:f-S} and \cref{eq:f-Sh} admit unique solutions, named $\bar{u}$ and $\bar{u}_h$, respectively.

These unique solutions can be characterized using a so called variational inequality:
\begin{theorem}
\label{thm:variational-ineq}
Let $U$ and $H$ be real Hilbert spaces, a nonempty and convex set $U_{ad} \subset U$, $z \in H$ and a constant $\lambda \geq 0$. Let $S : U \to H$ be a linear and continuous operator.
The element $\bar{u} \in U_{ad}$ solves
\[
	\min_{u \in U_{ad}} f(u) \coloneqq \frac{1}{2} \| Su - z \|_H^2 + \frac{\lambda}{2} \| u \|_U^2
\]
if and only if
\[
	( S \bar{u} - z, S ( u - \bar{u} ) )_U + \lambda ( \bar{u}, u - \bar{u} )_U \geq 0 \quad \forall u \in U_{ad}.
\]
\end{theorem}
\begin{proof}
See \cite[Satz 2.22]{Troeltzsch} for this statement.
\end{proof}
By using the adjoint operator $S^*$, one can characterize the solution of an optimal control problem using the so called adjoint state:
\begin{IEEEeqnarray*}{r"rCl"l}
	& ( S \bar{u} - z, S ( u - \bar{u} ) )_U + \lambda ( \bar{u}, u - \bar{u} )_U &\geq& 0 & \forall u \in U_{ad}, \\
	\Longleftrightarrow & ( S^* ( S \bar{u} - z ) + \lambda \bar{u}, u - \bar{u} )_U &\geq& 0 & \forall u \in U_{ad}.
\end{IEEEeqnarray*}
Hence, one can define the solution by making use of the adjoint state $p \coloneqq  S^* ( S \bar{u} - z ) $.
For the parabolic case, it is however easier to work with the form given in \cref{thm:variational-ineq} and proving equivalence for a reasonably defined $p$ rather than attempting to determine $S^*$ directly.

For the boundary control problem, one can show that the solution of the the adjoint problem defined as follows
\begin{equation}
\label{eq:generalized-problem-adj}
\begin{IEEEeqnarraybox}[][c]{rCl"l}
-\frac{\partial p}{\partial t} (x, t) - \lapl p(x, t) &=& a_Q(x, t) & \text{for } (x, t) \in \Omega, \\
n_x(x,t) \cdot \nabla_x p(x, t) + \alpha p(x, t) &=& a_\Sigma(x, t) & \text{for } (x, t) \in \Sigma, \\
p(x, T) &=& a_\Omega(x) & \text{for } (x, t) \in \Omega.
\end{IEEEeqnarraybox}
\end{equation}
characterizes the solution $\bar{u} \in U_{ad}$ equivalently to \cref{thm:variational-ineq} by
\[
	( \beta \bar{p} + \lambda \bar{u}, u - \bar{u} )_U \geq 0 \quad \forall u \in U_{ad}.
\]
Refer to \cite[Lemma 3.17, Satz 3.18, p.\ 126f.]{Troeltzsch} for this result.

Nonetheless, we have to derive a similarly defined adjoint state for the discrete operator.
For this, we pick special right hand sides for the \cref{eq:generalized-problem} where each boundary value depends on a control, called $v$, $u$ and $w$, respectively. The motivation behind this is so that we can transfer the result on the adjoint to the interior heat source problem.
\begin{equation}
\label{eq:generalized-problem-state}
\begin{IEEEeqnarraybox}[][c]{rCl"l}
\frac{\partial y}{\partial t} (x, t) - \lapl y(x, t) &=& b_Q(x, t) v & \text{for } (x, t) \in \Omega, \\
n_x(x,t) \cdot \nabla_x y(x, t) + \alpha y(x, t) &=& b_\Sigma(x, t) u & \text{for } (x, t) \in \Sigma, \\
y(x, 0) &=& b_\Omega(x) w & \text{for } (x, t) \in \Omega.
\end{IEEEeqnarraybox}
\end{equation}
By applying the discontinuous Galerkin formulation for \cref{eq:generalized-problem-state} we obtain
\begin{equation}
\label{eq:generalized-problem-state-dG}
A_h \boldsymbol{y} = \boldsymbol{g_I}(b_Q v) + \boldsymbol{g_R}(b_\Sigma u) + \boldsymbol{y_0}(b_\Omega w).
\end{equation}
and by treating \cref{eq:generalized-problem-adj} accordingly, we obtain
\begin{equation}
\label{eq:generalized-problem-adj-dG}
A_h^\tp \boldsymbol{p} = \boldsymbol{g_I}(a_Q) + \boldsymbol{g_R}(a_\Sigma) + \boldsymbol{y_T}(a_\Omega),
\end{equation}
where $\boldsymbol{y_T} : L^2(\Omega) \to \R^m$ is defined in an analogous fashion as before:
\begin{IEEEeqnarray*}{rCl}
	\boldsymbol{y_T}[i] (y_T) &=& \int_\Omega y_T \varphi_i(\cdot, T) \dd x \dd t.
\end{IEEEeqnarray*}
One would assert that \cref{eq:generalized-problem-adj-dG} is the adjoint in the given sense to \cref{eq:generalized-problem-state-dG}, because \cref{eq:generalized-problem-adj} is the adjoint in that sense to \cref{eq:generalized-problem-state} for the continuous case.
In order to prove this, we need the following statement:
\begin{lemma}
\label{thm:discrete-adj-state-helper}
The solution $y_h$ of \cref{eq:generalized-problem-state-dG} and $p_h$ of\cref{eq:generalized-problem-adj-dG} adhere to the following equation:
\[
\begin{IEEEeqnarraybox}[][c]{l}
	\iint_Q b_Q v p_h \dd x \dd t + \int_\Omega b_\Omega w p_h(\cdot, 0) \dd x + \iint_{\Sigma_R} b_\Sigma u p_h \dd s(x) \dd t \\
	\IEEEeqnarraymulticol{1}{r}{ \qquad\qquad {} = \iint_Q a_Q y_h \dd x \dd t + \int_\Omega a_\Omega y_h(\cdot, T) \dd x + \iint_{\Sigma_R} a_\Sigma y_h \dd s(x) \dd t. }
\end{IEEEeqnarraybox}
\]
\end{lemma}
\begin{proof}
Our formulation for the discretization of \cref{eq:generalized-problem-state} and \cref{eq:generalized-problem-adj} are respectively:
\begin{IEEEeqnarray*}{rCl}
	a(y_h, v_h) + b(y_h, v_h) &=& \iint_Q b_Q v v_h \dd x \dd t + \int_\Omega b_\Omega w v_h(\cdot, 0) \dd x + \iint_{\Sigma_R} b_\Sigma u v_h \dd s(x) \dd t, \\
	a(v_h, p_h) + b(v_h, p_h) &=& \iint_Q a_Q v_h \dd x \dd t + \int_\Omega a_\Omega v_h(\cdot, T) \dd x + \iint_{\Sigma_R} a_\Sigma v_h \dd s(x) \dd t,
\end{IEEEeqnarray*}
where each equation holds for all $v_h \in \Shp(\meshT_N)$.
We test the first equation with $p_h$ and the second one with $y_h$:
\begin{IEEEeqnarray*}{rCl}
	a(y_h, p_h) + b(y_h, p_h) &=& \iint_Q b_Q v p_h \dd x \dd t + \int_\Omega b_\Omega w p_h(\cdot, 0) \dd x + \iint_{\Sigma_R} b_\Sigma u p_h \dd s(x) \dd t \\
	a(y_h, p_h) + b(y_h, p_h) &=& \iint_Q a_Q y_h \dd x \dd t + \int_\Omega a_\Omega y_h(\cdot, T) \dd x + \iint_{\Sigma_R} a_\Sigma y_h \dd s(x) \dd t.
\end{IEEEeqnarray*}
Noticing that the left hand sides of both equations are the same gives us the desired statement:
\[
\begin{IEEEeqnarraybox}[][c]{l}
	\iint_Q b_Q v p_h \dd x \dd t + \int_\Omega b_\Omega w p_h(\cdot, 0) \dd x + \iint_{\Sigma_R} b_\Sigma u p_h \dd s(x) \dd t \\
	\IEEEeqnarraymulticol{1}{r}{ \qquad\qquad {} = \iint_Q a_Q y_h \dd x \dd t + \int_\Omega a_\Omega y_h(\cdot, T) \dd x + \iint_{\Sigma_R} a_\Sigma y_h \dd s(x) \dd t. }
\end{IEEEeqnarraybox}
\]
\end{proof}

For the boundary control problem, we define the adjoined state $p_h$ as the solution of
\begin{equation}
\label{eq:adjoined-discrete}
	a(v_h, p_h) + b(v_h, p_h) = \int_\Omega (y(T) - y_\Omega) y_h(\cdot, T) \dd x \quad \forall v_h \in \Shp(\meshT_N).
\end{equation}
Using \cref{thm:discrete-adj-state-helper}, we can prove:
\begin{theorem}
\label{eq:discrete-variational-ineq}
A control $\bar{u}_h \in L^2(\Omega)$ is optimal for \cref{eq:generalized-problem-state-dG} if and only if with the associated adjoined state $\bar{p}_h$ defined as the solution of \cref{eq:generalized-problem-adj-dG} fulfills the following variational inequality
\[
	( \beta(x, t) \bar{p}_h (x, t) + \lambda \bar{u}_h(x, t), u_h - \bar{u}_h )_= 0
\]
almost everywhere.
\end{theorem}
\begin{proof}
This proof is very similar to \cite[Satz 3.19, p.\ 128f.]{Troeltzsch}.
Using the previously derived minimization formulation \cref{eq:f-Sh} and employing \cref{thm:variational-ineq}, we know
\begin{IEEEeqnarray*}{rCl}
	0 &\leq& ( S_h \bar{u}_h - z_h, S_h(u_h - \bar{u}_h) )_{L^2(\Omega)} + \lambda(\bar{u}_h, u_h - \bar{u}_h)_{L^2(\Sigma)} \\
	&=& \int_\Omega (\bar{y}_h(T) - y_\Omega)(y_h(T) - \bar{y}_h(T)) \dd x + \lambda \iint_\Sigma \bar{u}_h ( u_h - \bar{u}_h ) \dd s(x) \dd  t.
\end{IEEEeqnarray*}
for all $u_h \in U_{ad}$ and their associated states $y_h$.
Note that
\[
	S_h u_h - S_h \bar{u}_h = S_h u_h + z_h - S_h \bar{u}_h - z_h = y_h(T) - \bar{y}_h(T).
\]
Using \cref{eq:aid-variational-ineq} with $b_Q = 0$, $b_\Omega = 0$, $a_Q = 0$, $a_\Sigma = 0$, $a_\Omega = \bar{y}_h - y_\Omega$, $b_\Sigma = \beta$, we have
\[
	\iint_\Sigma \beta p_h \tilde{u}_h \dd s(x) \dd t = (\bar{y}_h(T) - \Omega, \tilde{y}_h(T))_{L^2(\Omega)},
\]
where we set $\tilde{u}_h = u_h - \bar{u}_h$ and $\tilde{y}_h = y_h - \bar{y}_h$.
Inserting this into the variational inequality we obtain
\begin{IEEEeqnarray*}{rCl}
	0 &\leq& \int_\Omega (\bar{y}_h(T) - y_\Omega)(y_h(T) - \bar{y}_h(T)) \dd x + \lambda \iint_\Sigma \bar{u}_h ( u_h - \bar{u}_h ) \dd s(x) \dd  t \\
	&=& \iint_\Sigma \beta p_h ( u_h - \bar{u}_h ) \dd s(x) \dd t + \lambda \iint_\Sigma \bar{u}_h ( u_h - \bar{u}_h ) \dd s(x) \dd  t \\
	&=& \iint_\Sigma ( \beta p_h + \lambda \bar{u}_h ) ( u_h - \bar{u}_h ) \dd s(x) \dd  t \\
\end{IEEEeqnarray*}
Due to the choice $U_{ad} = L^2(\Omega)$, we know this holds in the stronger sense
\[
	\beta p_h + \lambda \bar{u}_h = 0
\]
almost everywhere.
\end{proof}
The proof of \cref{eq:discrete-variational-ineq} shows that we can identify
\[
	p_h = S_h^*( S_h u_h - z_h ),
\]
alas $S_h^*$ is the solution operation analogous to $S_h$ stemming from the discontinuous Galerkin method applied to \cref{eq:dg-adjoint-prob}.
Overall, we have the following optimality system for the continuous case:
\begin{IEEEeqnarray*}{c}
\begin{IEEEeqnarraybox}{r"l}
\begin{IEEEeqnarraybox}{rCl}
\frac{\partial y}{\partial t} - \lapl y &=& 0 \\
\partial_\nu y + \alpha y &=& - \beta^2 \lambda^{-1} p \\
y(0) &=& y_0
\end{IEEEeqnarraybox} & 
\begin{IEEEeqnarraybox}{rCl}
-\frac{\partial p}{\partial t} - \lapl p &=& 0 \\
\partial_\nu p + \alpha p &=& 0 \\
p(T) &=& y(T) - y_\Omega
\end{IEEEeqnarraybox}
\end{IEEEeqnarraybox} \\
u = - \lambda^{-1} \beta p
\end{IEEEeqnarray*}
and the analogous for the discrete case:
\begin{IEEEeqnarray*}{c}
\begin{IEEEeqnarraybox}{rCl"rCl}
A(y_h, v_h) &=& \langle \beta u_h, v_h \rangle_{L^2(\Sigma_R)} & A(v_h, p_h) &=& \langle y_h(T) - y_\Omega, v_h \rangle_{L^2(\Sigma_T)}
\end{IEEEeqnarraybox} \\
u_h = - \lambda^{-1} \beta p_h
\end{IEEEeqnarray*}
We observe two things that may not come as a surprise: Firstly, our discrete optimality system is equivalent to applying the dG method to the state and adjoint equation of the continuous case individually, and secondly that the adjoint operator of the discrete state equation uses the transposed matrix.

With this, we have shown that the method we're working with is well defined and yields a solution to an optimal control problem similar to the continuous one. In a further step, we would like to prove convergence of the discrete problem.
Given that \cref{thm:dg-convergence} requires at least $y \in H^2(\meshT_N) \subset H^2(Q)$, we have to make the following assumption:
\begin{assumption}
\label{as:continuous-Hs-regularity}
The solution of the system \cref{eq:generalized-weak-form} lies in $H^s(Q) \cap W(0,T)$ with $s \geq 2$, i.e.
\[
	\| y \|_{H^s(Q)} \leq c_w' \left( \| g_I \|_{L^2(Q)} + \| g_R \|_{L^2(\Sigma)} + \| y_0 \|_{L^2(\Omega)} \right)
\]
with a constant $c_w' > 0$ independent of $g_I$, $g_R$ and $y_0$.
\end{assumption}
\begin{theorem}
Let $\bar{u}$ be the optimal control for the continuous problem and let \cref{as:continuous-Hs-regularity} hold. Moreover, be $\bar{u}_h$ the discretized solution.
Then there is a $C > 0$ independent of $h$, such that:
\[
	\| \bar{u} - \bar{u}_h \|_{L^2(\Sigma)} + \| \bar{y}(T) - \bar{y}_h(T) \|_{L^2(\Omega)} \leq C h^{\min \{ s, p+1\} - 1} \left( \| \bar{u} \|_{L^2(\Sigma)} + \| y_\Omega \|_{L^2(\Omega)} \right).
\]
\end{theorem}
\begin{proof}
The optimal solutions to both problems fulfill
\begin{IEEEeqnarray*}{rCl"l}
	0 &=& ( S \bar{u} - z, S v )_{L^2(\Omega)} + \lambda(\bar{u}, v)_{L^2(\Sigma)} & \text{for all } v \in L^2(\Sigma) \\
	0 &=& ( S_h \bar{u}_h - z_h, S_h v )_{L^2(\Omega)} + \lambda(\bar{u}_h, v)_{L^2(\Sigma)} & \text{for all } v \in L^2(\Sigma)
\end{IEEEeqnarray*}
We perform a zero addition of the second term evaluated with $v = \bar{u}$:
\begin{IEEEeqnarray*}{rCl}
	0 &=& ( S \bar{u} - z, S v )_{L^2(\Omega)} + \lambda(\bar{u}, v)_{L^2(\Sigma)} \\
	&& {} - \left[ ( S_h \bar{u} - z_h, S_h v )_{L^2(\Omega)} + \lambda(\bar{u}, v)_{L^2(\Sigma)} \right] \\
	&& {} + \left[ ( S_h \bar{u} - z_h, S_h v )_{L^2(\Omega)} + \lambda(\bar{u}, v)_{L^2(\Sigma)} \right] \\
	&& {} - ( S_h \bar{u}_h - z_h, S_h v )_{L^2(\Omega)} - \lambda(\bar{u}_h, v)_{L^2(\Sigma)} 
\end{IEEEeqnarray*}
Moreover, we calculate
\begin{IEEEeqnarray*}{rCl}
	\IEEEeqnarraymulticol{3}{l}{ ( S \bar{u} - z, S v )_{L^2(\Omega)} + \lambda(\bar{u}, v)_{L^2(\Sigma)} - \left[ ( S_h \bar{u} - z_h, S_h v )_{L^2(\Omega)} + \lambda(\bar{u}, v)_{L^2(\Sigma)} \right] } \\
	&=& ( S \bar{u} - z, S v )_{L^2(\Omega)} - ( S_h \bar{u} - z_h, S_h v )_{L^2(\Omega)} \\
	&=& ( S \bar{u} - z - [ S_h \bar{u} - z_h ], S v )_{L^2(\Omega)} - ( S_h \bar{u} - z_h, S_h v -S v )_{L^2(\Omega)} \\
	&=& ( S \bar{u} + G_0 y_0 - y_\Omega - S_h \bar{u} - G_0^h y_0 + y_\Omega, S v )_{L^2(\Omega)} \\
	&& \quad {} - ( S_h \bar{u} + G_0^h y_0 - y_\Omega, S_h v -S v )_{L^2(\Omega)} \\
	&=& ( \bar{y}(T) - \bar{y}^h(T) , S v )_{L^2(\Omega)} - ( \bar{y}^h(T) - y_\Omega, S_h v -S v )_{L^2(\Omega)} \\
	&\leq& \left| ( \bar{y}(T) - \bar{y}^h(T) , y^v(T) )_{L^2(\Omega)} \right| + \left| ( \bar{y}^h(T) - y_\Omega, S_h v -S v )_{L^2(\Omega)} \right| \\
	&\leq& \left\| \bar{y}(T) - \bar{y}^h(T) \right\|_{L^2(\Omega)} \| y^v(T) \|_{L^2(\Omega)} \\
	&& \quad {}+ \left\| \bar{y}^h(T) - y_\Omega \right\|_{L^2(\Omega)} \left\| y^v_h(T) - y_v(T) \right\|_{L^2(\Omega)} \\
	&\leq& \left\| \bar{y} - \bar{y}^h \right\|_{L^2(\Sigma_T)} \| y^v \|_{L^2(\Sigma_T)} + \left( \left\| \bar{y}^h \right\|_{L^2(\Sigma_T)} + \| y_\Omega \|_{L^2(\Omega)} \right) \left\| y^v_h - y_v \right\|_{L^2(\Sigma_T)} \\
	&\overset{\text{\cref{eq:dg-DG-norm}}}{\leq}& \left\lDG \bar{y} - \bar{y}^h \right\rDG \| y^v \|_{L^2(\Sigma_T)} + \left( \left\lDG \bar{y}^h \right\rDG + \| y_\Omega \|_{L^2(\Omega)} \right) \left\lDG y^v_h - y_v \right\rDG \\
	&\overset{\text{\cref{thm:generalized-problem-solution}}}{\leq}& c_w \left\lDG \bar{y} - \bar{y}^h \right\rDG \| v \|_{L^2(\Sigma)} + \left( \left\lDG \bar{y}^h \right\rDG + \| y_\Omega \|_{L^2(\Omega)} \right) \left\lDG y^v_h - y_v \right\rDG \\
	&\overset{\text{\cref{thm:Astab-est}}}{\leq}& c_w \left\lDG \bar{y} - \bar{y}^h \right\rDG \| v \|_{L^2(\Sigma)} + \left( \frac{1}{c_S^A} \beta \| \bar{u} \|_{L^2(\Sigma)} + \| y_\Omega \|_{L^2(\Omega)} \right) \left\lDG y^v_h - y_v \right\rDG \\
	&\overset{\text{\cref{thm:dg-convergence}}}{\leq}& c_w c \max\{1, \alpha\} h^{\min \{ s, p+1\} - 1} |\bar{y}|_{H^s(\meshT_N)} \| v \|_{L^2(\Sigma)} \\
	&& \quad {}+ \left( \frac{1}{c_S^A} \beta \| \bar{u} \|_{L^2(\Sigma)} + \| y_\Omega \|_{L^2(\Omega)} \right) c \max\{1, \alpha\} h^{\min \{ s, p+1\} - 1} |y_v|_{H^s(\meshT_N)} \\
	&\overset{\text{\cref{as:continuous-Hs-regularity}}}{\leq}& c_w c_w' c \max\{1, \alpha\} h^{\min \{ s, p+1\} - 1} \|\bar{u}\|_{L^2(\Sigma)} \| v \|_{L^2(\Sigma)} \\
	&& \quad {}+ \left( \frac{1}{c_S^A} \beta \| \bar{u} \|_{L^2(\Sigma)} + \| y_\Omega \|_{L^2(\Omega)} \right) c c_w' \max\{1, \alpha\} h^{\min \{ s, p+1\} - 1} \|v\|_{L^2(\Sigma)} \\
	&\leq& C h^{\min \{ s, p+1\} - 1} \left( \| \bar{u} \|_{L^2(\Sigma)} + \| y_\Omega \|_{L^2(\Omega)} \right) \| v \|_{L^2(\Sigma)}
\end{IEEEeqnarray*}
For the remaining two terms, we have
\begin{IEEEeqnarray*}{rCl}
	\IEEEeqnarraymulticol{3}{l}{ \left[ ( S_h \bar{u} - z_h, S_h v )_{L^2(\Omega)} + \lambda(\bar{u}, v)_{L^2(\Sigma)} \right] - ( S_h \bar{u}_h - z_h, S_h v )_{L^2(\Omega)} - \lambda(\bar{u}_h, v)_{L^2(\Sigma)} } \\
	\qquad &=& (S_h (\bar{u} - \bar{u}_h), S_h v)_{L^2(\Omega)} + \lambda (\bar{u} - \bar{u}_h, v)_{L^2(\Sigma)}
\end{IEEEeqnarray*}
Choosing the test function $v = \bar{u} - \bar{u}_h$, we can estimate:
\begin{IEEEeqnarray*}{rCl}
	(S_h (\bar{u} - \bar{u}_h), S_h (\bar{u} - \bar{u}_h))_{L^2(\Omega)} + \lambda (\bar{u} - \bar{u}_h, \bar{u} - \bar{u}_h)_{L^2(\Sigma)} &\geq& \lambda (\bar{u} - \bar{u}_h, \bar{u} - \bar{u}_h)_{L^2(\Sigma)} \\
	&=& \lambda \| \bar{u} - \bar{u}_h \|_{L^2(\Sigma)}^2
\end{IEEEeqnarray*}
Bringing these together, we obtain:
\begin{IEEEeqnarray*}{rCl}
	0 &\geq& ( S \bar{u} - z, S ( \bar{u} - \bar{u}_h ) )_{L^2(\Omega)} + \lambda(\bar{u}, \bar{u} - \bar{u}_h)_{L^2(\Sigma)} \\
	&& {} - \left[ ( S_h \bar{u} - z_h, S_h (\bar{u} - \bar{u}_h) )_{L^2(\Omega)} + \lambda(\bar{u}, \bar{u} - \bar{u}_h)_{L^2(\Sigma)} \right] \\
	&& {} + \lambda \| \bar{u} - \bar{u}_h \|_{L^2(\Sigma)}^2
\end{IEEEeqnarray*}
By bringing the terms to the other side of the equation and taking the absolute, we can apply the previously derived estimate:
\begin{IEEEeqnarray*}{rCl}
	\IEEEeqnarraymulticol{3}{l}{ C h^{\min \{ s, p+1\} - 1} \left( \| \bar{u} \|_{L^2(\Sigma)} + \| y_\Omega \|_{L^2(\Omega)} \right) \| \bar{u} - \bar{u}_h \|_{L^2(\Sigma)} } \\
	\qquad &\geq& ( S \bar{u} - z, S ( \bar{u} - \bar{u}_h ) )_{L^2(\Omega)} + \lambda(\bar{u}, \bar{u} - \bar{u}_h)_{L^2(\Sigma)} \\
	&& \quad {} - \left[ ( S_h \bar{u} - z_h, S_h (\bar{u} - \bar{u}_h) )_{L^2(\Omega)} + \lambda(\bar{u}, \bar{u} - \bar{u}_h)_{L^2(\Sigma)} \right] \\
	&\geq& \lambda \| \bar{u} - \bar{u}_h \|_{L^2(\Sigma)}^2.
\end{IEEEeqnarray*}
Dividing by $\| \bar{u} - \bar{u}_h \|_{L^2(\Sigma)}$ leaves us with
\[
	C h^{\min \{ s, p+1\} - 1} \left( \| \bar{u} \|_{L^2(\Sigma)} + \| y_\Omega \|_{L^2(\Omega)} \right) \geq \lambda \| \bar{u} - \bar{u}_h \|_{L^2(\Sigma)}.
\]

In a further step, we have to estimate $\| \bar{y}(T) - \bar{y}_h(T) \|_{L^2(\Omega)}$:
\begin{IEEEeqnarray*}{rCl}
\| \bar{y}(T) - \bar{y}_h(T) \|_{L^2(\Omega)} &=& \| S \bar{u} - S_h \bar{u}_h \|_{L^2(\Omega)} \\
&\leq& \| S \bar{u} - S_h \bar{u} \|_{L^2(\Omega)} + \| S_h \bar{u} - S_h \bar{u}_h \|_{L^2(\Omega)} \\
&\leq& \| y(T) - y^h(T) \|_{L^2(\Omega)} + \| y^h(T) - y_h(T) \|_{L^2(\Omega)} \\
&\leq& c \max\{1, \alpha\} h^{\min \{ s, p+1\} - 1} |\bar{y}|_{H^s(\meshT_N)} + \frac{1}{c_S^A} \beta \| \bar{u} - \bar{u}_h \|_{L^2(\Sigma)} \\
&\leq& C h^{\min \{ s, p+1\} - 1} \| \bar{u} \|_{L^2(\Sigma)} \\
&& \quad {} + C' h^{\min \{ s, p+1\} - 1} \left( \| \bar{u} \|_{L^2(\Sigma)} + \| y_\Omega \|_{L^2(\Omega)} \right)
\end{IEEEeqnarray*}
Together, the estimates prove the claim made.
\end{proof}
\end{document}