\documentclass[../thesis.tex]{subfiles}

\begin{document}
\chapter{Discretization of the control problem}
\label{sec:Disc-Control-Problem}
In this chapter, we're going back to the optimal control problem introduced in \cref{sec:OptimalControlProblem}.
We are going to restrict ourselves to the boundary and inner heating problems with no restrictions made to the control itself, i.e.\ $u_a = -\infty$ and $u_b = \infty$.
\section{Existence of solutions for a generalized problem}
We're going to work with weak solutions in appropriate spaces. To this end, we introduce a generalized form of the state equation, that can cover both, \cref{eq:BoundaryOptimalControl,eq:InnerOptimalControl}:
\begin{equation}
\label{eq:generalized-problem}
\begin{IEEEeqnarraybox}[][c]{rCl"lCl}
\frac{\partial y}{\partial t} (x, t) - \lapl y(x, t) &=& g_I(x, t) & \text{for } (x, t) \in Q &\coloneqq& \Omega \times (0, T), \\
n_x(x,t) \cdot \nabla_x y(x, t) + \alpha y(x, t) &=& g_R(x, t) & \text{for } (x, t) \in \Sigma &\coloneqq& \partial \Omega \times (0, T) \\
y(x, 0) &=& y_0(x) & \text{for } (x, t) \in \Omega,
\end{IEEEeqnarraybox}
\end{equation}
where we assume that $\Omega$ is a bounded Lipschitz domain, $T > 0$ is a fixed time and $\alpha \in L^\infty(\Sigma)$, $\alpha(x, t) \geq 0$ almost everywhere. The functions $g_I(x, t) \in L^2(Q)$ and $g_R(x, t) \in L^2(\Sigma)$ act as generic right hand sides.

Formally, we test the first equation of \cref{eq:generalized-problem} by multiplying with a function $v$ and integrating over $Q$:
\begin{IEEEeqnarray*}{rCl}
	\iint_Q \left(\frac{\partial y}{\partial t} - \lapl y\right) v \dd x \dd t &=& \iint_Q \frac{\partial y}{\partial t} v \dd x \dd t - \iint_Q \lapl y v \dd x \dd t \\
	&=& \iint_Q g_I v \dd x \dd t
\end{IEEEeqnarray*}
We proceed by splitting the integral on the left hand side and consider each part individually.
For the term containing the time derivative, we then obtain using integration by parts and using that $y(\cdot, 0) = y_0(\cdot)$:
\begin{IEEEeqnarray*}{rCl}
	\iint_Q \frac{\partial y}{\partial t} v \dd x \dd t &=& - \iint_Q y \frac{\partial v}{\partial t} \dd x \dd t - \int_\Omega y(\cdot, 0) v(\cdot, 0) \dd x + \int_\Omega y(\cdot, T) v(\cdot, T) \dd x \\
	&=& - \iint_Q y \frac{\partial v}{\partial t} \dd x \dd t + \int_\Omega y(\cdot, T) v(\cdot, T) \dd x - \int_\Omega y_0(\cdot) v(\cdot, 0) \dd x.
\end{IEEEeqnarray*}
In order to deal with the term containing $\lapl y$, we use Green's first identity and exploit the given normal derivative on the boundary:
\begin{IEEEeqnarray*}{rCl}
	- \iint_Q \lapl y v \dd x \dd t &=& \iint_Q \nabla y \cdot \nabla v \dd x \dd t - \iint_{\Sigma} (\nabla y v) \cdot \nu \dd s(x) \dd t \\
	&=& \iint_Q \nabla y \cdot \nabla v \dd x \dd t - \iint_{\Sigma} \left(-\alpha y - g_R \right) v \dd s(x) \dd t \\
	&=& \iint_Q \nabla y \cdot \nabla v \dd x \dd t + \alpha \iint_{\Sigma} y v \dd s(x) \dd t - \iint_{\Sigma} g_R v \dd s(x) \dd t.
\end{IEEEeqnarray*}

Adding these two terms up again yields
\begin{IEEEeqnarray*}{rCl}
	\iint_Q \left(\frac{\partial y}{\partial t} - \lapl y\right) v \dd x \dd t &=& \iint_Q \nabla y \cdot \nabla v \dd x \dd t - \iint_Q y \frac{\partial v}{\partial t} \dd x \dd t \\
	&& \quad {} + \alpha \iint_{\Sigma} y v \dd s(x) \dd t + \int_\Omega y(\cdot, T) v(\cdot, T) \dd x \\
	&& \quad {} - \int_\Omega y_0(\cdot) v(\cdot, 0) \dd x  - \iint_{\Sigma} g_R v \dd s(x) \dd t.
\end{IEEEeqnarray*}
Reorganizing the terms gives us the formulation
\begin{equation}
\label{eq:generalized-weak-form}
\begin{IEEEeqnarraybox}[][c]{l}
	\iint_Q \nabla y \cdot \nabla v \dd x \dd t - \iint_Q y \frac{\partial v}{\partial t} \dd x \dd t + \alpha \iint_{\Sigma} y v \dd s(x) \dd t + \int_\Omega y(\cdot, T) v(\cdot, T) \dd x \qquad \\
	\IEEEeqnarraymulticol{1}{r}{ {} = \iint_Q g_I v \dd x \dd t + \int_\Omega y_0 v(\cdot, 0) \dd x + \iint_{\Sigma} g_R v \dd s(x) \dd t }.
\end{IEEEeqnarraybox}
\end{equation}
While we have only used a formal function $v$, we conclude that in order for these transformations to work, and Green's first identity and integration by parts to be applicable we need a few requirements:
$Q$ must be a bounded domain with a piecewise smooth boundary, $y$ must be twice continuously differentiable in space and once continuously differentiable in time, whereas $v$ has to be continuously differentiable in both space and time directions.

Before proceeding, we consider the following partial differential equation as well:
\begin{equation}
\label{eq:generalized-adjoint-problem}
\begin{IEEEeqnarraybox}[][c]{rCl"lCl}
- \frac{\partial p}{\partial t} (x, t) - \lapl p(x, t) &=& d_I(x, t) & \text{for } (x, t) \in Q, \\
n_x(x,t) \cdot \nabla_x p(x, t) + \alpha p(x, t) &=& d_R(x, t) & \text{for } (x, t) \in \Sigma, \\
p(x, T) &=& p_T(x) & \text{for } (x, t) \in \Omega,
\end{IEEEeqnarraybox}
\end{equation}
where $d_I(x, t) \in L^2(Q)$ and $d_R(x, t) \in L^2(\Sigma)$ are left as generic right hand side functions for now.
The motivation for considering this problem is that it will turn out to play an important role for defining the so called adjoint state en route to an optimality system for \cref{eq:BoundaryOptimalControl,eq:InnerOptimalControl}.

One could perform the analogous procedure we just performed for $p$.
Instead of doing that, we consider the function $\tilde{p}(x, t) \coloneqq p(x, T - t)$.
Observing that we have
\[
	\lapl \tilde{p}(x, t) = \lapl p(x, T - t) \quad \text{and} \quad \frac{\partial \tilde{p}}{\partial t}(x, t) = - \frac{\partial p}{\partial t}(x, T - t),
\] 
the boundary value problem for $p$ can be written instead as
\begin{IEEEeqnarray*}{rCl}
\frac{\partial \tilde{p}}{\partial t} - \lapl \tilde{p} &=& d_I(x, T - t) \\
\partial_\nu \tilde{p} + \alpha \tilde{p} &=& d_R(x, T - t) \\
\tilde{p}(0) &=& p(T) \\
p(x, t) &=& \tilde{p}(x, T - t)
\end{IEEEeqnarray*}
by applying the results for $y$, and substituting $\tilde{p}$ again, one arrives at the following weak formulation:
\begin{IEEEeqnarray*}{l}
	\iint_Q \nabla p \cdot \nabla v \dd x \dd t + \iint_Q p \frac{\partial v}{\partial t} \dd x \dd t + \alpha \iint_{\Sigma} p v \dd s(x) \dd t + \int_\Omega p(\cdot, 0) v(\cdot, 0) \dd x \qquad\qquad \\
	\IEEEeqnarraymulticol{1}{r}{ {} = \int_\Omega p(T) v(\cdot, T) \dd x + \iint_Q d_I v \dd x \dd t + \iint_\Sigma d_R v \dd s(x) \dd t }.
\end{IEEEeqnarray*}
Assuming that $\Omega$ is bounded Lipschitz domain and that $\alpha \in L^\infty(\Sigma)$, $\alpha (x, t) \geq 0$ almost everywhere in $\Sigma$, one can prove that these formal calculations can be put into a setting where they make sense and have a unique solution, which continuously depends on the right hand side.
To this end, one defines two different spaces for $v$ and $y$, one which has weak continuous space derivatives and one which has both, weak continuous space and time derivatives (see \cite[p.\ 111]{Troeltzsch} for the definition of the spaces and \cite[Satz 3.9, p.\ 112]{Troeltzsch} for the existence and uniqueness theorem).
This however is not a result directly useful to us, as we would like to seek $v$ and $y$ in the same space. As of such, we're going to skip this discussion here.

Instead, one can prove these solutions lie in a more general space in which is what we want to work with. As of such, we're first going to introduce that space, following \cite[3.4.1 Abstrakte Funktionen, p.\ 113ff.]{Troeltzsch}.
Further information on these spaces can be found for example in \cite{Wloka} or in \cite{HillePhillips}.
\begin{definition}[Abstract functions]
Let $\{ X, \| \cdot \|_X \}$ be a real Banach space.
An mapping of $[a, b] \subset \R$ into $X$ is called abstract function.
\end{definition}
We proceed to define some spaces of abstract functions that we will work with:
\begin{definition}
Let $\{ X, \| \cdot \|_X \}$ be a real Banach space.
An abstract function $y : [a, b] \to X$ is called continuous in the point $t \in [a, b]$, if $\tau \to t, \tau \in [a, b]$ implies the convergence $y(\tau) \to y(t)$ in X.

We denote by $C([a, b], X)$ the Banach space of all continuous abstract functions $y : [a, b] \to X$, equipped with the norm
\[
	\| y \|_{C([a, b], X)} = \max_{t \in [a, b]} \| y(t) \|_X.
\]
\end{definition}
As a next step, we aim to introduce $L^p$-spaces of abstract functions.
For this purpose, we require a notion of measurable functions in these spaces.
\begin{definition}
An abstract function $y : [a, b] \to X$ is called step function if there exist finitely many elements $y_i \in X$ and Lebesgue-measurable, pairwise disjoint sets $M_i \subset [a, b]$, $i = 1, \ldots, m$ with $[a, b] = \bigcup_{i=1}^m M_i$ and $y(t) = y_i$ for all $t \in M_i$, $i = 1, \ldots, m$.

If for an abstract function $y : [a, b] \to X$ there exists a sequence of step functions $\{ y_k \}_{k=1}^\infty$ such that $y(t) = \lim_{k\to\infty} y_k(t)$ for almost all $t \in [a, b]$, then we call $y$ measurable.
\end{definition}
This permits us to introduce the $L^p$-spaces we need:
\begin{definition}
\label{def:Bochner-space}
For a real Banach space $\{ X, \| \cdot \|_X \}$ let $L^p(a, b; X)$, $1 \leq p < \infty$ be the linear space of the equivalence classes of measurable abstract functions $y : [a, b] \to X$ with the property
\[
	\int_a^b \| y(t) \|_X^p \dd t < \infty.
\]
The norm of this space is
\[
	\| y \|_{L^p(a, b; X)} \coloneqq \left( \int_a^b \| y(t) \|_X^p \dd t \right)^{\frac{1}{p}}.
\]
The space $L^\infty(a, b; X)$ is the space of equivalence classes of measurable abstract functions $y : [a, b] \to X$ with
\[
	\| y \|_{L^\infty(a, b; X)} \coloneqq \esssup_{[a, b]} \| y(t) \|_{X} < \infty.
\]
\end{definition}
As a next step, we need a notion of derivatives for this space. For this, we introduce vector-valued distributions, see \cite[p.\ 117]{Troeltzsch}.
However, before we can do that, we need to introduce an integral operating on the $L^p$-spaces.
In their definition we used a Lebesgue integral over the real-valued $\| \cdot \|_X$ norm, but we now need to define integrals via the means of the step functions that we just introduced:
\begin{definition}
For any of the $L^p(a, b; X)$, $p \in [1, \infty]$ spaces we define the so called Bochner-integral as follows:
Given a step function $y$, it is defined as
\[
	\int_a^b y(t) \dd t \coloneqq \sum_{i=1}^m y_i |M_i|.
\]
Note that this is an element of $X$.

On the other hand, given a measurable function $y$ with a series of step functions $\{ y_k\}_{k=1}^\infty$, we define
\[
	\int_a^b y(t) \dd t = \lim_{k \to \infty} \int_a^b y_k(t) \dd t.
\]
\end{definition}
It can be shown, see e.g.\ \cite{HillePhillips}, that the Bochner-integral converges independently of the choice of the series $\{ y_k \}_{k=1}^\infty$.

Using the Bochner-integral we can now introduced vector-valued derivatives:
\begin{definition}
For a given function $y \in L^2(0, T; X)$ we define a vector-valued distribution $\distT : C_0^\infty(0, T) \to X$ by
\[
	\distT \varphi \coloneqq \int_0^T y(t) \varphi(t) \dd t \quad \forall \varphi \in C_0^\infty(0, T).
\] 
The derivative $\distT'$ is defined by
\[
	\distT' \varphi \coloneqq - \int_0^T y(t) \varphi'(t) \dd t.
\]
If a function $w = w(t)$ in $L^1(0, T; X)$ exists with the property that
\[
	\distT' \varphi = - \int_0^T y(t) \varphi'(t) \dd t = \int_0^T w(t) \varphi(t) \dd t \quad \forall \varphi \in C_0^\infty(0, T),
\]
then we define the distributional derivative $y'(t) \coloneqq w(t)$.
\end{definition}
With these definitions, we can introduce the space we were looking for (see \cite[p.\ 118]{Troeltzsch}):
\begin{definition}
Let $W(0, T)$ be the space of all $y \in L^2(0, T; X)$ with distributional derivative $y'$ in $L^2(0, T; X^\adj)$ with the norm
\[
	\| y \|_{W(0, T)} = \left( \int_0^T ( \| y(t) \|_X^2 + \| y'(t) \|_{X^\adj}^2 ) \dd t \right)^{\frac{1}{2}} < \infty,
\]
or in other words:
\[
	W(0, T) = \left\{ y \in L^2(0, T; X) \midcolon y' \in L^2(0, T; X^\adj) \right\}.
\]
\end{definition}
Using the notion of so called Gelfand triples, the following emedding theorem can be proved:
\begin{theorem}
\label{thm:W0T-continuous-embedding}
Any $y \in W(0, T)$ can be interpreted, after a potential change on a set of zero measure, be interpreted as an element of $C([0, T], H)$.
In this sense the embedding $W(0, T) \hookrightarrow C([0, T], H)$ exists and is continuous, i.e.\ there exists a constant $c_E$ such that
\[
	\| y \|_{C([0, T], H)} \leq c_E \| y \|_{W(0, T)} \quad \forall y \in W(0, T).
\] 
\end{theorem}
\begin{proof}
The proof can be found in \cite{Wloka} or \cite{Zeidler-IIA}.
\end{proof}
In the following we always choose $X = H^1(Q)$. A weak formulation of the problem \cref{eq:generalized-problem} in these spaces reads as follows:
Let $u \in W(0, T)$ such that equation \cref{eq:generalized-weak-form}, i.e.
\begin{IEEEeqnarray*}{l}
	\iint_Q \nabla y \cdot \nabla v \dd x \dd t - \iint_Q y \frac{\partial v}{\partial t} \dd x \dd t + \alpha \iint_{\Sigma} y v \dd s(x) \dd t + \int_\Omega y(\cdot, T) v(\cdot, T) \dd x \qquad \\
	\IEEEeqnarraymulticol{1}{r}{ {} = \iint_Q g_I v \dd x \dd t + \int_\Omega y_0 v(\cdot, 0) \dd x + \iint_{\Sigma} g_R v \dd s(x) \dd t }.
\end{IEEEeqnarray*}
holds for all $v \in W(0, T)$.
As intended, this choice of spaces permits to use the same space $W(0, T)$ for $u$ and $v$.

In order to proof that this formulation makes sense, one uses the aforementioned spaces with a stronger regularity requirement first, proves existence and uniqueness of a solution in them and then proves that the solution is in $W(0, T)$ and testing with $v \in W(0, T)$ makes sense. For more details, see \cite[3.3 Schwache Lösungen in $W^{1, 0}_2(Q)$]{Troeltzsch}.
Eventually, one can prove the following result:
\begin{theorem}
\label{thm:generalized-problem-solution}
The system \cref{eq:generalized-weak-form} omits a unique solution with the estimate
\[
	\| y \|_{W(0, T)} \leq c_w ( \| g_I \|_{L^2(Q)} + \| g_R \|_{L^2(\Sigma)} + \| y_0 \|_{L^2(\Omega)} )
\] 
with a constant $c_w > 0$ independent of $g_I$, $g_R$ and $y_0$.
Consequentially, the mapping $(g_I, g_R, y_0) \mapsto f$ is continuous from $L^2(Q) \times L^2(\Sigma) \times L^2(\Omega)$ to $W(0, T)$, especially $C([0, T], L^2(\Omega))$.
\end{theorem}
\begin{proof}
For the existence of a solution in a more general space, see \cite[Satz 3.9, p.\ 112]{Troeltzsch} and more specifically \cite[Satz 7.9, p.\ 289]{Troeltzsch}. The proof as presented there follows the monography \cite{Ladyzhenskaya}.

In order to prove solution lying in $W(0, T)$ see \cite[Satz 3.12, p.\ 120]{Troeltzsch} and for the continuity result, see \cite[Satz 3.13, p.\ 121]{Troeltzsch}.
One can also work with $W(0, T)$ directly to simplify the proof, see \cite{Wloka}. However, this approach shares less analogies with the work needed for treating the elliptic optimal control case.
\end{proof}
Due to \cref{thm:generalized-problem-solution}, linear and continuous operators $G_Q : L^2(Q) \to W(0, T)$, $G_\Sigma : L^2(\Sigma) \to W(0, T)$ and $G_0 : L^2(\Omega) \to W(0, T)$ exist such that
\[
	y = G_Q g_I + G_\Sigma g_R + G_0 y_0.
\]
\section{Optimal control with boundary controls}
We'll now go back to the optimal control problem with boundary controls:
\begin{equation}
\label{eq:BoundaryOptimalControl-restricted}
\begin{IEEEeqnarraybox}[][c]{c}
\min J(y, u) = \frac{1}{2} \int_\Omega \left( y(x, T) - y_\Omega(x) \right)^2 \dd x + \frac{\lambda}{2} \int_0^T \int_{\partial \Omega} u(x, t)^2 \dd s(x) \dd t \\
\noalign{\noindent subject to\vspace{\jot}}
\begin{IEEEeqnarraybox}{rCl"l}
\frac{\partial y}{\partial t} - \lapl y &=& 0 & \text{in } Q \\
\partial_\nu y + \alpha y &=& \beta u & \text{in } \Sigma\\
y(x, 0) &=& y_0(x) & \text{in } \Omega
\end{IEEEeqnarraybox}
\end{IEEEeqnarraybox}
\end{equation}
Due to \cref{thm:generalized-problem-solution}, we can express $y$ as
\[
	y = G_\Sigma \beta u + G_0 y_0.
\]
If we introduce an observation operator $E_T : y \mapsto y(T)$, we can use the embedding property of the solution and obtain
\[
	y(T) = E_T (G_\Sigma \beta u + G_0 y_0).
\]
By defining the so called state operator $S \coloneqq E_T G_\Sigma \beta \cdot$, we notice that with $z \coloneqq y_\Omega - (G_0 y_0)(T)$ it holds:
\[
	y(T) - y_\Omega = S u + (G_0 y_0)(T) - y_\Omega = S u - z.
\]
For the discontinuous Galerkin method introduced in \cref{sec:dG-method}, we can define an analogous operator. 
To that end, we define operators that map $g_I$, $g_R$ and $y_0$ into their discrete right hand sides, i.e.\ we define $\boldsymbol{g_I} : L^2(Q) \to \R^m$, $\boldsymbol{g_R} : L^2(\Sigma) \to \R^m$ and $\boldsymbol{y_0} : L^2(\Omega) \to \R^m$ component-wise by
\begin{IEEEeqnarray*}{rCl}
	\boldsymbol{g_I}[i] (g_I) &=& \iint_Q g_I \varphi_i \dd x \dd t, \\
	\boldsymbol{g_R}[i] (g_R) &=& \iint_\Sigma g_R \varphi_i \dd x \dd t, \\
	\boldsymbol{y_0}[i] (y_0) &=& \int_\Omega y_0 \varphi_i(\cdot, 0) \dd x.
\end{IEEEeqnarray*}
Moreover, we define an operator to map discrete solutions of \cref{eq:dg-discrete-prob} into the function they correspond to in $\Shp(\meshT_N)$:
\begin{IEEEeqnarray*}{rCl}
	E_V : \R^m &\to& \Shp(\meshT_N) \\
	\boldsymbol{v} &\mapsto& v \coloneqq \sum_{j=1}^M \boldsymbol{v}[j] \varphi_j(x, t).
\end{IEEEeqnarray*}
Then, we can define the solution operators
\begin{IEEEeqnarray*}{rCl}
	G_\Sigma^h (g_R) &\coloneqq& E_V A_h^{-1} \boldsymbol{g_R} (g_R), \\
	G_0^h (y_0) &\coloneqq& E_V A_h^{-1} \boldsymbol{y_0} (y_0).
\end{IEEEeqnarray*}
Note that these operators are well defined for $\sigma \geq 4 c_K$ due to \cref{thm:A-bijective}. Therefore, the rest of this discussion assumes that \cref{as:mesh-assumptions} holds and that $\sigma$ is chosen such that $\sigma \geq 4 c_K$.
As of such, we can define a discrete analogous to $S$, called $S_h$:
\[
	y_h(T) - y_\Omega = S_h u_h + (G_0^h y_0)(T) - y_\Omega = S_h u_h - z_h,
\]
where $S_h \coloneqq E_T G_\Sigma \beta \cdot$ and $z_h \coloneqq y_\Omega - (G_0^h y_0)(T)$.
Note that both operators, $S$ and $S_h$ work from $L^2(\Sigma)$ to $L^2(\Omega)$. One can immediately see that $S_h$ is a linear operator.
Using that the term $\| f_h(T) \|_{L^2(\Omega)}$ is contained in $\| f_h \|_A$ together with the ellipticity estimate \cref{thm:A-ellip-weak} (or alternatively, for quasi-uniform meshes, the estimate \cref{thm:Astab-est}) guarantees that $S_h$ is also a continuous operator between these spaces.

With these operators defined, we can express the continuous and discretized optimal control problems as
\begin{equation}
\label{eq:f-S}
\min_{u \in L^2(\Sigma)} f(u) \coloneqq \frac{1}{2} \| S u - z \|_{L^2(\Omega)}^2 + \frac{\lambda}{2} \| u \|_{L^2(\Sigma)}^2,
\end{equation}
and
\begin{equation}
\label{eq:f-Sh}
\min_{u_h \in L^2(\Sigma)} f_h(u_h) \coloneqq \frac{1}{2} \| S_h u_h - z_h \|_{L^2(\Omega)}^2 + \frac{\lambda}{2} \| u_h \|_{L^2(\Sigma)}^2.
\end{equation}
Note that the discretized optimal control is being searched in $L^2(\Sigma)$ and not a discretized space.
However, one will see afterwards that it indeed has to lie in the boundary projection of $\Shp(\meshT_N)$ after a potential modification on a set of zero measure.

The approach to look for $u_h$ in a non-discretized space is called variational discretization and was first introduced in \cite{Hinze}.
By not discretizing the control, \cite{Hinze} established error estimates for continuous finite element approaches for elliptic optimal control problems. This was later applied to parabolic problems in \cite{DeckelnickHinze}, where they use an approach via a continuous Galerkin scheme in space and a discontinuous approach in time. A similar approach can be found in \cite{MeidnerVexler-I}, which was later on generalized to the state-constrained case in \cite{MeidnerVexler-II}.

In order to discuss uniqueness and existence of solutions to these problems, we can use the following, general result:
\begin{theorem}
\label{thm:optimal-control-existence}
Let $\{ U, \| \cdot \|_U \}$ and $\{ H, \| \cdot \|_H \}$ be real Hilbert spaces and a nonempty, closed and convex set $U_{ad} \subset U$, an element $z \in H$ and a constant $\lambda > 0$.
Moreover, set $S : U \to H$ be a linear and continuous operator.
Then the quadratic optimization problem in the Hilbert space
\[
	\min_{u \in U_{ad}} f(u) \coloneqq \frac{1}{2} \| Su - z \|_H^2 + \frac{\lambda}{2} \| u \|_U^2
\]
admits an optimal solution $\bar{u}$. If $\lambda$ is positive or $S$ injective, then is is uniquely determined.
\end{theorem}
\begin{proof}
We present the proof given by \cite[Satz 2.14]{Troeltzsch} and \cite[Satz 2.15]{Troeltzsch}.

Assume $U_{ad}$ is bounded. Because $f \geq 0$, the infimum of all possible function values exists:
\[
	j \coloneqq \inf_{u \in U_{ad}} f(u).
\]
Hence, a sequence $\{ u_n \}_{n=1}^\infty \subset U_{ad}$ with $f(u_n) \to j$ for $n \to \infty$ exists.
Therefore one knows that $U_{ad}$ is weakly sequentially compact, i.e.\ a weakly convergent subsequence $\{ u_{n_k} \}_{k=1}^\infty \subset U_{ad}$ weakly converging against a $\bar{u} \in U_{ad}$ exists.
As $S$ is continuous and $f$ strictly convex (because $\lambda > 0$), one can conclude weak lower semi-continuity of $f$ and thus has
\[
	j \leq f(\bar{u}) \leq \min_{k \to \infty} \inf f(u_{n_k}) = j.
\]
Because $f$ is strictly convex, $\bar{u}$ is uniquely determined.

Now let $U_{ad}$ not be bounded. Because $U_{ad}$ is nonempty, $u_0 \in U_{ad}$ exists. For $\| u \|_U^2 > 2 \lambda^{-1} f(u_0)$, one has
\[
	f(u) = \frac{1}{2} \| S u - z \|_H^2 + \frac{\lambda}{2} \| u \|_U^2 \geq \frac{\lambda}{2} \| u \|_U^2 > f(u_0).
\]
Hence, one can simply search within the bounded, convex and closed set
\[
	U_{ad} \cap \{ u \in U : \| u \|_U^2 \leq 2 \lambda^{-1} f(u_0) \}
\]
and use the previously derived result.
\end{proof}
For $S$ continuity is guaranteed by \cref{thm:generalized-problem-solution} (with the choices $g_I = 0$, $g_R = \beta u$).
On the other hand, for the discrete operator $S_h$ this is guaranteed by \cref{thm:Astab-est}.
In conclusion \cref{thm:optimal-control-existence} implies that both \cref{eq:f-S} and \cref{eq:f-Sh} admit unique solutions, named $\bar{u}$ and $\bar{u}_h$, respectively.
Combined with the previously shown result that a solution in $L^2(\Sigma)$ implies that the optimal solution is in in the boundary projection of $\Shp(\meshT_N)$ after potentially changing it on a set of zero measure.

These unique solutions can be characterized using a so called variational inequality:
\begin{theorem}
\label{thm:variational-ineq}
Let $U$ and $H$ be real Hilbert spaces, a nonempty and convex set $U_{ad} \subset U$, $z \in H$ and a constant $\lambda \geq 0$. Let $S : U \to H$ be a linear and continuous operator.
The element $\bar{u} \in U_{ad}$ solves
\[
	\min_{u \in U_{ad}} f(u) \coloneqq \frac{1}{2} \| Su - z \|_H^2 + \frac{\lambda}{2} \| u \|_U^2
\]
if and only if
\[
	\langle S \bar{u} - z, S ( u - \bar{u} ) \rangle_U + \lambda \langle \bar{u}, u - \bar{u} \rangle_U \geq 0 \quad \forall u \in U_{ad}.
\]
\end{theorem}
\begin{proof}
See \cite[Satz 2.22]{Troeltzsch} for this statement.
\end{proof}
By using the adjoint operator $S^*$, one can characterize the solution of an optimal control problem using the so called adjoint state:
\begin{IEEEeqnarray*}{r"rCl"l}
	& \langle S \bar{u} - z, S ( u - \bar{u} ) \rangle_U + \lambda \langle \bar{u}, u - \bar{u} \rangle_U &\geq& 0 & \forall u \in U_{ad}, \\
	\Longleftrightarrow & \langle S^* ( S \bar{u} - z ) + \lambda \bar{u}, u - \bar{u} \rangle_U &\geq& 0 & \forall u \in U_{ad}.
\end{IEEEeqnarray*}
Hence, one can define the solution by making use of the adjoint state $p \coloneqq  S^* ( S \bar{u} - z ) $.
For the parabolic case, it is however easier to work with the form given in \cref{thm:variational-ineq} and proving equivalence for a reasonably defined $p$ rather than attempting to determine $S^*$ directly.

In order to be able to treat the various formulations of the optimal control problem at once, we introduce a general parabolic problem that will serve as the adjoint problem: Consider the boundary value problem
\begin{equation}
\label{eq:generalized-problem-adj}
\begin{IEEEeqnarraybox}[][c]{rCl"l}
-\frac{\partial p}{\partial t} (x, t) - \lapl p(x, t) &=& a_Q(x, t) & \text{for } (x, t) \in Q, \\
n_x(x,t) \cdot \nabla_x p(x, t) + \alpha p(x, t) &=& a_\Sigma(x, t) & \text{for } (x, t) \in \Sigma, \\
p(x, T) &=& a_\Omega(x) & \text{for } (x, t) \in \Omega,
\end{IEEEeqnarraybox}
\end{equation}
with $\alpha$ being a constant as before and $a_Q \in L^2(Q)$, $a_\Sigma \in L^2(\Sigma)$ and $a_\Omega \in L^2(\Omega)$.
Using a transformation $\tilde{p}(x, t) = p(x, T - t)$, this can be shown to have a unique weak solution solution by applying \cref{thm:generalized-problem-solution} to the transformation and transforming back.
We have previously derived a discretized solution for this problem in \cref{sec:adj-dG-treatment}. As argued there, the same transformation together with \cref{thm:A-bijective} and \cref{thm:dg-convergence} delivers the existence of a unique discrete solution that converges in the $\lDG \cdot \rDG$ norm.

For the boundary optimal control case, the solution of \cref{eq:generalized-problem-adj} with the choices $a_Q = 0$, $a_\Sigma = 0$, $a_\Omega = \bar{y}(T) - y_\Omega$ characterizes the solution $\bar{u} \in U_{ad}$ equivalently to \cref{thm:variational-ineq} with the inequality
\[
	\langle \beta \bar{p} + \lambda \bar{u}, u - \bar{u} \rangle_U \geq 0 \quad \forall u \in U_{ad}.
\]
This result has been shown in \cite[Lemma 3.17 and Satz 3.18, p.\ 126f.]{Troeltzsch}.

We aim to prove the analogous result for the discrete case, i.e.\ that the discretization introduced in \cref{sec:adj-dG-treatment} with similar right hand sides also delivers an adjoint state with the same functionality as in the continuous case.

Again, for the sake of covering multiple possible problem formulations at once, we work with \cref{eq:generalized-problem}, where each boundary term depends on controls, named $v$, $u$ and $w$, respectively.
For $b_Q \in L^\infty(Q)$, $b_\Sigma \in L^\infty(\Sigma)$, $b_\Omega \in L^\infty(\Omega)$ and given controls $v \in L^2(Q)$, $u \in L^2(\Sigma)$ and $w \in L^2(\Omega)$ as well as the constant $\alpha$, we define the problem
\begin{equation}
\label{eq:generalized-problem-state}
\begin{IEEEeqnarraybox}[][c]{rCl"l}
\frac{\partial y}{\partial t} (x, t) - \lapl y(x, t) &=& b_Q(x, t) v(x, t) & \text{for } (x, t) \in Q, \\
n_x(x,t) \cdot \nabla_x y(x, t) + \alpha y(x, t) &=& b_\Sigma(x, t) u(x, t) & \text{for } (x, t) \in \Sigma, \\
y(x, 0) &=& b_\Omega(x) w(x) & \text{for } (x, t) \in \Omega.
\end{IEEEeqnarraybox}
\end{equation}
Alas, we have right hand sides that consist of the product between some coefficients and a control.
 
By applying the discontinuous Galerkin formulation for \cref{eq:generalized-problem-state} we obtain
\begin{equation}
\label{eq:generalized-problem-state-dG}
A_h \boldsymbol{y} = \boldsymbol{g_I}(b_Q v) + \boldsymbol{g_R}(b_\Sigma u) + \boldsymbol{y_0}(b_\Omega w).
\end{equation}
and by treating \cref{eq:generalized-problem-adj} accordingly, we obtain
\begin{equation}
\label{eq:generalized-problem-adj-dG}
A_h^\tp \boldsymbol{p} = \boldsymbol{g_I}(a_Q) + \boldsymbol{g_R}(a_\Sigma) + \boldsymbol{y_T}(a_\Omega),
\end{equation}
where $\boldsymbol{y_T} : L^2(\Omega) \to \R^m$ is defined in an analogous fashion as before:
\begin{IEEEeqnarray*}{rCl}
	\boldsymbol{y_T}[i] (y_T) &=& \int_\Omega y_T \varphi_i(\cdot, T) \dd x \dd t.
\end{IEEEeqnarray*}
One would assert that \cref{eq:generalized-problem-adj-dG} is the adjoint in the given sense to \cref{eq:generalized-problem-state-dG}, because \cref{eq:generalized-problem-adj} is the adjoint problem with analogous right hand sides to \cref{eq:generalized-problem-state} for the continuous case.
In order to prove this, we need the following statement:
\begin{lemma}
\label{thm:discrete-adj-state-helper}
The solution $y_h$ of \cref{eq:f-Sh} and $p_h$ of \cref{eq:generalized-problem-adj-dG} with the choices of $a_Q = 0$, $a_\Sigma = 0$, and $a_\Omega = y_h(T) - y_\Omega$ adhere to the following equation:
\[
\begin{IEEEeqnarraybox}[][c]{l}
	\iint_Q b_Q v p_h \dd x \dd t + \int_\Omega b_\Omega w p_h(\cdot, 0) \dd x + \iint_{\Sigma_R} b_\Sigma u p_h \dd s(x) \dd t \\
	\IEEEeqnarraymulticol{1}{r}{ \qquad\qquad {} = \iint_Q a_Q y_h \dd x \dd t + \int_\Omega a_\Omega y_h(\cdot, T) \dd x + \iint_{\Sigma_R} a_\Sigma y_h \dd s(x) \dd t. }
\end{IEEEeqnarraybox}
\]
\end{lemma}
\begin{proof}
Our formulation for the discretization of \cref{eq:generalized-problem-state} and \cref{eq:generalized-problem-adj} are respectively:
\begin{IEEEeqnarray*}{rCl}
	a(y_h, v_h) + b(y_h, v_h) &=& \iint_Q b_Q v v_h \dd x \dd t + \int_\Omega b_\Omega w v_h(\cdot, 0) \dd x + \iint_{\Sigma_R} b_\Sigma u v_h \dd s(x) \dd t, \\
	a(v_h, p_h) + b(v_h, p_h) &=& \iint_Q a_Q v_h \dd x \dd t + \int_\Omega a_\Omega v_h(\cdot, T) \dd x + \iint_{\Sigma_R} a_\Sigma v_h \dd s(x) \dd t,
\end{IEEEeqnarray*}
where each equation holds for all $v_h \in \Shp(\meshT_N)$.
We test the first equation with $p_h$ and the second one with $y_h$:
\begin{IEEEeqnarray*}{rCl}
	a(y_h, p_h) + b(y_h, p_h) &=& \iint_Q b_Q v p_h \dd x \dd t + \int_\Omega b_\Omega w p_h(\cdot, 0) \dd x + \iint_{\Sigma_R} b_\Sigma u p_h \dd s(x) \dd t \\
	a(y_h, p_h) + b(y_h, p_h) &=& \iint_Q a_Q y_h \dd x \dd t + \int_\Omega a_\Omega y_h(\cdot, T) \dd x + \iint_{\Sigma_R} a_\Sigma y_h \dd s(x) \dd t.
\end{IEEEeqnarray*}
Noticing that the left hand sides of both equations are the same gives us the desired statement:
\[
\begin{IEEEeqnarraybox}[][c]{l}
	\iint_Q b_Q v p_h \dd x \dd t + \int_\Omega b_\Omega w p_h(\cdot, 0) \dd x + \iint_{\Sigma_R} b_\Sigma u p_h \dd s(x) \dd t \\
	\IEEEeqnarraymulticol{1}{r}{ \qquad\qquad {} = \iint_Q a_Q y_h \dd x \dd t + \int_\Omega a_\Omega y_h(\cdot, T) \dd x + \iint_{\Sigma_R} a_\Sigma y_h \dd s(x) \dd t. }
\end{IEEEeqnarraybox}
\]
\end{proof}
Using \cref{thm:discrete-adj-state-helper}, we can prove:
\begin{theorem}
\label{thm:discrete-variational-ineq}
A control $\bar{u}_h \in L^2(\Omega)$ is optimal for \cref{eq:f-Sh} if and only if with the associated adjoint state $\bar{p}_h$ defined as the solution of \cref{eq:generalized-problem-adj-dG} with the choices $a_Q = 0$, $a_\Sigma = 0$ and $a_\Omega = \bar{y}_h(T) - y_\Omega$ fulfills the following variational inequality
\[
	\langle \beta(x, t) \bar{p}_h (x, t) + \lambda \bar{u}_h(x, t), u_h - \bar{u}_h \rangle_{L^2(\Sigma)} \geq 0 \quad \forall u_h \in L^2(\Sigma)
\]
almost everywhere.
\end{theorem}
\begin{proof}
This proof is inspired by \cite[Satz 3.19, p.\ 128f.]{Troeltzsch}.
Using the previously derived minimization formulation \cref{eq:f-Sh} and employing \cref{thm:variational-ineq}, we know
\begin{IEEEeqnarray*}{rCl}
	0 &\leq& \langle S_h \bar{u}_h - z_h, S_h(u_h - \bar{u}_h) \rangle_{L^2(\Omega)} + \lambda\langle\bar{u}_h, u_h - \bar{u}_h \rangle_{L^2(\Sigma)} \\
	&=& \int_\Omega (\bar{y}_h(T) - y_\Omega)(y_h(T) - \bar{y}_h(T)) \dd x + \lambda \iint_\Sigma \bar{u}_h ( u_h - \bar{u}_h ) \dd s(x) \dd  t.
\end{IEEEeqnarray*}
for all $u_h \in L^2(\Omega)$ and their associated states $y_h$.
Note that
\[
	S_h u_h - S_h \bar{u}_h = S_h u_h + z_h - S_h \bar{u}_h - z_h = y_h(T) - \bar{y}_h(T).
\]
Using \cref{thm:discrete-adj-state-helper} with $b_Q = 0$, $b_\Sigma = \beta$, $b_\Omega = 0$, $a_Q = 0$, $a_\Sigma = 0$, $a_\Omega = \bar{y}_h(T) - y_\Omega$, we have
\[
	\iint_\Sigma \beta p_h \tilde{u}_h \dd s(x) \dd t = \langle \bar{y}_h(T) - y_\Omega, \tilde{y}_h(T) \rangle_{L^2(\Omega)},
\]
where we set $\tilde{u}_h = u_h - \bar{u}_h$ and $\tilde{y}_h = y_h - \bar{y}_h$.
Inserting this into the variational inequality we obtain
\begin{IEEEeqnarray*}{rCl}
	0 &\leq& \int_\Omega (\bar{y}_h(T) - y_\Omega)(y_h(T) - \bar{y}_h(T)) \dd x + \lambda \iint_\Sigma \bar{u}_h ( u_h - \bar{u}_h ) \dd s(x) \dd  t \\
	&=& \iint_\Sigma \beta p_h ( u_h - \bar{u}_h ) \dd s(x) \dd t + \lambda \iint_\Sigma \bar{u}_h ( u_h - \bar{u}_h ) \dd s(x) \dd  t \\
	&=& \iint_\Sigma ( \beta p_h + \lambda \bar{u}_h ) ( u_h - \bar{u}_h ) \dd s(x) \dd t
\end{IEEEeqnarray*}
This however is precisely the inequality we aimed to derive.
\end{proof}
\begin{remark}
Due to the choice $U_{ad} = L^2(\Sigma)$ for both the continuous and discretized formulations, we know the variational inequalities hold in the stronger sense
\begin{IEEEeqnarray*}{rCl}
	\beta p + \lambda \bar{u} &=& 0 \\
	\beta p_h + \lambda \bar{u}_h &=& 0
\end{IEEEeqnarray*}
almost everywhere.

If for example $\beta$ is constant, this means that $\bar{u}_h$ can be interpreted to be a boundary projection of a function in $\Shp(\meshT_N)$, i.e. the function $-\lambda^{-1} \beta p_h \in \Shp(\meshT_N)$.
\end{remark}
Bringing the optimality conditions for the continuous case into a single system, one obtains:
\begin{IEEEeqnarray*}{c}
\begin{IEEEeqnarraybox}{r"l}
\begin{IEEEeqnarraybox}{rCl}
\frac{\partial y}{\partial t} - \lapl y &=& 0 \\
\partial_\nu y + \alpha y &=& - \beta^2 \lambda^{-1} p \\
y(0) &=& y_0
\end{IEEEeqnarraybox} & 
\begin{IEEEeqnarraybox}{rCl}
-\frac{\partial p}{\partial t} - \lapl p &=& 0 \\
\partial_\nu p + \alpha p &=& 0 \\
p(T) &=& y(T) - y_\Omega
\end{IEEEeqnarraybox}
\end{IEEEeqnarraybox} \\
u = - \lambda^{-1} \beta p.
\end{IEEEeqnarray*}
For the discrete case, one obtains a corresponding formulation due to \cref{thm:discrete-variational-ineq} the system
\begin{IEEEeqnarray*}{c}
\begin{IEEEeqnarraybox}{rCl"rCl}
A(y_h, v_h) &=& \langle \beta u_h, v_h \rangle_{L^2(\Sigma_R)} + \langle y_0, v_h(0) \rangle_{L^2(\Omega)} & A(v_h, p_h) &=& \langle y_h(T) - y_\Omega, v_h(T) \rangle_{L^2(\Omega)}
\end{IEEEeqnarraybox} \\
u_h = - \lambda^{-1} \beta p_h
\end{IEEEeqnarray*}
We observe two things that may not come as a surprise: Firstly, our discrete optimality system is equivalent to applying the discontinuous Galerkin method to the state and adjoint equation of the continuous case individually, and secondly that the adjoint operator of the discrete state equation uses the transposed matrix.

With this, we have shown that the method we're working with is well defined and yields a solution to an optimal control problem similar to the continuous one.
In a further step, we would like to prove convergence of the discrete problem.
Given that \cref{thm:dg-convergence} requires at least $y \in H^2(\meshT_N) \subset H^2(Q)$, we have to make the following assumption:
\begin{assumption}
\label{as:continuous-Hs-regularity}
The solution of the system \cref{eq:generalized-weak-form} lies in $H^s(\meshT_N) \cap W(0,T)$ with $s \geq 2$, i.e.
\[
	\| y \|_{H^s(Q)} \leq c_w' \left( \| g_I \|_{L^2(Q)} + \| g_R \|_{L^2(\Sigma)} + \| y_0 \|_{L^2(\Omega)} \right)
\]
with a constant $c_w' > 0$ independent of $g_I$, $g_R$ and $y_0$.
\end{assumption}
As discussed in the remark after \cref{thm:dg-convergence}, a result like $y \in L^2(0, T; H^2(\Omega)) \cap H^1(0, T; L^2(\Omega))$, which often can be proved, is not sufficient.
\begin{theorem}
\label{thm:optimal-control-convergence}
Let $\bar{u}$ be the optimal control for the continuous problem \cref{eq:BoundaryOptimalControl-restricted} and let \cref{as:continuous-Hs-regularity} hold. Moreover, be $\bar{u}_h$ the optimal control to the discretized problem \cref{eq:f-Sh}.
Then there is a $C > 0$ independent of $h$, such that:
\[
	\| \bar{u} - \bar{u}_h \|_{L^2(\Sigma)} + \| \bar{y}(T) - \bar{y}_h(T) \|_{L^2(\Omega)} \leq C h^{\min \{ s, p+1\} - 1} \left( \| \bar{u} \|_{L^2(\Sigma)} + \| y_\Omega \|_{L^2(\Omega)} \right).
\]
\end{theorem}
\begin{proof}
This proof is based on the previously mentioned variational discretization technique presented in \cite{Hinze}.

The optimal solutions to both problems fulfill
\begin{IEEEeqnarray*}{rCl"l}
	0 &=& \langle S \bar{u} - z, S v \rangle_{L^2(\Omega)} + \lambda \langle\bar{u}, v \rangle_{L^2(\Sigma)} & \text{for all } v \in L^2(\Sigma) \\
	0 &=& \langle S_h \bar{u}_h - z_h, S_h v \rangle_{L^2(\Omega)} + \lambda \langle\bar{u}_h, v\rangle_{L^2(\Sigma)} & \text{for all } v \in L^2(\Sigma)
\end{IEEEeqnarray*}
We perform a zero addition of the second term evaluated with $v = \bar{u}$:
\begin{IEEEeqnarray*}{rCl}
	0 &=& \langle S \bar{u} - z, S v \rangle_{L^2(\Omega)} + \lambda \langle\bar{u}, v \rangle_{L^2(\Sigma)} \\
	&& {} - \left[ \langle S_h \bar{u} - z_h, S_h v \rangle_{L^2(\Omega)} + \lambda \langle \bar{u}, v \rangle_{L^2(\Sigma)} \right] \\
	&& {} + \left[ \langle S_h \bar{u} - z_h, S_h v \rangle_{L^2(\Omega)} + \lambda \langle \bar{u}, v \rangle_{L^2(\Sigma)} \right] \\
	&& {} - \langle S_h \bar{u}_h - z_h, S_h v \rangle_{L^2(\Omega)} - \lambda \langle \bar{u}_h, v \rangle_{L^2(\Sigma)} 
\end{IEEEeqnarray*}
By bringing the first two terms on the other side, we obtain
\begin{IEEEeqnarray*}{rCl}
	\IEEEeqnarraymulticol{3}{l}{ - \langle S \bar{u} - z, S v \rangle_{L^2(\Omega)} + \lambda \langle\bar{u}, v \rangle_{L^2(\Sigma)} + \left[ \langle S_h \bar{u} - z_h, S_h v \rangle_{L^2(\Omega)} + \lambda \langle \bar{u}, v \rangle_{L^2(\Sigma)} \right] } \\
	\qquad &=&  \left[ \langle S_h \bar{u} - z_h, S_h v \rangle_{L^2(\Omega)} + \lambda \langle \bar{u}, v \rangle_{L^2(\Sigma)} \right] - \langle S_h \bar{u}_h - z_h, S_h v \rangle_{L^2(\Omega)} - \lambda \langle \bar{u}_h, v \rangle_{L^2(\Sigma)} 
\end{IEEEeqnarray*}
We want to estimate the left hand side by an upper bound and the right hand side by a lower bound.
Therefore, we start with the left hand side:
\begin{IEEEeqnarray*}{rCl}
	\IEEEeqnarraymulticol{3}{l}{ \chi_1 \coloneqq - \langle S \bar{u} - z, S v \rangle_{L^2(\Omega)} + \lambda \langle \bar{u}, v \rangle_{L^2(\Sigma)} + \left[ \langle S_h \bar{u} - z_h, S_h v \rangle_{L^2(\Omega)} + \lambda \langle \bar{u}, v \rangle_{L^2(\Sigma)} \right] } \\
	&=& - \langle S \bar{u} - z, S v \rangle_{L^2(\Omega)} + \langle S_h \bar{u} - z_h, S_h v \rangle_{L^2(\Omega)} \\
\noalign{\noindent Performing a zero addition of $\langle S_h \bar{u} - z_h, S v \rangle$ gives \vspace{\jot}}
	\chi_1 &=& - \langle S \bar{u} - z - [ S_h \bar{u} - z_h ], S v \rangle_{L^2(\Omega)} + \langle S_h \bar{u} - z_h, S_h v -S v \rangle_{L^2(\Omega)} \\
	&=& - \langle S \bar{u} + (G_0 y_0)(T) - y_\Omega - S_h \bar{u} - (G_0^h y_0)(T) + y_\Omega, S v \rangle_{L^2(\Omega)} \\
	&& \quad {} + \langle S_h \bar{u} + (G_0^h y_0)(T) - y_\Omega, S_h v -S v \rangle_{L^2(\Omega)} \\
\noalign{\noindent Due to the definition of $\bar{y}(T) = S \bar{u} + (G_0 y_0)(T)$, we can replace that term. By defining $\bar{y}^h$ as the state deriving from the discrete state equation with the control $\bar{u}$, we can also replace $\bar{y}^h(T) = S_h \bar{u} + (G_0^h y_0)(T)$: \vspace{\jot}}
	\chi_1 &=& - \langle \bar{y}(T) - \bar{y}^h(T) , S v \rangle_{L^2(\Omega)} + \langle \bar{y}^h(T) - y_\Omega, S_h v -S v \rangle_{L^2(\Omega)} \\
\noalign{\noindent Using the triangle inequality and the Cauchy-Schwarz inequality, we obtain with the definitions $y^v \coloneqq G_\Sigma v$ and $y_h^v \coloneqq G_\Sigma^h v$, so that we have $y^v(T) = Sv$ and $y_h^v(T) = S_h v$, giving us\vspace{\jot}}
	\chi_1 &\leq& \left| \langle \bar{y}(T) - \bar{y}^h(T) , S v \rangle_{L^2(\Omega)} \right| + \left| \langle \bar{y}^h(T) - y_\Omega, S_h v -S v \rangle_{L^2(\Omega)} \right| \\
	&\leq& \left\| \bar{y}(T) - \bar{y}^h(T) \right\|_{L^2(\Omega)} \| y^v(T) \|_{L^2(\Omega)} \\
	&& \quad {}+ \left\| \bar{y}^h(T) - y_\Omega \right\|_{L^2(\Omega)} \left\| y^v_h(T) - y^v(T) \right\|_{L^2(\Omega)} \\
\noalign{\noindent By the definition of the $\lDG \cdot \rDG$ norm, we have the term $\| f(T) \|_{L^2(\Omega)}$ in it, c.f.\ \cref{eq:dg-DG-norm}, we can estimate \vspace{\jot}}
	\chi_1 &\overset{\text{\cref{eq:dg-DG-norm}}}{\leq}& \left\lDG \bar{y} - \bar{y}^h \right\rDG \| y^v(T) \|_{L^2(\Omega)} + \left( \left\lDG \bar{y}^h \right\rDG + \| y_\Omega \|_{L^2(\Omega)} \right) \left\lDG y^v_h - y_v \right\rDG \\
\noalign{\noindent Due to the continuity estimate on exact solutions given by \cref{thm:generalized-problem-solution,thm:W0T-continuous-embedding}, we estimate this further as\vspace{\jot}}
	\chi_1 &\overset{\text{\cref{thm:generalized-problem-solution}}}{\leq}& c_w c_E \left\lDG \bar{y} - \bar{y}^h \right\rDG \| v \|_{L^2(\Sigma)} + \left( \left\lDG \bar{y}^h \right\rDG + \| y_\Omega \|_{L^2(\Omega)} \right) \left\lDG y^v_h - y_v \right\rDG \\
\noalign{\noindent Using the stability estimate for $A(\cdot, \cdot)$ given by \cref{thm:Astab-est}, we can estimate the operator $G_\Sigma^h$ because it consists out of the inverse $A_h^{-1}$:}
	\chi_1 &\overset{\text{\cref{thm:Astab-est}}}{\leq}& c_w c_E \left\lDG \bar{y} - \bar{y}^h \right\rDG \| v \|_{L^2(\Sigma)} \\
	&& \quad {}+ \left( \frac{1}{c_S^A} \| \beta \|_{L^\infty(\Omega)} \| \bar{u} \|_{L^2(\Sigma)} + \| y_\Omega \|_{L^2(\Omega)} \right) \left\lDG y^v_h - y_v \right\rDG \\
\noalign{\noindent As a next step we can apply the result \cref{thm:dg-convergence} because we made \cref{as:continuous-Hs-regularity}:}
	\chi_1 &\overset{\text{\cref{thm:dg-convergence}}}{\leq}& c_w c_E c \max\{1, \alpha\} h^{\min \{ s, p+1\} - 1} |\bar{y}|_{H^s(\meshT_N)} \| v \|_{L^2(\Sigma)} \\
	&& {}+ \left( \frac{1}{c_S^A} \| \beta \|_{L^\infty(\Omega)} \| \bar{u} \|_{L^2(\Sigma)} + \| y_\Omega \|_{L^2(\Omega)} \right) c \max\{1, \alpha\} h^{\min \{ s, p+1\} - 1} |y_v|_{H^s(\meshT_N)} \\
\noalign{\noindent The assumption of continuous dependence of the solution in the stronger $H^2$ norm on the right hand side gives the estimate}
	\chi_1 &\overset{\text{\cref{as:continuous-Hs-regularity}}}{\leq}& c_w c_E c_w' c \max\{1, \alpha\} h^{\min \{ s, p+1\} - 1} ( \|\bar{u}\|_{L^2(\Sigma)} + \| y_\Omega \|_{L^2(\Omega)} ) \| v \|_{L^2(\Sigma)} \\
	&& {}+ \left( \frac{1}{c_S^A} \| \beta \|_{L^\infty(\Omega)} \| \bar{u} \|_{L^2(\Sigma)} + \| y_\Omega \|_{L^2(\Omega)} \right) c c_w' \max\{1, \alpha\} h^{\min \{ s, p+1\} - 1} \|v\|_{L^2(\Sigma)} \\
\noalign{\noindent Finally, by combining the constants into a single one called $C > 0$, we obtain}
	\chi_1 &\leq& C h^{\min \{ s, p+1\} - 1} \left( \| \bar{u} \|_{L^2(\Sigma)} + \| y_\Omega \|_{L^2(\Omega)} \right) \| v \|_{L^2(\Sigma)}
\end{IEEEeqnarray*}

For the remaining two terms, we have
\begin{IEEEeqnarray*}{rCl}
	\IEEEeqnarraymulticol{3}{l}{ \left[ \langle S_h \bar{u} - z_h, S_h v \rangle_{L^2(\Omega)} + \lambda \langle \bar{u}, v \rangle_{L^2(\Sigma)} \right] - \langle S_h \bar{u}_h - z_h, S_h v \rangle_{L^2(\Omega)} - \lambda \langle \bar{u}_h, v \rangle_{L^2(\Sigma)} } \\
	\qquad &=& \langle S_h (\bar{u} - \bar{u}_h), S_h v \rangle_{L^2(\Omega)} + \lambda \langle \bar{u} - \bar{u}_h, v \rangle_{L^2(\Sigma)}
\end{IEEEeqnarray*}
Choosing the test function $v = \bar{u} - \bar{u}_h$, we can estimate:
\begin{IEEEeqnarray*}{rCl}
	\langle S_h (\bar{u} - \bar{u}_h), S_h (\bar{u} - \bar{u}_h) \rangle_{L^2(\Omega)} + \lambda \langle \bar{u} - \bar{u}_h, \bar{u} - \bar{u}_h \rangle_{L^2(\Sigma)} &\geq& \lambda \langle \bar{u} - \bar{u}_h, \bar{u} - \bar{u}_h \rangle_{L^2(\Sigma)} \\
	&=& \lambda \| \bar{u} - \bar{u}_h \|_{L^2(\Sigma)}^2
\end{IEEEeqnarray*}

By applying these estimates to the equation we started from yields:
\begin{IEEEeqnarray*}{rCl}
	\IEEEeqnarraymulticol{3}{l}{ C h^{\min \{ s, p+1\} - 1} \left( \| \bar{u} \|_{L^2(\Sigma)} + \| y_\Omega \|_{L^2(\Omega)} \right) \| \bar{u} - \bar{u}_h \|_{L^2(\Sigma)} } \\
	\qquad &\geq& -\langle S \bar{u} - z, S ( \bar{u} - \bar{u}_h ) \rangle_{L^2(\Omega)} + \lambda \langle \bar{u}, \bar{u} - \bar{u}_h \rangle_{L^2(\Sigma)} \\
	&& \quad {} + \left[ \langle S_h \bar{u} - z_h, S_h (\bar{u} - \bar{u}_h) \rangle_{L^2(\Omega)} + \lambda \langle \bar{u}, \bar{u} - \bar{u}_h \rangle_{L^2(\Sigma)} \right] \\
	&=& \left[ \langle S_h \bar{u} - z_h, S_h (\bar{u} - \bar{u}_h) \rangle_{L^2(\Omega)} + \lambda \langle \bar{u}, (\bar{u} - \bar{u}_h) \rangle_{L^2(\Sigma)} \right] \\
	&& \quad {} - \langle S_h \bar{u}_h - z_h, S_h (\bar{u} - \bar{u}_h) \rangle_{L^2(\Omega)} - \lambda \langle \bar{u}_h, (\bar{u} - \bar{u}_h) \rangle_{L^2(\Sigma)}  \\
	&\geq& \lambda \| \bar{u} - \bar{u}_h \|_{L^2(\Sigma)}^2.
\end{IEEEeqnarray*}
Dividing by $\| \bar{u} - \bar{u}_h \|_{L^2(\Sigma)}$ leaves us with
\[
	C h^{\min \{ s, p+1\} - 1} \left( \| \bar{u} \|_{L^2(\Sigma)} + \| y_\Omega \|_{L^2(\Omega)} \right) \geq \lambda \| \bar{u} - \bar{u}_h \|_{L^2(\Sigma)}.
\]

In a further step, we have to estimate $\| \bar{y}(T) - \bar{y}_h(T) \|_{L^2(\Omega)}$:
\begin{IEEEeqnarray*}{rCl}
\| \bar{y}(T) - \bar{y}_h(T) \|_{L^2(\Omega)} &=& \| S \bar{u} - S_h \bar{u}_h \|_{L^2(\Omega)} \\
&\leq& \| S \bar{u} - S_h \bar{u} \|_{L^2(\Omega)} + \| S_h \bar{u} - S_h \bar{u}_h \|_{L^2(\Omega)} \\
&\leq& \| \bar{y}(T) - \bar{y}^h(T) \|_{L^2(\Omega)} + \| \bar{y}^h(T) - \bar{y}_h(T) \|_{L^2(\Omega)} \\
&\leq& c \max\{1, \alpha\} h^{\min \{ s, p+1\} - 1} |\bar{y}|_{H^s(\meshT_N)} + \frac{1}{c_S^A} \| \beta \|_{L^\infty(\Omega)} \| \bar{u} - \bar{u}_h \|_{L^2(\Sigma)} \\
&\leq& C h^{\min \{ s, p+1\} - 1} \| \bar{u} \|_{L^2(\Sigma)} \\
&& \quad {} + C' h^{\min \{ s, p+1\} - 1} \left( \| \bar{u} \|_{L^2(\Sigma)} + \| y_\Omega \|_{L^2(\Omega)} \right),
\end{IEEEeqnarray*}
where we used the same stability argument to estimate the stability of the discrete solution as before when estimating the control.
Together, the estimates prove the claim made.
\end{proof}
\begin{remark}
We observe that \cref{as:continuous-Hs-regularity} can be weaked somewhat:
Instead of making the demand for general right hand sides, it was only necessary to use this for
\begin{IEEEeqnarray*}{rCl}
\| \bar{y} \|_{H^s(Q)} &\leq& c_w' \left( \| g_I \|_{L^2(Q)} + \| g_R \|_{L^2(\Sigma)} + \| y_0 \|_{L^2(\Omega)} \right) \\
\| y^v \|_{H^s(Q)} &\leq& c_w' \left( \| g_I \|_{L^2(Q)} + \| g_R \|_{L^2(\Sigma)} + \| y_0 \|_{L^2(\Omega)} \right)
\end{IEEEeqnarray*}
\end{remark}
\section{Optimal control with interior heat sources}
Analogously to the previous section, we can treat the interior heat source problem with the same restrictions made:
\begin{equation}
\label{eq:InnerOptimalControl-restricted}
\begin{IEEEeqnarraybox}[][c]{c}
\min J(y, u) = \frac{1}{2} \iint_\Sigma \left( y(x, T) - y_\Sigma(x) \right)^2 \dd s(x) \dd t + \frac{\lambda}{2} \iint_Q u(x, t)^2 \dd s(x) \dd t \\
\noalign{\noindent subject to\vspace{\jot}}
\begin{IEEEeqnarraybox}{rCl"l}
\frac{\partial y}{\partial t} - \lapl y &=& \beta u & \text{in } Q\\
\partial_\nu y &=& 0 & \text{in } \Sigma \\
y(x, 0) &=& 0 & \text{in } \Omega
\end{IEEEeqnarraybox}
\end{IEEEeqnarraybox}
\end{equation}
The treatment of this equation is very similar to the one of \cref{eq:BoundaryOptimalControl-restricted}.
In this case, the solution of the state equation can be expressed as
\[
	y = G_Q \beta u.
\]
Here, we need the analogous observation operator $E_\Sigma : y \mapsto y|_\Sigma$, so that we can define $S \coloneqq E_\Sigma G_Q \beta u$. The operator $E_\Sigma$ is continuous from $W(0, T)$ to $L^2(\Sigma)$, hence $S : u \mapsto y|_\Sigma$ is continuous from $L^2(Q)$ to $L^2(\Sigma)$.

By applying \cref{thm:optimal-control-existence} one obtains the existence of a unique solution $\bar{u}$ again, and by using \cref{thm:variational-ineq}, one obtains the optimality condition
\[
	0 \leq \langle S \bar{u} - y_\Sigma, S u - S \bar{u} \rangle_{L^2(\Sigma)} + \lambda \langle \bar{u}, u - \bar{u} \rangle_{L^2(Q)} \quad \forall u \in U_{ad}.
\]
In this case, the adjoint is defined by \cref{eq:generalized-problem-adj} with $a_Q = 0$, $a_\Sigma = \bar{y} - y_\Sigma$ and $a_\Omega = 0$.
One can show (see \cite[Satz 3.21, p.\ 131]{Troeltzsch}) that a control $\bar{u} \in U_{ad}$ is optimal if and only if the adjoint state fulfills the variational inequality
\[
	\iint_Q ( \beta p + \lambda \bar{u} ) ( u - \bar{u} ) \dd x \dd t \geq 0 \quad \forall u \in U_{ad}.
\]

For the discrete case, we define
\[
	G_I^h(g_I) \coloneqq E_V A_h^{-1} \boldsymbol{g_I}(g_I)
\]
and then
\[
	S_h \coloneqq E_\Sigma G_I^h \beta \cdot.
\]
Given that $S_h$ is linear and continuous in this case as well, \cref{eq:generalized-problem-adj} gives us the existence of a unique solution $\bar{u}_h$ in the discrete setting.

As a next step we would like to apply \cref{thm:variational-ineq} again, too. Just as in the continuous case, we define it via \cref{eq:generalized-problem-adj-dG} with the choices $a_Q = 0$, $a_\Sigma = \bar{y}_h - y_\Sigma$ and $a_\Omega = 0$.
\begin{theorem}
There exists a unique optimal control $\bar{u}_h \in L^2(Q)$ for \cref{eq:InnerOptimalControl-restricted}.

It can be characterized with a variational inequality: A control $\bar{u}_h \in L^2(Q)$ is optimal if and only if with the associated adjoint state $\bar{p}_h$ defined as the solution of \cref{eq:generalized-problem-adj-dG} with the choices $a_Q = 0$, $a_\Sigma = \bar{y}_h - y_\Sigma$ and $a_\Omega = 0$ fulfills the following variational inequality
\[
	\langle \beta(x, t) \bar{p}_h (x, t) + \lambda \bar{u}_h(x, t), u_h - \bar{u}_h \rangle_{L^2(Q)} \geq 0 \quad \forall u_h \in L^2(Q)
\]
almost everywhere.
\end{theorem}
\begin{proof}
The proof is identical to the one of \cref{thm:discrete-variational-ineq} with the respective choices of $a_Q$, $a_\Sigma$ and $a_\Omega$ adjusted to the interior heating situation.
\end{proof}
The overall optimality system in this case turns out to be
\begin{IEEEeqnarray*}{c}
\begin{IEEEeqnarraybox}{rCl"rCl}
A(y_h, v_h) &=& \langle \beta u_h, v_h \rangle_{L^2(Q)} & A(v_h, p_h) &=& \langle y_h - y_\Sigma, v_h \rangle_{L^2(\Sigma)}
\end{IEEEeqnarraybox} \\
u_h = - \lambda^{-1} \beta p_h.
\end{IEEEeqnarray*}

In order to replicate the convergence theorem \cref{thm:optimal-control-convergence}, we need to make assumption \cref{as:continuous-Hs-regularity} again.
However, this alone is not enough: During the proof of \cref{thm:optimal-control-convergence} we used the estimate
\[
	\left\| \bar{y}(T) - \bar{y}^h(T) \right\|_{L^2(\Omega)} \leq \left\lDG \bar{y} - \bar{y}^h \right\rDG.
\]
In contrast, we now use the observation operator $E_\Sigma$ instead of $E_T$, which means we have to estimate
\[
	\left\| \bar{y} - \bar{y}^h \right\|_{L^2(\Sigma)} \leq C \left\lDG \bar{y} - \bar{y}^h \right\rDG.
\]
If $\alpha > 0$, this trivially holds for $C = \frac{1}{\alpha}$, but it's not clear why it would hold for $\alpha = 0$. Therefore, we obtain the following convergence theorem:
\begin{theorem}
Let for $\alpha > 0$ be $\bar{u}$ be the optimal control for the continuous problem \cref{eq:InnerOptimalControl-restricted} and let \cref{as:continuous-Hs-regularity} hold. Moreover, be $\bar{u}_h$ the discretized solution to \cref{eq:f-Sh}.
Then there is a $C > 0$ independent of $h$, such that:
\[
	\| \bar{u} - \bar{u}_h \|_{L^2(Q)} + \| \bar{y} - \bar{y}_h \|_{L^2(\Sigma)} \leq C h^{\min \{ s, p+1\} - 1} \left( \| \bar{u} \|_{L^2(Q)} + \| y_\Omega \|_{L^2(\Omega)} \right).
\]
\end{theorem}
\begin{proof}
Under the assumption $\alpha > 0$, the proof is analogous to the one of \cref{thm:optimal-control-convergence}. Instead of $z_h$ and $z$ one uses $y_\Omega$ directly though, as the state is now linearly dependent on the control and not affine-linear as in the boundary control case.
\end{proof}
\end{document}