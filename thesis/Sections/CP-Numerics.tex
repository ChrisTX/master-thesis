\documentclass[../thesis.tex]{subfiles}

\begin{document}
\chapter{Discretization of the control problem}
\label{sec:Disc-Control-Problem}
In this chapter, we're going back to the optimal control problem introduced in \cref{sec:OptimalControlProblem}.
We aim to apply the discontinuous Galerkin method introduced in \cref{sec:dG-method} and the convergence results from \cref{sec:dG-numerics} to prove convergence of the then discretized optimal control problem. 
\section{Existence of solutions for a generalized problem}
We're going to work with weak solutions in appropriate spaces. To this end, we introduce a generalized form of the state equation, that can cover various formulations of the optimal control problem:
\begin{equation}
\label{eq:generalized-problem}
\begin{IEEEeqnarraybox}[][c]{rCl"lCl}
\frac{\partial y}{\partial t} (x, t) - \lapl y(x, t) &=& g_I(x, t) & \text{for } (x, t) \in Q &\coloneqq& \Omega \times (0, T), \\
n_x(x,t) \cdot \nabla_x y(x, t) + \alpha y(x, t) &=& g_R(x, t) & \text{for } (x, t) \in \Sigma &\coloneqq& \partial \Omega \times (0, T) \\
y(x, 0) &=& y_0(x) & \text{for } (x, t) \in \Omega,
\end{IEEEeqnarraybox}
\end{equation}
where we assume that $\Omega$ is a bounded Lipschitz domain, $T > 0$ is a fixed time and $\alpha$ being a constant greater or equal to zero. The functions $g_I(x, t) \in L^2(Q)$ and $g_R(x, t) \in L^2(\Sigma)$ act as generic right hand sides.

Formally, we test the first equation of \cref{eq:generalized-problem} by multiplying with a function $v$ and integrating over $Q$:
\begin{IEEEeqnarray*}{rCl}
	\iint_Q \left(\frac{\partial y}{\partial t} - \lapl y\right) v \dd x \dd t &=& \iint_Q \frac{\partial y}{\partial t} v \dd x \dd t - \iint_Q \lapl y v \dd x \dd t \\
	&=& \iint_Q g_I v \dd x \dd t
\end{IEEEeqnarray*}
We proceed by splitting the integral on the left hand side and consider each part individually.
For the term containing the time derivative, we then obtain using integration by parts and using that $y(\cdot, 0) = y_0(\cdot)$:
\begin{IEEEeqnarray*}{rCl}
	\iint_Q \frac{\partial y}{\partial t} v \dd x \dd t &=& - \iint_Q y \frac{\partial v}{\partial t} \dd x \dd t - \int_\Omega y(\cdot, 0) v(\cdot, 0) \dd x + \int_\Omega y(\cdot, T) v(\cdot, T) \dd x \\
	&=& - \iint_Q y \frac{\partial v}{\partial t} \dd x \dd t + \int_\Omega y(\cdot, T) v(\cdot, T) \dd x - \int_\Omega y_0(\cdot) v(\cdot, 0) \dd x.
\end{IEEEeqnarray*}
In order to deal with the term containing $\lapl y$, we use Green's first identity and exploit the given normal derivative on the boundary:
\begin{IEEEeqnarray*}{rCl}
	- \iint_Q \lapl y v \dd x \dd t &=& \iint_Q \nabla y \cdot \nabla v \dd x \dd t - \iint_{\Sigma} (\nabla y v) \cdot \nu \dd s(x) \dd t \\
	&=& \iint_Q \nabla y \cdot \nabla v \dd x \dd t - \iint_{\Sigma} \left(-\alpha y - g_R \right) v \dd s(x) \dd t \\
	&=& \iint_Q \nabla y \cdot \nabla v \dd x \dd t + \alpha \iint_{\Sigma} y v \dd s(x) \dd t - \iint_{\Sigma} g_R v \dd s(x) \dd t.
\end{IEEEeqnarray*}

Adding these two terms up again yields
\begin{IEEEeqnarray*}{rCl}
	\iint_Q \left(\frac{\partial y}{\partial t} - \lapl y\right) v \dd x \dd t &=& \iint_Q \nabla y \cdot \nabla v \dd x \dd t - \iint_Q y \frac{\partial v}{\partial t} \dd x \dd t \\
	&& \quad {} + \alpha \iint_{\Sigma} y v \dd s(x) \dd t + \int_\Omega y(\cdot, T) v(\cdot, T) \dd x \\
	&& \quad {} - \int_\Omega y_0(\cdot) v(\cdot, 0) \dd x  - \iint_{\Sigma} g_R v \dd s(x) \dd t.
\end{IEEEeqnarray*}
Reorganizing the terms gives us the formulation
\begin{equation}
\label{eq:generalized-weak-form}
\begin{IEEEeqnarraybox}[][c]{l}
	\iint_Q \nabla y \cdot \nabla v \dd x \dd t - \iint_Q y \frac{\partial v}{\partial t} \dd x \dd t + \alpha \iint_{\Sigma} y v \dd s(x) \dd t + \int_\Omega y(\cdot, T) v(\cdot, T) \dd x \qquad \\
	\IEEEeqnarraymulticol{1}{r}{ {} = \iint_Q g_I v \dd x \dd t + \int_\Omega y_0 v(\cdot, 0) \dd x + \iint_{\Sigma} g_R v \dd s(x) \dd t }.
\end{IEEEeqnarraybox}
\end{equation}
While we have only used a formal function $v$, we conclude that in order for these transformations to work, and Green's first identity and integration by parts to be applicable we need a few requirements:
$Q$ must be a bounded domain with a piecewise smooth boundary, $y$ must be twice continuously differentiable in space and once continuously differentiable in time, whereas $v$ has to be continuously differentiable in both space and time directions.

Before proceeding, we consider the following partial differential equation as well:
\begin{equation}
\label{eq:generalized-adjoint-problem}
\begin{IEEEeqnarraybox}[][c]{rCl"lCl}
- \frac{\partial p}{\partial t} (x, t) - \lapl p(x, t) &=& d_I(x, t) & \text{for } (x, t) \in Q, \\
n_x(x,t) \cdot \nabla_x p(x, t) + \alpha p(x, t) &=& d_R(x, t) & \text{for } (x, t) \in \Sigma, \\
p(x, T) &=& p_T(x) & \text{for } (x, t) \in \Omega,
\end{IEEEeqnarraybox}
\end{equation}
where $d_I(x, t) \in L^2(Q)$ and $d_R(x, t) \in L^2(\Sigma)$ are left as generic right hand side functions for now.
The motivation for considering this problem is that it will turn out to play an important role for defining the so called adjoint state en route to an optimality system for the optimal control problems considered.

One could perform the analogous procedure we just performed for $p$.
Instead of doing that, we consider the function $\tilde{p}(x, t) \coloneqq p(x, T - t)$.
Observing that we have
\[
	\lapl \tilde{p}(x, t) = \lapl p(x, T - t) \quad \text{and} \quad \frac{\partial \tilde{p}}{\partial t}(x, t) = - \frac{\partial p}{\partial t}(x, T - t),
\] 
the boundary value problem for $p$ can be written instead as
\begin{IEEEeqnarray*}{rCl}
\frac{\partial \tilde{p}}{\partial t} - \lapl \tilde{p} &=& d_I(x, T - t) \\
\partial_\nu \tilde{p} + \alpha \tilde{p} &=& d_R(x, T - t) \\
\tilde{p}(0) &=& p(T) \\
p(x, t) &=& \tilde{p}(x, T - t)
\end{IEEEeqnarray*}
by applying the results for $y$, and substituting $\tilde{p}$ again, one arrives at the following weak formulation:
\begin{IEEEeqnarray*}{l}
	\iint_Q \nabla p \cdot \nabla v \dd x \dd t + \iint_Q p \frac{\partial v}{\partial t} \dd x \dd t + \alpha \iint_{\Sigma} p v \dd s(x) \dd t + \int_\Omega p(\cdot, 0) v(\cdot, 0) \dd x \qquad\qquad \\
	\IEEEeqnarraymulticol{1}{r}{ {} = \int_\Omega p(T) v(\cdot, T) \dd x + \iint_Q d_I v \dd x \dd t + \iint_\Sigma d_R v \dd s(x) \dd t }.
\end{IEEEeqnarray*}
Assuming that $\Omega$ is bounded Lipschitz domain and that $\alpha \geq 0$ holds, one can prove that these formal calculations can be put into a setting where they make sense and have a unique solution, which continuously depends on the right hand side.
To this end, one defines two different spaces for $v$ and $y$, one which has weak continuous space derivatives and one which has both, weak continuous space and time derivatives (see \cite[p.\ 111]{Troeltzsch} for the definition of the spaces and \cite[Satz 3.9, p.\ 112]{Troeltzsch} for the existence and uniqueness theorem).
This however is not a result directly useful to us, as we would like to seek $v$ and $y$ in the same space. As of such, we're going to skip this discussion here.

Instead, one can prove these solutions lie in a more general space in which is what we want to work with. As of such, we're first going to introduce that space, following \cite[3.4.1 Abstrakte Funktionen, p.\ 113ff.]{Troeltzsch}.
Further information on these spaces can be found for example in \cite{Wloka} or in \cite{HillePhillips}.
\begin{definition}[Abstract functions]
Let $\{ X, \| \cdot \|_X \}$ be a real Banach space.
An mapping of $[a, b] \subset \R$ into $X$ is called abstract function.
\end{definition}
We proceed to define some spaces of abstract functions that we will work with:
\begin{definition}
Let $\{ X, \| \cdot \|_X \}$ be a real Banach space.
An abstract function $y : [a, b] \to X$ is called continuous in the point $t \in [a, b]$, if $\tau \to t, \tau \in [a, b]$ implies the convergence $y(\tau) \to y(t)$ in X.

We denote by $C([a, b], X)$ the Banach space of all continuous abstract functions $y : [a, b] \to X$, equipped with the norm
\[
	\| y \|_{C([a, b], X)} = \max_{t \in [a, b]} \| y(t) \|_X.
\]
\end{definition}
As a next step, we aim to introduce $L^p$-spaces of abstract functions.
For this purpose, we require a notion of measurable functions in these spaces.
\begin{definition}
An abstract function $y : [a, b] \to X$ is called step function if there exist finitely many elements $y_i \in X$ and Lebesgue-measurable, pairwise disjoint sets $M_i \subset [a, b]$, $i = 1, \ldots, m$ with $[a, b] = \bigcup_{i=1}^m M_i$ and $y(t) = y_i$ for all $t \in M_i$, $i = 1, \ldots, m$.

If for an abstract function $y : [a, b] \to X$ there exists a sequence of step functions $\{ y_k \}_{k=1}^\infty$ such that $y(t) = \lim_{k\to\infty} y_k(t)$ for almost all $t \in [a, b]$, then we call $y$ measurable.
\end{definition}
This permits us to introduce the $L^p$-spaces we need:
\begin{definition}
\label{def:Bochner-space}
For a real Banach space $\{ X, \| \cdot \|_X \}$ let $L^p(a, b; X)$, $1 \leq p < \infty$ be the linear space of the equivalence classes of measurable abstract functions $y : [a, b] \to X$ with the property
\[
	\int_a^b \| y(t) \|_X^p \dd t < \infty.
\]
The norm of this space is
\[
	\| y \|_{L^p(a, b; X)} \coloneqq \left( \int_a^b \| y(t) \|_X^p \dd t \right)^{\frac{1}{p}}.
\]
The space $L^\infty(a, b; X)$ is the space of equivalence classes of measurable abstract functions $y : [a, b] \to X$ with
\[
	\| y \|_{L^\infty(a, b; X)} \coloneqq \esssup_{[a, b]} \| y(t) \|_{X} < \infty.
\]
\end{definition}
As a next step, we need a notion of derivatives for this space. For this, we introduce vector-valued distributions, see \cite[p.\ 117]{Troeltzsch}.
However, before we can do that, we need to introduce an integral operating on the $L^p$-spaces.
In their definition we used a Lebesgue integral over the real-valued $\| \cdot \|_X$ norm, but we now need to define integrals via the means of the step functions that we just introduced:
\begin{definition}
For any of the $L^p(a, b; X)$, $p \in [1, \infty]$ spaces we define the so called Bochner-integral as follows:
Given a step function $y$, it is defined as
\[
	\int_a^b y(t) \dd t \coloneqq \sum_{i=1}^m y_i |M_i|.
\]
Note that this is an element of $X$.

On the other hand, given a measurable function $y$ with a series of step functions $\{ y_k\}_{k=1}^\infty$, we define
\[
	\int_a^b y(t) \dd t = \lim_{k \to \infty} \int_a^b y_k(t) \dd t.
\]
\end{definition}
It can be shown, see e.g.\ \cite{HillePhillips}, that the Bochner-integral converges independently of the choice of the series $\{ y_k \}_{k=1}^\infty$.

Using the Bochner-integral we can now introduced vector-valued derivatives:
\begin{definition}
For a given function $y \in L^2(0, T; X)$ we define a vector-valued distribution $\distT : C_0^\infty(0, T) \to X$ by
\[
	\distT \varphi \coloneqq \int_0^T y(t) \varphi(t) \dd t \quad \forall \varphi \in C_0^\infty(0, T).
\] 
The derivative $\distT'$ is defined by
\[
	\distT' \varphi \coloneqq - \int_0^T y(t) \varphi'(t) \dd t.
\]
If a function $w = w(t)$ in $L^1(0, T; X)$ exists with the property that
\[
	\distT' \varphi = - \int_0^T y(t) \varphi'(t) \dd t = \int_0^T w(t) \varphi(t) \dd t \quad \forall \varphi \in C_0^\infty(0, T),
\]
then we define the distributional derivative $y'(t) \coloneqq w(t)$.
\end{definition}
With these definitions, we can introduce the space we were looking for (see \cite[p.\ 118]{Troeltzsch}):
\begin{definition}
Let $W(0, T)$ be the space of all $y \in L^2(0, T; X)$ with distributional derivative $y'$ in $L^2(0, T; X^\adj)$ with the norm
\[
	\| y \|_{W(0, T)} = \left( \int_0^T ( \| y(t) \|_X^2 + \| y'(t) \|_{X^\adj}^2 ) \dd t \right)^{\frac{1}{2}} < \infty,
\]
or in other words:
\[
	W(0, T) = \left\{ y \in L^2(0, T; X) \midcolon y' \in L^2(0, T; X^\adj) \right\}.
\]
\end{definition}
Using the notion of so called Gelfand triples, the following embedding theorem can be proved:
\begin{theorem}
\label{thm:W0T-continuous-embedding}
Any $y \in W(0, T)$ can be interpreted, after a potential change on a set of zero measure, be interpreted as an element of $C([0, T], L^2(\Omega))$.
In this sense the embedding $W(0, T) \hookrightarrow C([0, T], L^2(\Omega))$ exists and is continuous, i.e.\ there exists a constant $c_E$ such that
\[
	\| y \|_{C([0, T], L^2(\Omega))} \leq c_E \| y \|_{W(0, T)} \quad \forall y \in W(0, T).
\] 
\end{theorem}
\begin{proof}
The proof can be found in \cite{Wloka} or \cite{Zeidler-IIA}.
\end{proof}
In the following we always choose $X = H^1(Q)$. A weak formulation of the problem \cref{eq:generalized-problem} in these spaces reads as follows:
Let $u \in W(0, T)$ such that equation \cref{eq:generalized-weak-form}, i.e.
\begin{IEEEeqnarray*}{l}
	\iint_Q \nabla y \cdot \nabla v \dd x \dd t - \iint_Q y \frac{\partial v}{\partial t} \dd x \dd t + \alpha \iint_{\Sigma} y v \dd s(x) \dd t + \int_\Omega y(\cdot, T) v(\cdot, T) \dd x \qquad \\
	\IEEEeqnarraymulticol{1}{r}{ {} = \iint_Q g_I v \dd x \dd t + \int_\Omega y_0 v(\cdot, 0) \dd x + \iint_{\Sigma} g_R v \dd s(x) \dd t }.
\end{IEEEeqnarray*}
holds for all $v \in W(0, T)$.
As intended, this choice of spaces permits to use the same space $W(0, T)$ for $u$ and $v$.

In order to proof that this formulation makes sense, one uses the aforementioned spaces with a stronger regularity requirement first, proves existence and uniqueness of a solution in them and then proves that the solution is in $W(0, T)$ and testing with $v \in W(0, T)$ makes sense. For more details, see \cite[3.3 Schwache LÃ¶sungen in $W^{1, 0}_2(Q)$]{Troeltzsch}.
Eventually, one can prove the following result:
\begin{theorem}
\label{thm:generalized-problem-solution}
The system \cref{eq:generalized-weak-form} omits a unique solution with the estimate
\[
	\| y \|_{W(0, T)} \leq c_w ( \| g_I \|_{L^2(Q)} + \| g_R \|_{L^2(\Sigma)} + \| y_0 \|_{L^2(\Omega)} )
\] 
with a constant $c_w > 0$ independent of $g_I$, $g_R$ and $y_0$.
Consequentially, the mapping $(g_I, g_R, y_0) \mapsto f$ is continuous from $L^2(Q) \times L^2(\Sigma) \times L^2(\Omega)$ to $W(0, T)$, especially $C([0, T], L^2(\Omega))$.
\end{theorem}
\begin{proof}
For the existence of a solution in a more general space, see \cite[Satz 3.9, p.\ 112]{Troeltzsch} and more specifically \cite[Satz 7.9, p.\ 289]{Troeltzsch}. The proof as presented there follows the monograph \cite{Ladyzhenskaya}.

In order to prove solution lying in $W(0, T)$ see \cite[Satz 3.12, p.\ 120]{Troeltzsch} and for the continuity result, see \cite[Satz 3.13, p.\ 121]{Troeltzsch}.
One can also work with $W(0, T)$ directly to simplify the proof, see \cite{Wloka}. However, this approach shares less analogies with the work needed for treating the elliptic optimal control case.
\end{proof}
Due to \cref{thm:generalized-problem-solution}, linear and continuous operators $G_Q : L^2(Q) \to W(0, T)$, $G_\Sigma : L^2(\Sigma) \to W(0, T)$ and $G_0 : L^2(\Omega) \to W(0, T)$ exist such that
\[
	y = G_Q g_I + G_\Sigma g_R + G_0 y_0.
\]
\section{Optimal control with boundary controls}
\label{sec:boundary-control-opt}
We'll now go back to the optimal control problem with boundary controls:
\begin{problem}
\label{prb:BoundaryOptimalControl-restricted}
\begin{IEEEeqnarray*}{c}
\min J(y, u) = \frac{1}{2} \int_\Omega \left( y(x, T) - y_\Omega(x) \right)^2 \dd x + \frac{\lambda}{2} \iint_{\Sigma} u(x, t)^2 \dd s(x) \dd t \\
\noalign{\noindent subject to\vspace{\jot}}
\begin{IEEEeqnarraybox}{rCl"l}
\frac{\partial y}{\partial t} - \lapl y &=& 0 & \text{in } Q \\
\partial_\nu y + \alpha y &=& \beta u & \text{in } \Sigma\\
y(x, 0) &=& y_0(x) & \text{in } \Omega
\end{IEEEeqnarraybox} \\
\noalign{\noindent and\vspace{\jot}}
u_a(x, t) \leq u(x, t) \leq u_b(x, t) \quad \text{a.e.\ in $\Sigma$}.
\end{IEEEeqnarray*}
\end{problem}
As before, we assume that $\Omega$ is a Lipschitz domain, $y_\Omega \in L^2(\Omega)$, $\alpha \geq 0$, $\beta \in L^\infty(\Sigma)$ and $\lambda > 0$.
Moreover, let $u_a, u_b \in L^2(\Sigma)$ with $u_a(x, t) \leq u_b(x, t)$ for almost all $(x, t) \in \Sigma$.
Note that we will exclude the case $\lambda = 0$, as the control does not have to be unique in that scenario and this will therefore prevent us from finding a convergence proof.

Due to \cref{thm:generalized-problem-solution}, we can express $y$ as
\[
	y = G_\Sigma \beta u + G_0 y_0.
\]
If we introduce an observation operator $E_T : y \mapsto y(T)$, we can use the embedding property of the solution and obtain
\[
	y(T) = E_T (G_\Sigma \beta u + G_0 y_0).
\]
Because of \cref{thm:W0T-continuous-embedding}, the operator $E_T$ is continuous from $W(0, T)$ to $L^2(\Omega)$.
By defining the so called state operator $S \coloneqq E_T G_\Sigma \beta \cdot$, we notice that with $z \coloneqq y_\Omega - (G_0 y_0)(T)$ it holds:
\[
	y(T) - y_\Omega = S u + (G_0 y_0)(T) - y_\Omega = S u - z.
\]
For the discontinuous Galerkin method introduced in \cref{sec:dG-method}, we can define an analogous operator. 
To that end, we define operators that map $g_I$, $g_R$ and $y_0$ into their discrete right hand sides, i.e.\ we define $\boldsymbol{g_I} : L^2(Q) \to \R^m$, $\boldsymbol{g_R} : L^2(\Sigma) \to \R^m$ and $\boldsymbol{y_0} : L^2(\Omega) \to \R^m$ component-wise by
\begin{IEEEeqnarray*}{rCl}
	\boldsymbol{g_I}[i] (g_I) &=& \iint_Q g_I \varphi_i \dd x \dd t, \\
	\boldsymbol{g_R}[i] (g_R) &=& \iint_\Sigma g_R \varphi_i \dd s(x) \dd t, \\
	\boldsymbol{y_0}[i] (y_0) &=& \int_\Omega y_0 \varphi_i(\cdot, 0) \dd x.
\end{IEEEeqnarray*}
Moreover, we define an operator to map discrete solutions of \cref{eq:dg-discrete-prob} into the function they correspond to in $\Shp(\meshT_N)$:
\begin{IEEEeqnarray*}{rCl}
	E_V : \R^m &\to& \Shp(\meshT_N) \\
	\boldsymbol{v} &\mapsto& v \coloneqq \sum_{j=1}^M \boldsymbol{v}[j] \varphi_j(x, t).
\end{IEEEeqnarray*}
Then, we can define the solution operators
\begin{IEEEeqnarray*}{rCl}
	G_\Sigma^h (g_R) &\coloneqq& E_V A_h^{-1} \boldsymbol{g_R} (g_R), \\
	G_0^h (y_0) &\coloneqq& E_V A_h^{-1} \boldsymbol{y_0} (y_0).
\end{IEEEeqnarray*}
Note that these operators are well defined for $\sigma \geq 4 c_K$ due to \cref{thm:A-bijective}. Therefore, the rest of this discussion assumes that \cref{as:mesh-assumptions} holds and that $\sigma$ is chosen such that $\sigma \geq 4 c_K$.
As of such, we can define a discrete analogous to $S$, called $S_h$:
\[
	y_h(T) - y_\Omega = S_h u_h + (G_0^h y_0)(T) - y_\Omega = S_h u_h - z_h,
\]
where $S_h \coloneqq E_T G_\Sigma \beta \cdot$ and $z_h \coloneqq y_\Omega - (G_0^h y_0)(T)$.
Note that both operators, $S$ and $S_h$ work from $L^2(\Sigma)$ to $L^2(\Omega)$. One can immediately see that $S_h$ is a linear operator.
Using that the term $\| y_h(T) \|_{L^2(\Omega)}$ is contained in $\| y_h \|_A$ together with the ellipticity estimate \cref{thm:A-ellip-weak} (or alternatively, for quasi-uniform meshes, the estimate \cref{thm:Astab-est}) guarantees that $S_h$ is also a continuous operator between these spaces.

Furthermore, we introduce the set of admissible controls $U_{ad}$ as
\[
	U_{ad} \coloneqq \left\{ u \in L^2(\Sigma) \midcolon u_a(x, t) \leq u(x, t) \leq u_b(x, t) \quad \text{a.e.\ on $\Sigma$} \right\}.
\]
With this set and the operators defined, we can express the continuous and discretized optimal control problems in the following reduced form:
\begin{equation}
\label{eq:f-S}
\min_{u \in U_{ad}} J(u) \coloneqq \frac{1}{2} \| S u - z \|_{L^2(\Omega)}^2 + \frac{\lambda}{2} \| u \|_{L^2(\Sigma)}^2,
\end{equation}
and
\begin{equation}
\label{eq:f-Sh}
\min_{u_h \in U_{ad}} J_h(u_h) \coloneqq \frac{1}{2} \| S_h u_h - z_h \|_{L^2(\Omega)}^2 + \frac{\lambda}{2} \| u_h \|_{L^2(\Sigma)}^2.
\end{equation}
Note that the discretized optimal control is being searched in $U_{ad} \subset L^2(\Sigma)$ and not a discretized space.

The approach to look for $u_h$ in a non-discretized space is called variational discretization and was first introduced in \cite{Hinze}.
By applying no discretization to the control, \cite{Hinze} established error estimates for continuous finite element approaches for elliptic optimal control problems. This was later applied to parabolic problems in \cite{DeckelnickHinze}, where they use an approach via a continuous Galerkin scheme in space and a discontinuous approach in time. A similar approach can be found in \cite{MeidnerVexler-I}, which was later on generalized to the state-constrained case in \cite{MeidnerVexler-II}.

In order to discuss uniqueness and existence of solutions to these problems, we need a general result on minimizers of functionals.
For this purpose, we require some results from functional analysis.
Each of the following theorems can be found in both \cite{Alt} and \cite{Werner}.
\begin{theorem}
\label{thm:bounded-set-seq-comp}
Any convex and closed subset of a Banach space is weakly sequentially compact.
If the space is reflexive and the set bounded on top of this, then the set is weakly compact.
\end{theorem}
This is a result of Mazur's theorem. Furthermore, one requires
\begin{theorem}
\label{thm:weakly-lower-seq-cont}
In a Banach space $U$ any convex and continuous functional $J$ is weakly lower sequentially semi-continuous, i.e.\ $u_n \rightharpoonup u$ for $n \to \infty$ implies
\[
	\lim_{n \to \infty} J(u_n) \geq J(u).
\]
\end{theorem}
With \cref{thm:bounded-set-seq-comp,thm:weakly-lower-seq-cont} introduced, we can prove the following, general result of minimizers:
\begin{theorem}
\label{thm:optimal-control-existence}
Let $\{ U, \| \cdot \|_U \}$ and $\{ H, \| \cdot \|_H \}$ be real Hilbert spaces and a nonempty, closed and convex set $U_{ad} \subset U$, an element $z \in H$ and a constant $\lambda > 0$.
Moreover, set $S : U \to H$ be a linear and continuous operator.
Then the quadratic optimization problem in the Hilbert space
\[
	\min_{u \in U_{ad}} J(u) \coloneqq \frac{1}{2} \| Su - z \|_H^2 + \frac{\lambda}{2} \| u \|_U^2
\]
admits an optimal solution $\bar{u}$. If $\lambda$ is positive or $S$ injective, then is is uniquely determined.
\end{theorem}
\begin{proof}
We present the proof given by \cite[Satz 2.14]{Troeltzsch} and \cite[Satz 2.15]{Troeltzsch}.

Assume $U_{ad}$ is bounded. In that case, because $J \geq 0$, the infimum of all possible function values exists:
\[
	j \coloneqq \inf_{u \in U_{ad}} J(u).
\]
Hence, a sequence $\{ u_n \}_{n=1}^\infty \subset U_{ad}$ with $J(u_n) \to j$ for $n \to \infty$ exists.
As $U_{ad}$ is bounded, closed and convex, one knows by \cref{thm:bounded-set-seq-comp} that $U_{ad}$ is weakly sequentially compact, which in turn implies existence of a weakly convergent subsequence $\{ u_{n_k} \}_{k=1}^\infty \subset U_{ad}$ weakly converging against a $\bar{u} \in U_{ad}$.

The continuity of $S$ also implies that the functional $J$ is continuous.
Moreover, $\lambda \geq 0$ ensures that $J$ is a convex functional.
We now know by \cref{thm:weakly-lower-seq-cont} that $J$ is weakly lower semi-continuous and thus has the property
\[
	j \leq J(\bar{u}) \leq \min_{k \to \infty} \inf J(u_{n_k}) = j.
\]
Therefore $J(\bar{u}) = j$ holds, which was to be shown.

In the case $\lambda > 0$ one can even see $J$ being strictly convex, ensuring that $\bar{u}$ is also uniquely determined.

Now let $U_{ad}$ be as in the theorem and thus not be bounded anymore. As $U_{ad}$ is nonempty, some $u_0 \in U_{ad}$ exists. For any $u \in U$ with $\| u \|_U^2 > 2 \lambda^{-1} J(u_0)$, one has
\[
	J(u) = \frac{1}{2} \| S u - z \|_H^2 + \frac{\lambda}{2} \| u \|_U^2 \geq \frac{\lambda}{2} \| u \|_U^2 > J(u_0).
\]
Hence, one can simply search within the bounded, convex and closed set
\[
	U_{ad} \cap \{ u \in U : \| u \|_U^2 \leq 2 \lambda^{-1} J(u_0) \}
\]
and use the previously derived result. Any function not in this restricted set has to have a function value greater than $u_0$ and therefore cannot be a minimizer.
\end{proof}
By pointing out that $U_{ad}$ as we defined it is going to be convex, closed and nonempty we can apply the result of \cref{thm:optimal-control-existence} directly. Note that $U_{ad}$ is nonempty since $u_a, u_b \in L^2(\Sigma)$ are both necessary admissible by the way we defined $U_{ad}$.
In conclusion \cref{thm:optimal-control-existence} implies that both \cref{eq:f-S} and \cref{eq:f-Sh} admit unique solutions, named $\bar{u}$ and $\bar{u}_h$, respectively.
Combined with the previously shown result that a solution in $L^2(\Sigma)$ implies that the optimal solution is in in the boundary projection of $\Shp(\meshT_N)$ after potentially changing it on a set of zero measure.

These unique solutions can be characterized using a so called variational inequality:
\begin{theorem}
\label{thm:variational-ineq}
Let $U$ and $H$ be real Hilbert spaces, a nonempty and convex set $U_{ad} \subset U$, $z \in H$ and a constant $\lambda \geq 0$. Let $S : U \to H$ be a linear and continuous operator.
The element $\bar{u} \in U_{ad}$ solves
\[
	\min_{u \in U_{ad}} J(u) \coloneqq \frac{1}{2} \| Su - z \|_H^2 + \frac{\lambda}{2} \| u \|_U^2
\]
if and only if
\[
	\langle S \bar{u} - z, S ( u - \bar{u} ) \rangle_H + \lambda \langle \bar{u}, u - \bar{u} \rangle_U \geq 0 \quad \text{for all } u \in U_{ad}.
\]
\end{theorem}
\begin{proof}
See \cite[Satz 2.22]{Troeltzsch} for this statement.
\end{proof}
By using the adjoint operator $S^*$, one can characterize the solution of an optimal control problem using the so called adjoint state:
\begin{IEEEeqnarray*}{r"rCl"l}
	& \langle S \bar{u} - z, S ( u - \bar{u} ) \rangle_H + \lambda \langle \bar{u}, u - \bar{u} \rangle_U &\geq& 0 & \text{for all } u \in U_{ad}, \\
	\Longleftrightarrow & \langle S^* ( S \bar{u} - z ) + \lambda \bar{u}, u - \bar{u} \rangle_U &\geq& 0 & \text{for all } u \in U_{ad}.
\end{IEEEeqnarray*}
Hence, one can define the solution by making use of the adjoint state $p \coloneqq  S^* ( S \bar{u} - z ) $.
For the parabolic case, it is however easier to work with the form given in \cref{thm:variational-ineq} and proving equivalence for a reasonably defined $p$ rather than attempting to determine $S^*$ directly.

In order to be able to treat the various formulations of the optimal control problem at once, we introduce a general parabolic problem that will serve as the adjoint problem: Consider the boundary value problem
\begin{equation}
\label{eq:generalized-problem-adj}
\begin{IEEEeqnarraybox}[][c]{rCl"l}
-\frac{\partial p}{\partial t} (x, t) - \lapl p(x, t) &=& a_Q(x, t) & \text{for } (x, t) \in Q, \\
n_x(x,t) \cdot \nabla_x p(x, t) + \alpha p(x, t) &=& a_\Sigma(x, t) & \text{for } (x, t) \in \Sigma, \\
p(x, T) &=& a_\Omega(x) & \text{for } x \in \Omega,
\end{IEEEeqnarraybox}
\end{equation}
with $\alpha$ being a constant as before and $a_Q \in L^2(Q)$, $a_\Sigma \in L^2(\Sigma)$ and $a_\Omega \in L^2(\Omega)$.
Using a transformation $\tilde{p}(x, t) = p(x, T - t)$, this can be shown to have a unique weak solution solution by applying \cref{thm:generalized-problem-solution} to the transformation and transforming back.
We have previously derived a discretized solution for this problem in \cref{sec:adj-dG-treatment}. As argued there, the same transformation together with \cref{thm:A-bijective} and \cref{thm:dg-convergence} delivers the existence of a unique discrete solution that converges in the $\lDG \cdot \rDG$ norm.

For the boundary optimal control case, the solution of \cref{eq:generalized-problem-adj} with the choices $a_Q = 0$, $a_\Sigma = 0$, $a_\Omega = \bar{y}(T) - y_\Omega$ characterizes the solution $\bar{u} \in U_{ad}$ equivalently to \cref{thm:variational-ineq} with the inequality
\[
	\langle \beta \bar{p} + \lambda \bar{u}, u - \bar{u} \rangle_U \geq 0 \quad \forall u \in U_{ad}.
\]
This result has been shown in \cite[Lemma 3.17 and Satz 3.18, p.\ 126f.]{Troeltzsch}.

We aim to prove the analogous result for the discrete case, i.e.\ that the discretization introduced in \cref{sec:adj-dG-treatment} with similar right hand sides also delivers an adjoint state with the same functionality as in the continuous case.

Again, for the sake of covering multiple possible problem formulations at once, we work with \cref{eq:generalized-problem}, where each boundary term depends on controls, named $v$, $u$ and $w$, respectively.
For $b_Q \in L^\infty(Q)$, $b_\Sigma \in L^\infty(\Sigma)$, $b_\Omega \in L^\infty(\Omega)$ and given controls $v \in L^2(Q)$, $u \in L^2(\Sigma)$ and $w \in L^2(\Omega)$ as well as the constant $\alpha$, we define the problem
\begin{equation}
\label{eq:generalized-problem-state}
\begin{IEEEeqnarraybox}[][c]{rCl"l}
\frac{\partial y}{\partial t} (x, t) - \lapl y(x, t) &=& b_Q(x, t) v(x, t) & \text{for } (x, t) \in Q, \\
n_x(x,t) \cdot \nabla_x y(x, t) + \alpha y(x, t) &=& b_\Sigma(x, t) u(x, t) & \text{for } (x, t) \in \Sigma, \\
y(x, 0) &=& b_\Omega(x) w(x) & \text{for } x \in \Omega.
\end{IEEEeqnarraybox}
\end{equation}
Alas, we have right hand sides that consist of the product between some coefficients and a control.
 
By applying the discontinuous Galerkin formulation for \cref{eq:generalized-problem-state} we obtain
\begin{equation}
\label{eq:generalized-problem-state-dG}
A_h \boldsymbol{y} = \boldsymbol{g_I}(b_Q v) + \boldsymbol{g_R}(b_\Sigma u) + \boldsymbol{y_0}(b_\Omega w).
\end{equation}
and by treating \cref{eq:generalized-problem-adj} accordingly, we obtain
\begin{equation}
\label{eq:generalized-problem-adj-dG}
A_h^\tp \boldsymbol{p} = \boldsymbol{g_I}(a_Q) + \boldsymbol{g_R}(a_\Sigma) + \boldsymbol{y_T}(a_\Omega),
\end{equation}
where $\boldsymbol{y_T} : L^2(\Omega) \to \R^m$ is defined in an analogous fashion as before:
\begin{IEEEeqnarray*}{rCl}
	\boldsymbol{y_T}[i] (y_T) &=& \int_\Omega y_T \varphi_i(\cdot, T) \dd x.
\end{IEEEeqnarray*}
One would assert that \cref{eq:generalized-problem-adj-dG} is the adjoint in the given sense to \cref{eq:generalized-problem-state-dG}, because \cref{eq:generalized-problem-adj} is the adjoint problem with analogous right hand sides to \cref{eq:generalized-problem-state} for the continuous case.
In order to prove this, we need the following statement, which corresponds to the result \cite[Lemma 3.17, p.\ 126]{Troeltzsch} for the continuous formulation.
\begin{lemma}
\label{thm:discrete-adj-state-helper}
The solution $y_h$ of \cref{eq:f-Sh} and $p_h$ of \cref{eq:generalized-problem-adj-dG} with the choices of $a_Q = 0$, $a_\Sigma = 0$, and $a_\Omega = y_h(T) - y_\Omega$ adhere to the following equation:
\[
\begin{IEEEeqnarraybox}[][c]{l}
	\iint_Q b_Q v p_h \dd x \dd t + \int_\Omega b_\Omega w p_h(\cdot, 0) \dd x + \iint_{\Sigma_R} b_\Sigma u p_h \dd s(x) \dd t \\
	\IEEEeqnarraymulticol{1}{r}{ \qquad\qquad {} = \iint_Q a_Q y_h \dd x \dd t + \int_\Omega a_\Omega y_h(\cdot, T) \dd x + \iint_{\Sigma_R} a_\Sigma y_h \dd s(x) \dd t. }
\end{IEEEeqnarraybox}
\]
\end{lemma}
\begin{proof}
Our formulation for the discretization of \cref{eq:generalized-problem-state} and \cref{eq:generalized-problem-adj} are respectively:
\begin{IEEEeqnarray*}{rCl}
	a(y_h, v_h) + b(y_h, v_h) &=& \iint_Q b_Q v v_h \dd x \dd t + \int_\Omega b_\Omega w v_h(\cdot, 0) \dd x + \iint_{\Sigma_R} b_\Sigma u v_h \dd s(x) \dd t, \\
	a(v_h, p_h) + b(v_h, p_h) &=& \iint_Q a_Q v_h \dd x \dd t + \int_\Omega a_\Omega v_h(\cdot, T) \dd x + \iint_{\Sigma_R} a_\Sigma v_h \dd s(x) \dd t,
\end{IEEEeqnarray*}
where each equation holds for all $v_h \in \Shp(\meshT_N)$.
We test the first equation with $p_h$ and the second one with $y_h$:
\begin{IEEEeqnarray*}{rCl}
	a(y_h, p_h) + b(y_h, p_h) &=& \iint_Q b_Q v p_h \dd x \dd t + \int_\Omega b_\Omega w p_h(\cdot, 0) \dd x + \iint_{\Sigma_R} b_\Sigma u p_h \dd s(x) \dd t \\
	a(y_h, p_h) + b(y_h, p_h) &=& \iint_Q a_Q y_h \dd x \dd t + \int_\Omega a_\Omega y_h(\cdot, T) \dd x + \iint_{\Sigma_R} a_\Sigma y_h \dd s(x) \dd t.
\end{IEEEeqnarray*}
Noticing that the left hand sides of both equations are the same gives us the desired statement:
\[
\begin{IEEEeqnarraybox}[][c]{l}
	\iint_Q b_Q v p_h \dd x \dd t + \int_\Omega b_\Omega w p_h(\cdot, 0) \dd x + \iint_{\Sigma_R} b_\Sigma u p_h \dd s(x) \dd t \\
	\IEEEeqnarraymulticol{1}{r}{ \qquad\qquad {} = \iint_Q a_Q y_h \dd x \dd t + \int_\Omega a_\Omega y_h(\cdot, T) \dd x + \iint_{\Sigma_R} a_\Sigma y_h \dd s(x) \dd t. }
\end{IEEEeqnarraybox}
\]
\end{proof}
Using \cref{thm:discrete-adj-state-helper}, we can prove:
\begin{theorem}
\label{thm:discrete-variational-ineq}
A control $\bar{u}_h \in U_{ad}$ is optimal for \cref{eq:f-Sh} if and only if with the associated adjoint state $\bar{p}_h$ defined as the solution of \cref{eq:generalized-problem-adj-dG} with the choices $a_Q = 0$, $a_\Sigma = 0$ and $a_\Omega = \bar{y}_h(T) - y_\Omega$ fulfills the following variational inequality
\[
	\langle \beta(x, t) \bar{p}_h (x, t) + \lambda \bar{u}_h(x, t), u_h - \bar{u}_h \rangle_{L^2(\Sigma)} \geq 0 \quad \text{for all } u_h \in U_{ad}
\]
almost everywhere.
\end{theorem}
\begin{proof}
This proof is inspired by \cite[Satz 3.19, p.\ 128f.]{Troeltzsch}.
Using the previously derived minimization formulation \cref{eq:f-Sh} and employing \cref{thm:variational-ineq}, we know
\begin{IEEEeqnarray*}{rCl}
	0 &\leq& \langle S_h \bar{u}_h - z_h, S_h(u_h - \bar{u}_h) \rangle_{L^2(\Omega)} + \lambda\langle\bar{u}_h, u_h - \bar{u}_h \rangle_{L^2(\Sigma)} \\
	&=& \int_\Omega (\bar{y}_h(T) - y_\Omega)(y_h(T) - \bar{y}_h(T)) \dd x + \lambda \iint_\Sigma \bar{u}_h ( u_h - \bar{u}_h ) \dd s(x) \dd  t.
\end{IEEEeqnarray*}
for all $u_h \in U_{ad}$ and their associated states $y_h$.
Note that
\[
	S_h u_h - S_h \bar{u}_h = S_h u_h + z_h - S_h \bar{u}_h - z_h = y_h(T) - \bar{y}_h(T).
\]
Using \cref{thm:discrete-adj-state-helper} with $b_Q = 0$, $b_\Sigma = \beta$, $b_\Omega = 0$, $a_Q = 0$, $a_\Sigma = 0$, $a_\Omega = \bar{y}_h(T) - y_\Omega$, we have
\[
	\iint_\Sigma \beta p_h \tilde{u}_h \dd s(x) \dd t = \langle \bar{y}_h(T) - y_\Omega, \tilde{y}_h(T) \rangle_{L^2(\Omega)},
\]
where we set $\tilde{u}_h = u_h - \bar{u}_h$ and $\tilde{y}_h = y_h - \bar{y}_h$.
Inserting this into the variational inequality we obtain
\begin{IEEEeqnarray*}{rCl}
	0 &\leq& \int_\Omega (\bar{y}_h(T) - y_\Omega)(y_h(T) - \bar{y}_h(T)) \dd x + \lambda \iint_\Sigma \bar{u}_h ( u_h - \bar{u}_h ) \dd s(x) \dd  t \\
	&=& \iint_\Sigma \beta p_h ( u_h - \bar{u}_h ) \dd s(x) \dd t + \lambda \iint_\Sigma \bar{u}_h ( u_h - \bar{u}_h ) \dd s(x) \dd  t \\
	&=& \iint_\Sigma ( \beta p_h + \lambda \bar{u}_h ) ( u_h - \bar{u}_h ) \dd s(x) \dd t,
\end{IEEEeqnarray*}
holding for all $u_h \in U_{ad}$.
This however is precisely the inequality we aimed to derive.
\end{proof}
\begin{remark}
In the case with no restrictions made to the control, i.e.\ $u_a = -\infty$ and $u_b = \infty$, the choice of $U_{ad} = L^2(\Sigma)$ implies that the variational inequalities hold in the stronger sense
\begin{IEEEeqnarray*}{rCl}
	\beta p + \lambda \bar{u} &=& 0 \\
	\beta p_h + \lambda \bar{u}_h &=& 0
\end{IEEEeqnarray*}
almost everywhere on $\Sigma$.

Moreover, if for example $\beta$ is constant, this means that $\bar{u}_h$ can be interpreted to be a boundary projection of a function in $\Shp(\meshT_N)$, i.e. the function $-\lambda^{-1} \beta p_h \in \Shp(\meshT_N)$.
\end{remark}
An important consequence of a result like \cref{thm:discrete-variational-ineq} is that there exists a projection formula characterizing the optimal control:
\begin{theorem}
\label{thm:projection-formula}
A control $\bar{u}_h \in U_{ad}$ is optimal for \cref{eq:f-Sh} if and only if the associated adjoint state $\bar{p}_h$ fulfills the equality
\[
	\bar{u}_h = \projP_{[u_a, u_b]} \left\{ - \lambda^{-1} \beta \bar{p}_h \right\}.
\]
\end{theorem}
\begin{proof}
See \cite[p.\ 53ff.]{Troeltzsch} for the deviation of the projection formula from the variational inequality.
Note that we're only considering the case $\lambda > 0$.
\end{proof}
The same projection formula holds for the continuous case, i.e.\ $\bar{u} \in U_{ad}$ is optimal for \cref{eq:f-S} if and only if the adjoint state $\bar{p}$ fulfills
\[
	\bar{u} = \projP_{[u_a, u_b]} \left\{ - \lambda^{-1} \beta \bar{p} \right\}.
\]
This statement has been proved as \cite[Satz 3.20]{Troeltzsch}.

Bringing the optimality conditions for the continuous case into a single system, one obtains:
\begin{IEEEeqnarray*}{c}
\begin{IEEEeqnarraybox}{r"l}
\begin{IEEEeqnarraybox}{rCl}
\frac{\partial y}{\partial t} - \lapl y &=& 0 \\
\partial_\nu y + \alpha y &=& \beta u \\
y(0) &=& y_0
\end{IEEEeqnarraybox} & 
\begin{IEEEeqnarraybox}{rCl}
-\frac{\partial p}{\partial t} - \lapl p &=& 0 \\
\partial_\nu p + \alpha p &=& 0 \\
p(T) &=& y(T) - y_\Omega
\end{IEEEeqnarraybox}
\end{IEEEeqnarraybox} \\
u = \projP_{[u_a, u_b]} \left\{ - \lambda^{-1} \beta p \right\}.
\end{IEEEeqnarray*}
For the discrete case, one obtains a corresponding formulation due to \cref{thm:discrete-variational-ineq,thm:projection-formula} the system
\begin{IEEEeqnarray*}{c}
\begin{IEEEeqnarraybox}{rCl"l}
A(y_h, v_h) &=& \left\langle \beta u_h, v_h \right\rangle_{L^2(\Sigma_R)} + \langle y_0, v_h(0) \rangle_{L^2(\Omega)} & \text{for all $v_h \in \Shp(\meshT_N)$}\\
A(v_h, p_h) &=& \langle y_h(T) - y_\Omega, v_h(T) \rangle_{L^2(\Omega)} & \text{for all $v_h \in \Shp(\meshT_N)$}
\end{IEEEeqnarraybox} \\
u_h = \projP_{[u_a, u_b]} \left\{ - \lambda^{-1} \beta p_h \right\}.
\end{IEEEeqnarray*}
We observe two things that may not come as a surprise: Firstly, our discrete optimality system is equivalent to applying the discontinuous Galerkin method to the state and adjoint equation of the continuous case individually, and secondly that the adjoint operator of the discrete state equation uses the transposed matrix.

In the case of unrestricted controls, i.e.\ $u_a = - \infty$ and $u_b = \infty$, this leads to a method which we can solve as linear equation system directly.

With this, we have shown that the setting we're working with is well defined and yields a solution to an optimal control problem similar to the continuous one.
In a further step, we would like to prove convergence of the discrete problem.
Given that \cref{thm:dg-convergence} requires at least $y \in H^2(\meshT_N) \subset H^2(Q)$, we have to make the following assumption:
\begin{assumption}
\label{as:continuous-Hs-regularity}
The solution of the system \cref{eq:generalized-weak-form} lies in $H^s(Q) \cap W(0,T)$ with $s \geq 2$, i.e.
\[
	\| y \|_{H^s(Q)} \leq c_w' \left( \| g_I \|_{L^2(Q)} + \| g_R \|_{L^2(\Sigma)} + \| y_0 \|_{L^2(\Omega)} \right)
\]
with a constant $c_w' > 0$ independent of $g_I$, $g_R$ and $y_0$.
\end{assumption}
As discussed in the remark after \cref{thm:dg-convergence}, a result like $y \in L^2(0, T; H^2(\Omega)) \cap H^1(0, T; L^2(\Omega))$, which often can be proved, is not sufficient.
\begin{theorem}
\label{thm:optimal-control-convergence}
Let $\bar{u}$ be the optimal control for the continuous problem \cref{prb:BoundaryOptimalControl-restricted} and let \cref{as:continuous-Hs-regularity} hold. Moreover, be $\bar{u}_h$ the optimal control to the discretized problem \cref{eq:f-Sh}.
Then there is a $C > 0$ independent of $h$, such that:
\[
	\| \bar{u} - \bar{u}_h \|_{L^2(\Sigma)} + \| \bar{y}(T) - \bar{y}_h(T) \|_{L^2(\Omega)} \leq C h^{\min \{ s, p+1\} - 1} \left( \| \bar{u} \|_{L^2(\Sigma)} + \| y_\Omega \|_{L^2(\Omega)} \right).
\]
\end{theorem}
\begin{proof}
This proof is based on the previously mentioned variational discretization technique presented in \cite{Hinze}.
The choice of the control space $L^2(\Omega)$ for the discretized problem becomes essential here: We may test the discretized variational inequality with the optimal control of the continuous problem and vice versa. 

The optimal solutions to both problems fulfill
\begin{IEEEeqnarray*}{rCl"l}
	0 &\leq& \langle S \bar{u} - z, S (u - \bar{u}) \rangle_{L^2(\Omega)} + \lambda \langle\bar{u}, u - \bar{u} \rangle_{L^2(\Sigma)} & \text{for all } u \in U_{ad}, \\
	0 &\leq& \langle S_h \bar{u}_h - z_h, S_h (u_h - \bar{u}_h) \rangle_{L^2(\Omega)} + \lambda \langle\bar{u}_h, u_h - \bar{u}_h \rangle_{L^2(\Sigma)} & \text{for all } u_h \in U_{ad}.
\end{IEEEeqnarray*}
By testing the first equation with $\bar{u}_h$ and the second one with $\bar{u}$ we obtain after multiplying the second one by minus one:
\begin{IEEEeqnarray*}{rCl}
	0 &\leq& \langle S \bar{u} - z, S (\bar{u}_h - \bar{u}) \rangle_{L^2(\Omega)} + \lambda \langle\bar{u}, \bar{u}_h - \bar{u} \rangle_{L^2(\Sigma)}, \\
	0 &\geq& \langle S_h \bar{u}_h - z_h, S_h (\bar{u}_h - \bar{u}) \rangle_{L^2(\Omega)} + \lambda \langle\bar{u}_h, \bar{u}_h - \bar{u} \rangle_{L^2(\Sigma)}.
\end{IEEEeqnarray*}
Now we can subtract the second equation from the first to obtain:
\begin{IEEEeqnarray*}{rCl}
0 &\leq& \langle S \bar{u} - z, S (\bar{u}_h - \bar{u}) \rangle_{L^2(\Omega)} + \lambda \langle\bar{u}, \bar{u}_h - \bar{u} \rangle_{L^2(\Sigma)} \\
&& \quad {} - \langle S_h \bar{u}_h - z_h, S_h (\bar{u}_h - \bar{u}) \rangle_{L^2(\Omega)} - \lambda \langle\bar{u}_h, \bar{u}_h - \bar{u} \rangle_{L^2(\Sigma)} \\
&=& \langle S \bar{u} - z, S (\bar{u}_h - \bar{u}) \rangle_{L^2(\Omega)} - \langle S_h \bar{u}_h - z_h, S_h (\bar{u}_h - \bar{u}) \rangle_{L^2(\Omega)} + \lambda \langle\bar{u} - \bar{u}_h, \bar{u}_h - \bar{u} \rangle_{L^2(\Sigma)}
\end{IEEEeqnarray*}
By bringing the last term over to the other side we obtain:
\begin{IEEEeqnarray*}{rCl}
\lambda \langle\bar{u} - \bar{u}_h, \bar{u} - \bar{u}_h \rangle_{L^2(\Sigma)} &\leq& \langle S \bar{u} - z, S (\bar{u}_h - \bar{u}) \rangle_{L^2(\Omega)} - \langle S_h \bar{u}_h - z_h, S_h (\bar{u}_h - \bar{u}) \rangle_{L^2(\Omega)}
\end{IEEEeqnarray*}
Given that we have $\lambda \| \bar{u} - \bar{u}_h \|_{L^2(\Sigma)}^2$ on the left hand side, we want to estimate the right hand side with an appropriate upper bound.
For this purpose, we introduce the notation $v \coloneqq \bar{u}_h - \bar{u}$. The motivation here is to shorten notation a bit, and the estimation on the right hand side actually holds for any $v \in U_{ad}$.
We start with the right hand side and perform a zero addition of $\langle S_h \bar{u} - z_h, S v \rangle$:
\begin{IEEEeqnarray*}{rCl}
	\chi &\coloneqq& \langle S \bar{u} - z, S v \rangle_{L^2(\Omega)} - \langle S_h \bar{u} - z_h, S_h v \rangle_{L^2(\Omega)} \\
	&=& \langle S \bar{u} - z - [ S_h \bar{u} - z_h ], S v \rangle_{L^2(\Omega)} - \langle S_h \bar{u} - z_h, S_h v -S v \rangle_{L^2(\Omega)} \\
	&=& \langle S \bar{u} + (G_0 y_0)(T) - y_\Omega - S_h \bar{u} - (G_0^h y_0)(T) + y_\Omega, S v \rangle_{L^2(\Omega)} \\
	&& \quad {} - \langle S_h \bar{u} + (G_0^h y_0)(T) - y_\Omega, S_h v -S v \rangle_{L^2(\Omega)}, \\
\noalign{\noindent where we made use of the definitions of $z$ and $z_h$, respectively. Due to the definition of $\bar{y}(T) = S \bar{u} + (G_0 y_0)(T)$, we can replace that term. By defining $\bar{y}^h$ as the state deriving from the discrete state equation with the control $\bar{u}$, we can also replace $\bar{y}^h(T) = S_h \bar{u} + (G_0^h y_0)(T)$: \vspace{\jot}}
	\chi &=& \langle \bar{y}(T) - \bar{y}^h(T) , S v \rangle_{L^2(\Omega)} - \langle \bar{y}^h(T) - y_\Omega, S_h v -S v \rangle_{L^2(\Omega)}. \\
\noalign{\noindent Using the triangle inequality and the Cauchy-Schwarz inequality, we obtain with the observation that $(G_\Sigma \beta v)(T) = Sv$ and $( G_\Sigma^h \beta v)(T) = S_h v$ we can transform this into:\vspace{\jot}}
	\chi &\leq& \left| \langle \bar{y}(T) - \bar{y}^h(T) , S v \rangle_{L^2(\Omega)} \right| + \left| \langle \bar{y}^h(T) - y_\Omega, S_h v -S v \rangle_{L^2(\Omega)} \right| \\
	&\leq& \left\| \bar{y}(T) - \bar{y}^h(T) \right\|_{L^2(\Omega)} \| (G_\Sigma \beta v)(T) \|_{L^2(\Omega)} \\
	&& \quad {}+ \left\| \bar{y}^h(T) - y_\Omega \right\|_{L^2(\Omega)} \left\| ( G_\Sigma^h \beta v)(T) - (G_\Sigma \beta v)(T) \right\|_{L^2(\Omega)}. \\
\noalign{\noindent By the definition of the $\lDG y \rDG$ norm, we have the term $\| y(T) \|_{L^2(\Omega)}$ in it, c.f.\ \cref{eq:dg-DG-norm}, we can estimate after employing the triangle inequality once again \vspace{\jot}}
	\chi &\leq& \left\lDG \bar{y} - \bar{y}^h \right\rDG \| (G_\Sigma \beta v)(T) \|_{L^2(\Omega)} + \left( \left\lDG \bar{y}^h \right\rDG + \| y_\Omega \|_{L^2(\Omega)} \right) \left\lDG G_\Sigma^h \beta v - G_\Sigma \beta v \right\rDG. \\
\noalign{\noindent Due to \cref{thm:W0T-continuous-embedding} we estimate this further via \vspace{\jot}}
	\IEEEeqnarraymulticol{3}{c}{ \| (G_\Sigma \beta v)(T) \|_{L^2(\Omega)} \leq \| (G_\Sigma \beta v) \|_{C([0, T], L^2(\Omega))} \leq c_E \| (G_\Sigma \beta v) \|_{W(0, T)}, } \\
\noalign{\noindent which then leads with \cref{thm:generalized-problem-solution} to \vspace{\jot}}
	\chi &\leq& c_w c_E \left\lDG \bar{y} - \bar{y}^h \right\rDG \| \beta \|_{L^\infty(\Sigma)} \| v \|_{L^2(\Sigma)} + \left( \left\lDG \bar{y}^h \right\rDG + \| y_\Omega \|_{L^2(\Omega)} \right) \left\lDG G_\Sigma^h \beta v - G_\Sigma \beta v \right\rDG. \\
\noalign{\noindent Using the stability estimate for $A(\cdot, \cdot)$ given by \cref{thm:Astab-est}, we can estimate the operator $G_\Sigma^h$ because it consists out of the inverse $A_h^{-1}$:}
	\chi &\leq& c_w c_E \left\lDG \bar{y} - \bar{y}^h \right\rDG \| \beta \|_{L^\infty(\Sigma)} \| v \|_{L^2(\Sigma)} \\
	&& {}+ \left( \frac{1}{c_S^A} \left( \| \beta \|_{L^\infty(\Sigma)} \| \bar{u} \|_{L^2(\Sigma)} + \| y_\Omega \|_{L^2(\Omega)} \right) + \| y_\Omega \|_{L^2(\Omega)} \right) \left\lDG G_\Sigma^h \beta v - G_\Sigma \beta v \right\rDG. \\
\noalign{\noindent As a next step we can apply the result \cref{thm:dg-convergence} because we made \cref{as:continuous-Hs-regularity}:}
	\chi &\leq& c \max\{1, \alpha\} h^{\min \{ s, p+1\} - 1} \cdot \Bigg[ c_w c_E |\bar{y}|_{H^s(\meshT_N)} \| \beta \|_{L^\infty(\Sigma)} \| v \|_{L^2(\Sigma)} \\
	&& {}+ \left( \frac{1}{c_S^A} \left( \| \beta \|_{L^\infty(\Sigma)} \| \bar{u} \|_{L^2(\Sigma)} + \| y_\Omega \|_{L^2(\Omega)} \right) + \| y_\Omega \|_{L^2(\Omega)} \right) |G_\Sigma \beta v|_{H^s(\meshT_N)} \Bigg]. \\
\noalign{\noindent Again via the \cref{as:continuous-Hs-regularity} this leads to}
	\chi &\leq& c \max\{1, \alpha\} h^{\min \{ s, p+1\} - 1} \cdot \Bigg[ c_w c_E c_w'  ( \| \beta \|_{L^\infty(\Sigma)} \|\bar{u}\|_{L^2(\Sigma)} + \| y_\Omega \|_{L^2(\Omega)} ) \\
	&& {}+ c_w' \left( \frac{1}{c_S^A} \left( \| \beta \|_{L^\infty(\Sigma)} \| \bar{u} \|_{L^2(\Sigma)} + \| y_\Omega \|_{L^2(\Omega)} \right) + \| y_\Omega \|_{L^2(\Omega)} \right)  \Bigg] \| \beta \|_{L^\infty(\Sigma)} \|v\|_{L^2(\Sigma)} . \\
\noalign{\noindent Finally, by combining the constants into a single one called $C > 0$, we obtain}
	\chi &\leq& C h^{\min \{ s, p+1\} - 1} \left( \| \bar{u} \|_{L^2(\Sigma)} + \| y_\Omega \|_{L^2(\Omega)} \right) \| v \|_{L^2(\Sigma)}.
\end{IEEEeqnarray*}
This is the estimate for the right hand side we were looking for.

By applying these estimates to the equation we started from with the choice of test function $v = \bar{u}_h - \bar{u}$ yields:
\begin{IEEEeqnarray*}{rCl}
	\IEEEeqnarraymulticol{3}{l}{ C h^{\min \{ s, p+1\} - 1} \left( \| \bar{u} \|_{L^2(\Sigma)} + \| y_\Omega \|_{L^2(\Omega)} \right) \| \bar{u} - \bar{u}_h \|_{L^2(\Sigma)} } \\
	\qquad &\geq& \langle S \bar{u} - z, S (\bar{u}_h - \bar{u}) \rangle_{L^2(\Omega)} - \langle S_h \bar{u} - z_h, S_h (\bar{u}_h - \bar{u}) \rangle_{L^2(\Omega)} \\
	&\geq& \lambda \| \bar{u} - \bar{u}_h \|_{L^2(\Sigma)}^2.
\end{IEEEeqnarray*}
Dividing by $\lambda \| \bar{u} - \bar{u}_h \|_{L^2(\Sigma)}$ leaves us with
\[
	C \lambda^{-1} h^{\min \{ s, p+1\} - 1} \left( \| \bar{u} \|_{L^2(\Sigma)} + \| y_\Omega \|_{L^2(\Omega)} \right) \geq \| \bar{u} - \bar{u}_h \|_{L^2(\Sigma)}.
\]

In a further step, we have to estimate $\| \bar{y}(T) - \bar{y}_h(T) \|_{L^2(\Omega)}$:
\begin{IEEEeqnarray*}{rCl}
\| \bar{y}(T) - \bar{y}_h(T) \|_{L^2(\Omega)} &=& \| S \bar{u} - S_h \bar{u}_h \|_{L^2(\Omega)} \\
&\leq& \| S \bar{u} - S_h \bar{u} \|_{L^2(\Omega)} + \| S_h \bar{u} - S_h \bar{u}_h \|_{L^2(\Omega)} \\
&\leq& \| \bar{y}(T) - \bar{y}^h(T) \|_{L^2(\Omega)} + \| \bar{y}^h(T) - \bar{y}_h(T) \|_{L^2(\Omega)} \\
&\leq& c \max\{1, \alpha\} h^{\min \{ s, p+1\} - 1} |\bar{y}|_{H^s(\meshT_N)} + \frac{1}{c_S^A} \| \beta \|_{L^\infty(\Sigma)} \| \bar{u} - \bar{u}_h \|_{L^2(\Sigma)} \\
&\leq& C h^{\min \{ s, p+1\} - 1} \left( \| \beta \|_{L^\infty(\Sigma)} \| \bar{u} \|_{L^2(\Sigma)} + \| y_\Omega \|_{L^2(\Omega)} \right)\\
&& \quad {} + C' h^{\min \{ s, p+1\} - 1} \left( \| \bar{u} \|_{L^2(\Sigma)} + \| y_\Omega \|_{L^2(\Omega)} \right),
\end{IEEEeqnarray*}
where we used the same stability argument to estimate the stability of the discrete solution as before when estimating the control:
\begin{IEEEeqnarray*}{rCl}
\| \bar{y}^h(T) - \bar{y}_h(T) \|_{L^2(\Omega)} &\leq& \lDG \bar{y}^h  - \bar{y}_h \rDG \\
&=& \lDG G_\Sigma^h \beta \bar{u} + (G_0^h y_0) - G_\Sigma^h \beta \bar{u}_h - (G_0^h y_0) \rDG \\
&\leq& \frac{1}{c_S^A} \| \beta \|_{L^\infty(\Sigma)} \| \bar{u} - \bar{u}_h \|_{L^2(\Sigma)}.
\end{IEEEeqnarray*}
Together, the estimates prove the claim made.
\end{proof}
\begin{remark}
We observe that \cref{as:continuous-Hs-regularity} can be weakened somewhat:
Instead of making the demand for general right hand sides, it was only necessary to use this for two specific evaluations, namely $G_\Sigma \beta (\bar{u}_h - \bar{u})$ and $\bar{y}$.
Especially the former of the two conditions might be difficult to prove as it includes the optimal control of the discretized problem.
\end{remark}
\cref{thm:optimal-control-convergence} only discusses the convergence of $\bar{y}_h$ and $\bar{y}$ on the domain they're being observed on.
We can however derive a convergence of $\bar{y}_h$ towards $\bar{y}$ in the $\lDG \cdot \rDG$ norm:
\begin{theorem}
\label{thm:discrete-state-convergence}
Let the assumptions of \cref{thm:optimal-control-convergence} hold and be $\bar{u}_h$ and $\bar{u}$ as in the theorem, with their respective associated states $\bar{y}_h$ and $\bar{y}$.
Then it holds for some constant $C > 0$
\begin{IEEEeqnarray*}{rCl}
	\lDG \bar{y}_h - \bar{y} \rDG &\leq& C h^{\min \{ s, p+1\} -1} ( \|\bar{u}\|_{L^2(\Sigma)} + \| y_\Omega \|_{L^2(\Omega)} ).
\end{IEEEeqnarray*}
\end{theorem}
\begin{proof}
Using the triangle inequality we estimate:
\begin{IEEEeqnarray*}{rCl}
	\lDG \bar{y}_h - \bar{y} \rDG &\leq& \lDG \bar{y}_h - \bar{y}^h \rDG + \lDG \bar{y}^h - \bar{y} \rDG.
\end{IEEEeqnarray*}
For the first term, we use the previously applied stability estimate \cref{thm:Astab-est} followed by \cref{thm:optimal-control-convergence}:
\begin{IEEEeqnarray*}{rCl}
\lDG \bar{y}^h  - \bar{y}_h \rDG &=& \lDG G_\Sigma^h \beta \bar{u} + (G_0^h y_0) - G_\Sigma^h \beta \bar{u}_h - (G_0^h y_0) \rDG \\
&\leq& \frac{1}{c_S^A} \| \beta \|_{L^\infty(\Sigma)} \| \bar{u} - \bar{u}_h \|_{L^2(\Sigma)} \\
&\leq& \frac{1}{c_S^A} \| \beta \|_{L^\infty(\Sigma)} \cdot C \lambda^{-1} h^{\min \{ s, p+1\} - 1} \left( \| \bar{u} \|_{L^2(\Sigma)} + \| y_\Omega \|_{L^2(\Omega)} \right).
\end{IEEEeqnarray*}
For the second term, we can apply \cref{thm:dg-convergence} if \cref{as:continuous-Hs-regularity} is assumed:
\begin{IEEEeqnarray*}{rCl}
\lDG \bar{y}^h - \bar{y} \rDG &\leq& c \max\{ 1, \alpha \} h^{\min \{ s, p+1\} -1} |y|_{H^s(\meshT_N)} \\
&\leq&  c c_w' \max\{ 1, \alpha \} h^{\min \{ s, p+1\} -1} ( \| \beta \|_{L^\infty(\Sigma)} \|\bar{u}\|_{L^2(\Sigma)} + \| y_\Omega \|_{L^2(\Omega)} ).
\end{IEEEeqnarray*}
By combining both estimates, we obtain the desired result.
\end{proof}
Of course, we also obtain an estimate for $\| \bar{y}_h - \bar{y} \|_{L^2(Q)}$ from this by using \cref{thm:dg-L2-estimate}:
\begin{corollary}
\label{thm:discrete-state-convergence-L2}
Let the assumptions of \cref{thm:optimal-control-convergence} hold and be $\bar{u}_h$ and $\bar{u}$ as in the theorem, with their respective associated states $\bar{y}_h$ and $\bar{y}$.
Then it holds for some constant $C > 0$
\begin{IEEEeqnarray*}{rCl}
	\| \bar{y}_h - \bar{y} \|_{L^2(Q)} &\leq& C h^{\min \{ s, p+1\}} ( \|\bar{u}\|_{L^2(\Sigma)} + \| y_\Omega \|_{L^2(\Omega)} ).
\end{IEEEeqnarray*}
\end{corollary}
Given the problem interpretation, we want to find an optimal control for the continuous setting.
As of such, we are also interested in the convergence of the continuous state associated with the discretized control towards the state associated with the optimal control:
\begin{theorem}
\label{thm:state-error-bound}
Let the assumptions of \cref{thm:optimal-control-convergence} hold and $\bar{u}_h$ be as in the theorem.
Furthermore, let $y_{\bar{u}_h}$ be the \emph{actual} state associated with the control $\bar{u}_h$, i.e.\
\[
	y_{\bar{u}_h} \coloneqq G_\Sigma \beta \bar{u}_h + G_0 y_0.
\]
Then it holds for some constant $C > 0$ independent of $h$, with the state $\bar{y}$ associated with the optimal control $\bar{u}$ that
\[
	\| y_{\bar{u}_h} - \bar{y} \|_{W(0, T)} \leq C h^{\min \{ s, p+1\} - 1} \left( \| \bar{u} \|_{L^2(\Sigma)} + \| y_\Omega \|_{L^2(\Omega)} \right).
\]
If \cref{as:continuous-Hs-regularity} holds, this can be strengthened to
\[
	| y_{\bar{u}_h} - \bar{y} |_{H^s(Q)} \leq C h^{\min \{ s, p+1\} - 1} \left( \| \bar{u} \|_{L^2(\Sigma)} + \| y_\Omega \|_{L^2(\Omega)} \right).
\]
\end{theorem}
\begin{proof}
\begin{IEEEeqnarray*}{rCl}
	\| y_{\bar{u}_h} - \bar{y} \|_{W(0, T)} &=& \| G_\Sigma \beta \bar{u}_h + G_0 y_0 - G_\Sigma \beta \bar{u} - G_0 y_0 \|_{W(0, T)} \\
	&=& \| G_\Sigma (\beta (\bar{u}_h - \bar{u})) \|_{W(0, T)}.
\end{IEEEeqnarray*}
By \cref{thm:generalized-problem-solution}, this becomes:
\begin{IEEEeqnarray*}{rCl}
	\| y_{\bar{u}_h} - \bar{y} \|_{W(0, T)} &\leq& c_w \|\beta \|_{L^2(\Sigma)} \| \bar{u}_h - \bar{u} \|_{L^2(\Sigma)}.
\end{IEEEeqnarray*}	
Finally, using \cref{thm:optimal-control-convergence}, we obtain
\begin{IEEEeqnarray*}{rCl}
	\| y_{\bar{u}_h} - \bar{y} \|_{W(0, T)} &\leq& c_w \|\beta \|_{L^2(\Sigma)} C h^{\min \{ s, p+1\} - 1} \left( \| \bar{u} \|_{L^2(\Sigma)} + \| y_\Omega \|_{L^2(\Omega)} \right).
\end{IEEEeqnarray*}
Reorganizing the constants into a new one proves the claim.

Naturally, we could also use \cref{as:continuous-Hs-regularity} instead of \cref{thm:generalized-problem-solution} and obtain the same estimate in the $| \cdot |_{H^s(0, T)}$ norm.
\end{proof}
Furthermore, we can prove that the functionals $J$ and $J_h$ converge in a pointwise fashion:
\begin{lemma}
\label{thm:J-convergence}
Let \cref{as:continuous-Hs-regularity} hold. Then with some constant $C > 0$ independent of $h$, we have for any $u \in U_{ad}$ the estimate:
\[
	| J(u) - J_h(u) | \leq \frac{1}{2} C h^{\min \{ s, p+1\} - 1} \left( \| \beta \|_{L^\infty(\Sigma)} \| u \|_{L^2(\Sigma)} + \| y_0 \|_{L^2(\Omega)} \right).
\]
\end{lemma}
\begin{proof}
We start by considering $|J(u) - J_h(u)|$ and make use of the definitions and the triangle inequality:
\begin{IEEEeqnarray*}{rCl}
|J(u) - J_h(u)| &=& \left| \frac{1}{2} \| S u - z \|_{L^2(\Omega)}^2 + \frac{\lambda}{2} \| u \|_{L^2(\Sigma)}^2 - J_h(u) \right| \\
&=& \left| \frac{1}{2} \| S u - z - (S_h u - z_h) + S_h u + z_h \|_{L^2(\Omega)}^2 + \frac{\lambda}{2} \| u \|_{L^2(\Sigma)}^2 - J_h(u) \right| \\
&\leq& \Bigg| \frac{1}{2} \| S u + (G_0 y_0) - S_h u - (G_0^h y_0) \|_{L^2(\Omega)} \\
&& \quad {}+ \frac{1}{2} \| S_h u - z_h \|_{L^2(\Omega)}^2 + \frac{\lambda}{2} \| u \|_{L^2(\Sigma)}^2 - J_h(u) \Bigg| \\
&=& \left| \frac{1}{2} \| S u + (G_0 y_0) - S_h u - (G_0^h y_0) \|_{L^2(\Omega)} + J_h(u) - J_h(u) \right| \\
&=& \frac{1}{2} \| S u + (G_0 y_0) - S_h u - (G_0^h y_0) \|_{L^2(\Omega)}.
\end{IEEEeqnarray*}
We can apply the convergence result \cref{thm:dg-convergence} again and obtain by subsequently exploiting \cref{as:continuous-Hs-regularity}:
\begin{IEEEeqnarray*}{rCl}
	|J(u) - J_h(u)| &\leq& \frac{1}{2} c \max\{ 1, \alpha \} h^{\min \{ s, p+1\} -1} |y|_{H^s(\meshT_N)} \\
	&\leq& \frac{1}{2} c c_w' \max\{ 1, \alpha \} h^{\min \{ s, p+1\} -1} (\| \beta \|_{L^\infty(\Sigma)} \| u \|_{L^2(\Sigma)} + \| y_0 \|_{L^2(\Omega)} ).
\end{IEEEeqnarray*}
This was the result we aimed to prove.
\end{proof}
\needspace{15\baselineskip}
\section{Optimal control with interior heat sources}
\label{sec:inner-optimal-control}
With methods analogous to the previous section we now treat a problem formulation with a distributed control, representing an interior source of heat:
\begin{problem}
\label{prb:InnerOptimalControl-restricted}
\begin{IEEEeqnarray*}{c}
\min J(y, u) = \frac{1}{2} \iint_\Sigma \left( y(x, t) - y_\Sigma(x, t) \right)^2 \dd s(x) \dd t + \frac{\lambda}{2} \iint_{Q} u(x, t)^2 \dd x \dd t \\
\noalign{\noindent subject to\vspace{\jot}}
\begin{IEEEeqnarraybox}{rCl"l}
\frac{\partial y}{\partial t} - \lapl y &=& \beta u & \text{in } Q \\
\partial_\nu y + \alpha y &=& 0 & \text{in } \Sigma\\
y(x, 0) &=& 0 & \text{in } \Omega
\end{IEEEeqnarraybox} \\
\noalign{\noindent and\vspace{\jot}}
u_a(x, t) \leq u(x, t) \leq u_b(x, t) \quad \text{a.e.\ in $Q$}.
\end{IEEEeqnarray*}
\end{problem}
This time around, we have an interior heat source with our observation taking place on the boundary of the domain.
In other words, one can interpret this task as the search for an unknown, inner heat source fitting to an observed heat distribution on the boundary.

As before, we assume $\Omega \subset \R^d$ to be a Lipschitz domain, $y_\Sigma \in L^2(\Sigma)$, $u_a, u_b \in L^2(Q)$ with $u_a(x, t) \leq u_b(x, t)$ and some constants $\lambda \geq 0$ and $\alpha \geq 0$. Moreover, $\beta \in L^\infty(Q)$ can function as a restriction to where the control may operate.

The treatment of this problem is very similar to the one of \cref{prb:BoundaryOptimalControl-restricted}.
In this case, the solution of the state equation can be expressed as
\[
	y = G_Q \beta u.
\]
Here, we need the observation operator $E_\Sigma : y \mapsto y|_\Sigma$, so that we can define the control-to-state operator $S \coloneqq E_\Sigma G_Q \beta u$.
The operator $E_\Sigma$ is continuous from $W(0, T)$ to $L^2(0, T; L^2(\partial \Omega)) \cong L^2(\Sigma)$, hence $S : u \mapsto y|_\Sigma$ is continuous from $L^2(Q)$ to $L^2(\Sigma)$.

By applying \cref{thm:optimal-control-existence} one obtains the existence of a unique solution $\bar{u}$ again, and by using \cref{thm:variational-ineq}, one obtains the optimality condition
\[
	0 \leq \langle S \bar{u} - y_\Sigma, S u - S \bar{u} \rangle_{L^2(\Sigma)} + \lambda \langle \bar{u}, u - \bar{u} \rangle_{L^2(Q)} \quad \forall u \in U_{ad},
\]
where $U_{ad}$ is defined in an analogous fashion as before:
\[
	U_{ad} \coloneqq \left\{ u \in L^2(Q) \midcolon u_a(x, t) \leq u(x, t) \leq u_b(x, t) \quad \text{a.e.\ on $Q$} \right\}.
\]

In this case, the adjoint state is defined by \cref{eq:generalized-problem-adj} with $a_Q = 0$, $a_\Sigma = \bar{y} - y_\Sigma$ and $a_\Omega = 0$.
One can show (see \cite[Satz 3.21, p.\ 131]{Troeltzsch}) that a control $\bar{u} \in U_{ad}$ is optimal if and only if the adjoint state fulfills the variational inequality
\[
	\iint_Q ( \beta p + \lambda \bar{u} ) ( u - \bar{u} ) \dd x \dd t \geq 0 \quad \forall u \in U_{ad}.
\]

For the discrete case, we define
\[
	G_I^h(g_I) \coloneqq E_V A_h^{-1} \boldsymbol{g_I}(g_I)
\]
and then
\[
	S_h \coloneqq E_\Sigma G_I^h \beta \cdot.
\]
Resulting from the lack of an initial condition, no definition of $z$, and accordingly $z_h$ is necessary in this case.

The operator $S_h$ is obviously linear, but continuity poses a problem that needs discussion.
For the treatment of \cref{prb:BoundaryOptimalControl-restricted}, we used the argument that one could estimate the norm of the observation, i.e.\ $\| y_h(T) \|_{L^2(\Omega)}$ via $\| y_h \|_{A}$, or accordingly, $\lDG y_h \rDG$.
However, such an argument does not hold in this case, as $\| y_h \|_{L^2(\Sigma)}$ cannot be estimated by $\lDG y_h \rDG$, at least not if Neumann boundary conditions are assumed.
We may however argue that $\| y_h \|_{L^2(\Sigma)} \leq \alpha^{-\frac{1}{2}} \lDGw y_h \rDGw$ holds, which in the case of $\alpha \neq 0$ together with the the ellipticity estimate \cref{thm:A-ellip-weak} (or alternatively, for quasi-uniform meshes, the estimate \cref{thm:Astab-est}) shows continuity of the operator $S_h$.

In the case $\alpha \neq 0$, we can apply \cref{thm:optimal-control-existence} because $S_h$ is going to be continuous.
However, the case $\alpha = 0$ poses a problem, as not even a convergence argument could be made because of the exponent of $\alpha$ being negative. In fact, it's not clear if the results even apply for this case at all. Therefore, we assume $\alpha \neq 0$ in the following.

As a next step we would like to apply \cref{thm:variational-ineq} again, too. Just as in the continuous case, we define the adjoint state via \cref{eq:generalized-problem-adj-dG} with the choices $a_Q = 0$, $a_\Sigma = \bar{y}_h - y_\Sigma$ and $a_\Omega = 0$.
With this choice, we can immediately conclude the following result:
\begin{theorem}
\label{thm:inner-variational-ineq}
There exists a unique optimal control $\bar{u}_h \in U_{ad}$ for \cref{prb:InnerOptimalControl-restricted}.

It can be characterized with a variational inequality: A control $\bar{u}_h \in U_{ad}$ is optimal if and only if with the associated adjoint state $\bar{p}_h$ defined as the solution of \cref{eq:generalized-problem-adj-dG} with the choices $a_Q = 0$, $a_\Sigma = \bar{y}_h - y_\Sigma$ and $a_\Omega = 0$ fulfills the following variational inequality
\[
	\langle \beta(x, t) \bar{p}_h (x, t) + \lambda \bar{u}_h(x, t), u_h - \bar{u}_h \rangle_{L^2(Q)} \geq 0 \quad \forall u_h \in U_{ad}
\]
almost everywhere.
\end{theorem}
\begin{proof}
The proof is identical to the one of \cref{thm:discrete-variational-ineq} with the respective choices of $a_Q$, $a_\Sigma$ and $a_\Omega$ adjusted to the interior heating situation.
\end{proof}
Just as before, the same projection formula follows from this inequality:
\begin{theorem}
\label{thm:inner-projection-formula}
A control $\bar{u}_h \in U_{ad}$ is optimal for \cref{prb:InnerOptimalControl-restricted} if and only if the associated adjoint state $\bar{p}_h$ fulfills the equality
\[
	\bar{u}_h = \projP_{[u_a, u_b]} \left\{ - \lambda^{-1} \beta \bar{p}_h \right\}.
\]
\end{theorem}
\begin{proof}
The proof is exactly the same as for \cref{thm:projection-formula}.
\end{proof}
Furthermore, one can again summarize the results into optimality systems.
For the continuous formulation, this is:
\begin{IEEEeqnarray*}{c}
\begin{IEEEeqnarraybox}{r"l}
\begin{IEEEeqnarraybox}{rCl}
\frac{\partial y}{\partial t} - \lapl y &=& \beta u \\
\partial_\nu y + \alpha y &=& 0 \\
y(0) &=& 0
\end{IEEEeqnarraybox} & 
\begin{IEEEeqnarraybox}{rCl}
-\frac{\partial p}{\partial t} - \lapl p &=& y - y_\Sigma \\
\partial_\nu p + \alpha p &=& 0 \\
p(T) &=& 0
\end{IEEEeqnarraybox}
\end{IEEEeqnarraybox} \\
u = \projP_{[u_a, u_b]} \left\{ - \lambda^{-1} \beta p \right\}.
\end{IEEEeqnarray*}
For the discrete case, one obtains a corresponding formulation due to \cref{thm:inner-variational-ineq,thm:inner-projection-formula} the system
\begin{IEEEeqnarray*}{c}
\begin{IEEEeqnarraybox}{rCl"l}
A(y_h, v_h) &=& \left\langle \beta u_h, v_h \right\rangle_{L^2(Q)} & \text{for all $v_h \in \Shp(\meshT_N)$}\\
A(v_h, p_h) &=& \langle y_h - y_\Sigma, v_h \rangle_{L^2(\Sigma)} & \text{for all $v_h \in \Shp(\meshT_N)$}
\end{IEEEeqnarraybox} \\
u_h = \projP_{[u_a, u_b]} \left\{ - \lambda^{-1} \beta p_h \right\}.
\end{IEEEeqnarray*}
In order to replicate the convergence theorem \cref{thm:optimal-control-convergence}, we need to make assumption \cref{as:continuous-Hs-regularity} again, which can be weakened as explained in the remark after \cref{thm:optimal-control-convergence}.

\begin{theorem}
Let for $\alpha > 0$ be $\bar{u}$ be the optimal control for the continuous problem \cref{prb:InnerOptimalControl-restricted} and let \cref{as:continuous-Hs-regularity} hold. Moreover, be $\bar{u}_h$ the discretized solution to \cref{eq:f-Sh}.
Then there is a $C > 0$ independent of $h$, such that:
\[
	\| \bar{u} - \bar{u}_h \|_{L^2(Q)} + \| \bar{y} - \bar{y}_h \|_{L^2(\Sigma)} \leq C h^{\min \{ s, p+1\} - 1} \left( \| \bar{u} \|_{L^2(Q)} + \| y_\Sigma \|_{L^2(\Sigma)} \right).
\]
\end{theorem}
\begin{proof}
Under the assumption $\alpha > 0$, the proof is analogous to the one of \cref{thm:optimal-control-convergence}. Instead of $z_h$ and $z$ one uses $y_\Sigma$ directly though, as the state is now linearly dependent on the control and not affine-linear as in the boundary control case.
\end{proof}
As before, one can shown that the results of \cref{thm:state-error-bound,thm:discrete-state-convergence} hold in this case, too.
We can also transport \cref{thm:J-convergence}:
\begin{lemma}
\label{thm:J-convergence-inner}
Let \cref{as:continuous-Hs-regularity} hold. Then with some constant $C > 0$ independent of $h$, we have for any $u \in U_{ad}$ the estimate:
\[
	| J(u) - J_h(u) | \leq \frac{1}{2} C h^{\min \{ s, p+1\} -1} \| u \|_{L^2(Q)}.
\]
\end{lemma}
\begin{proof}
We proceed as before with the proof of \cref{thm:J-convergence}:
\begin{IEEEeqnarray*}{rCl}
|J(u) - J_h(u)| &=& \left| \frac{1}{2} \| S u - y_\Sigma \|_{L^2(\Sigma)}^2 + \frac{\lambda}{2} \| u \|_{L^2(Q)}^2 - J_h(u) \right| \\
&=& \left| \frac{1}{2} \| S u - y_\Sigma - (S_h u - y_\Sigma) + S_h u + y_\Sigma \|_{L^2(\Sigma)}^2 + \frac{\lambda}{2} \| u \|_{L^2(Q)}^2 - J_h(u) \right| \\
&\leq& \left| \frac{1}{2} \| S u - S_h u \|_{L^2(\Sigma)} + \frac{1}{2} \| S_h u - y_\Sigma \|_{L^2(\Sigma)}^2 + \frac{\lambda}{2} \| u \|_{L^2(Q)}^2 - J_h(u) \right| \\
&=& \left| \frac{1}{2} \| S u - S_h u \|_{L^2(\Sigma)} + J_h(u) - J_h(u) \right| \\
&=& \frac{1}{2} \| S u - S_h u \|_{L^2(\Sigma)}.
\end{IEEEeqnarray*}
Once again, we can apply the convergence result \cref{thm:dg-convergence} again and obtain
\begin{IEEEeqnarray*}{rCl}
	|J(u) - J_h(u)| &\leq& \frac{1}{2} c \max\{ 1, \alpha \} h^{\min \{ s, p+1\} -1} |y|_{H^s(\meshT_N)} \\
	&\overset{\text{\cref{as:continuous-Hs-regularity}}}{\leq}& \frac{1}{2} c c_w' \max\{ 1, \alpha \} h^{\min \{ s, p+1\} -1} (\| \beta \|_{L^\infty(Q)} \| u \|_{L^2(Q)}).
\end{IEEEeqnarray*}
This is the result we wanted to prove.
\end{proof}
\begin{remark}
Another interesting fact that comes to light when considering this formulation is that the proof of \cref{thm:optimal-control-convergence} can be transported to formulations as long as the term $S \bar{u} - S_h \bar{u}$ can be estimated in some way.
However, in order for that to be possible, the norm must be possible to estimate against the $\lDG \cdot \rDG$ norm.
This however, is also necessary for showing that $S_h$ is well defined.
In a way this means that either the discontinuous Galerkin approach converges for a given formulation or is not well defined for it at all.
\end{remark}
\section{Optimal control with state observations on the full space}
\label{sec:symmetric-Problem}
We want to introduce a third formulation of the optimal control problem.
In contrast to the last problem, we now want to minimize the error to a given state on the full space-time domain $Q$.
The motivation for considering this problem for us is that we obtain an optimality system for which we can easily generate numerical samples using eigenfunctions, as both the state and adjoint state will have homogeneous boundary conditions.
This leads to the following formulation:
\begin{problem}
\label{prb:SymmetricOptimalControl-restricted}
\begin{IEEEeqnarray*}{c}
\min J(y, u) = \frac{1}{2} \iint_Q \left( y(x, t) - y_Q(x, t) \right)^2 \dd x \dd t + \frac{\lambda}{2} \iint_{Q} u(x, t)^2 \dd x \dd t \\
\noalign{\noindent subject to\vspace{\jot}}
\begin{IEEEeqnarraybox}{rCl"l}
\frac{\partial y}{\partial t} - \lapl y &=& \beta u + q & \text{in } Q \\
y &=& 0 & \text{in } \Sigma\\
y(x, 0) &=& 0 & \text{in } \Omega
\end{IEEEeqnarraybox} \\
\noalign{\noindent and\vspace{\jot}}
u_a(x, t) \leq u(x, t) \leq u_b(x, t) \quad \text{a.e.\ in $Q$}.
\end{IEEEeqnarray*}
\end{problem}
Once again we assume $\Omega \subset \R^d$ to be a Lipschitz domain, $y_Q \in L^2(Q)$, $u_a, u_b \in L^2(Q)$ with $u_a(x, t) \leq u_b(x, t)$ and some constant $\lambda \geq 0$. As before, $\beta \in L^\infty(Q)$ can function as a restriction to where the control may operate. Additionally, we have an inner source function $q \in L^2(Q)$.
Unlike previously, we now use homogeneous Dirichlet boundary conditions.

As in the previous section, we have
\[
	y = G_Q \beta u.
\]
The observation operator is now $E_Q : y \mapsto y$ though, the identity of $L^2(Q)$. This leads to a control to space operator of $S \coloneqq G_Q \beta \cdot$. As the identity is obviously continuous, so is $S$.

For the discontinuous Galerkin method, we can employ \cref{thm:dg-L2-estimate} and combine it with \cref{thm:Astab-est} in order to show the continuity of the corresponding discrete control-to-state operator $S_h \coloneqq G_Q^h \beta \cdot$.
This means that \cref{thm:optimal-control-existence,thm:variational-ineq} can be applied for this problem formulation also.

As before, \cref{thm:discrete-adj-state-helper} suffices for deriving the formulation adjoint state in the sense of \cref{thm:discrete-variational-ineq}. The projection formula \cref{thm:projection-formula} derives for this formulation via the same way.
In the end, we obtain the following optimality system for the continuous case:
\begin{IEEEeqnarray*}{c}
\begin{IEEEeqnarraybox}{r"l}
\begin{IEEEeqnarraybox}{rCl}
\frac{\partial y}{\partial t} - \lapl y &=& q + \beta u \\
y &=& 0 \\
y(0) &=& 0
\end{IEEEeqnarraybox} & 
\begin{IEEEeqnarraybox}{rCl}
-\frac{\partial p}{\partial t} - \lapl p &=& y - y_Q \\
p &=& 0 \\
p(T) &=& 0
\end{IEEEeqnarraybox}
\end{IEEEeqnarraybox} \\
u = \projP_{[u_a, u_b]} \left\{ - \lambda^{-1} \beta p \right\}.
\end{IEEEeqnarray*}
This optimality system follows from the result \cite[Lemma 3.17, p.\ 126]{Troeltzsch}. Alternatively see the work \cite{MeidnerVexler-I}, which treats this formulation, too.
For the discrete case, one obtains a corresponding formulation, derived in the same fashion as \cref{thm:discrete-variational-ineq,thm:projection-formula} were the system
\begin{equation}
\label{eq:symmetric-discrete}
\begin{IEEEeqnarraybox}[][c]{c}
\begin{IEEEeqnarraybox}{rCl"l}
A(y_h, v_h) &=& \left\langle \beta u_h, v_h \right\rangle_{L^2(\Sigma_R)} + \langle q, v_h \rangle_{L^2(Q)} & \text{for all $v_h \in \Shp(\meshT_N)$} \\
A(v_h, p_h) &=& \langle y_h - y_Q, v_h \rangle_{L^2(Q)} & \text{for all $v_h \in \Shp(\meshT_N)$}
\end{IEEEeqnarraybox} \\
u_h = \projP_{[u_a, u_b]} \left\{ - \lambda^{-1} \beta p_h \right\}.
\end{IEEEeqnarraybox}
\end{equation}
Alas, we have a unique optimal control for the discrete problem, too.

An interesting difference comes up when proving the convergence, though. Previously, the observation operators forced us to estimate the convergence on boundary terms, where we needed to apply \cref{thm:dg-convergence} rather than \cref{thm:dg-convergence-L2}. One can translate the proof of \cref{thm:optimal-control-convergence} for this formulation by using \cref{thm:dg-convergence-L2} rather than \cref{thm:dg-convergence}.
This has the effect that one obtains a stronger convergence result:
\begin{theorem}
\label{thm:optimal-control-convergence-symm}
Let $\bar{u}$ be the optimal control for the continuous problem \cref{prb:SymmetricOptimalControl-restricted} and let \cref{as:continuous-Hs-regularity} hold. Moreover, be $\bar{u}_h$ the optimal control to the discretized problem \cref{eq:symmetric-discrete}.
Then there is a $C > 0$ independent of $h$, such that:
\[
	\| \bar{u} - \bar{u}_h \|_{L^2(Q)} + \| \bar{y} - \bar{y}_h \|_{L^2(Q)} \leq C h^{\min \{ s, p+1\}} \left( \| \bar{u} \|_{L^2(Q)} + \| q \|_{L^2(Q)} + \| y_Q \|_{L^2(Q)} \right).
\]
\end{theorem}
For the same reason, we also have for $J_h$ a convergence with this speed:
\begin{lemma}
\label{thm:J-convergence-symm}
Let \cref{as:continuous-Hs-regularity} hold. Then with some constant $C > 0$ independent of $h$, we have for any $u \in U_{ad}$ the estimate:
\[
	| J(u) - J_h(u) | \leq \frac{1}{2} C h^{\min \{ s, p+1\}} \| u \|_{L^2(Q)}.
\]
\end{lemma}
\end{document}